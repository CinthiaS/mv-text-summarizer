{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01c3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "#import smogn\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pysbd.utils import PySBDFactory\n",
    "import math\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from tensorflow.keras.models import model_from_json\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp_md = en_core_web_sm.load()\n",
    "\n",
    "import en_core_web_md\n",
    "nlp_md = en_core_web_md.load()\n",
    "#!python -m spacy download en_core_web_md\n",
    "#nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = \"/scratch/cinthiasouza/mv-text-summarizer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36daaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "from src import ensemble_tree_models\n",
    "from src import tunning_hyperparametrs as th\n",
    "#from src import mlp_regressor\n",
    "#from src import mlp_classifier\n",
    "from src import summarization\n",
    "from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from src import pipeline_extract_features as pef\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab819ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name_section, initial_batch, embed_dim, batch_len, path_base, verbose):\n",
    "    \n",
    "    features_columns = ['sentences', 'text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords', 'tf-isf',\n",
    "       'position_score', 'paragraph_score', 'number_citations', 'length_score',\n",
    "       'pos_score', 'ner_score', 'dist_centroid', 'articles']\n",
    "    \n",
    "    scores_columns = ['rouge_1', 'rouge_2', 'rouge_l', 'label', 'articles']\n",
    "    \n",
    "    embeddings_columns = [i for i in range(embed_dim)]\n",
    "    embeddings_columns.append(\"article\")\n",
    "    \n",
    "\n",
    "    \"\"\" if os.path.isfile('batches_{}.json'.format(batch_len)):\n",
    "\n",
    "        print(\"Load Batch Files\")\n",
    "        with open('batches_{}.json'.format(batch_len)) as f:\n",
    "            batches = json.load(f)\n",
    "\n",
    "        batch_files = [value for key, value in batches.items()]\n",
    "    else: \n",
    "\n",
    "        print(\"Creating Batch Files\")\n",
    "        batch_files = utils.create_batches(path_base, tam=batch_len)\n",
    "        utils.save_batches(batch_files)\n",
    "        \n",
    "\n",
    "    batche_files = batch_files[initial_batch:]\"\"\"\n",
    "\n",
    "    batche_files = os.listdir(path_base)\n",
    "\n",
    "    print(\"Name section: \" + name_section)\n",
    "    vfunc = np.vectorize(pef.extract_features_file)\n",
    "    \n",
    "    print(\"Iniciando a extração de features...\")\n",
    "    \n",
    "    for batch in batche_files:\n",
    "\n",
    "    \n",
    "        pef.extract_features_batches(\n",
    "            vfunc, [batch], path_base, name_section=name_section, features_columns=features_columns,\n",
    "            scores_columns=scores_columns, embeddings_columns=embeddings_columns, verbose=verbose)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1de5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_batch=0\n",
    "#batche_files = batch_files[initial_batch:]\n",
    "\n",
    "path = [\"../../PMC002xxxxxx_pp\", \"../../PMC003xxxxxx_pp\", \"../../PMC005xxxxxx_pp\", \"../../PMC006xxxxxx_pp\"]\n",
    "\n",
    "sections = ['introduction', 'materials', 'conclusion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7c858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name section: introduction\n",
      "Iniciando a extração de features...Name section: materialsName section: conclusion\n",
      "\n",
      "\n",
      "Iniciando a extração de features...Iniciando a extração de features...\n",
      "\n",
      "Name section: materials\n",
      "Iniciando a extração de features...Name section: introductionName section: conclusion\n",
      "\n",
      "\n",
      "Iniciando a extração de features...Iniciando a extração de features...\n",
      "\n",
      "Name section: introduction\n",
      "Iniciando a extração de features...\n",
      "Name section: materialsName section: conclusion\n",
      "\n",
      "Iniciando a extração de features...\n",
      "Iniciando a extração de features...\n",
      "Name section: materialsName section: introductionName section: conclusion\n",
      "Iniciando a extração de features...\n",
      "\n",
      "Iniciando a extração de features...\n",
      "\n",
      "Iniciando a extração de features...\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "l1 = Queue()\n",
    "p1 = Process(\n",
    "    target=main, args=(\n",
    "        sections[0], initial_batch, 300,\n",
    "        700, path[0], False))\n",
    "\n",
    "l2 = Queue()\n",
    "p2 = Process(target=main,  args=(\n",
    "        sections[1], initial_batch, 300,\n",
    "        700, path[0], False))\n",
    "\n",
    "l3 = Queue()\n",
    "p3 = Process(target=main, args=(\n",
    "        sections[2], initial_batch, 300,\n",
    "        700, path[0], False))\n",
    "\n",
    "l4 = Queue()\n",
    "p4 = Process(\n",
    "    target=main, args=(\n",
    "        sections[0], initial_batch, 300,\n",
    "        700, path[1], False))\n",
    "\n",
    "l5 = Queue()\n",
    "p5 = Process(target=main,  args=(\n",
    "        sections[1], initial_batch, 300,\n",
    "        700, path[1], False))\n",
    "\n",
    "l6 = Queue()\n",
    "p6 = Process(target=main, args=(\n",
    "        sections[2], initial_batch, 300,\n",
    "        700, path[1], False))\n",
    "\n",
    "l7 = Queue()\n",
    "p7 = Process(\n",
    "    target=main, args=(\n",
    "        sections[0], initial_batch, 300,\n",
    "        700, path[2], False))\n",
    "\n",
    "l8 = Queue()\n",
    "p8 = Process(target=main,  args=(\n",
    "        sections[1], initial_batch, 300,\n",
    "        700, path[2], False))\n",
    "\n",
    "l9 = Queue()\n",
    "p9 = Process(target=main, args=(\n",
    "        sections[2], initial_batch, 300,\n",
    "        700, path[2], False))\n",
    "\n",
    "l10 = Queue()\n",
    "p10 = Process(\n",
    "    target=main, args=(\n",
    "        sections[0], initial_batch, 300,\n",
    "        700, path[3], False))\n",
    "\n",
    "l11 = Queue()\n",
    "p11 = Process(target=main,  args=(\n",
    "        sections[1], initial_batch, 300,\n",
    "        700, path[3], False))\n",
    "\n",
    "l12 = Queue()\n",
    "p12 = Process(target=main, args=(\n",
    "        sections[2], initial_batch, 300,\n",
    "        700, path[3], False))\n",
    "p1.start()   \n",
    "p2.start()   \n",
    "p3.start()\n",
    "p4.start()   \n",
    "p5.start()   \n",
    "p6.start()\n",
    "p7.start()   \n",
    "p8.start()   \n",
    "p9.start()\n",
    "p10.start()   \n",
    "p11.start()   \n",
    "p12.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf32b033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5403780\ttotal: 245us\tremaining: 245us\n",
      "1:\tlearn: 0.4369467\ttotal: 761us\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "# Initialize data\n",
    "cat_features = [0, 1]\n",
    "\n",
    "train_data = [[\"a\", \"b\", 1, 4, 5, 6],\n",
    "              [\"a\", \"b\", 4, 5, 6, 7],\n",
    "              [\"c\", \"d\", 30, 40, 50, 60]]\n",
    "train_labels = [1, 1, 0]\n",
    "eval_data = [[\"a\", \"b\", 2, 4, 6, 8],\n",
    "             [\"a\", \"d\", 1, 4, 50, 60]]\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                           learning_rate=1,\n",
    "                           depth=2, class_weights={0:0.95, 1:0.05})\n",
    "# Fit model\n",
    "model.fit(train_data, train_labels, cat_features)\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(eval_data)\n",
    "# Get predicted probabilities for each class\n",
    "preds_proba = model.predict_proba(eval_data)\n",
    "# Get predicted RawFormulaVal\n",
    "preds_raw = model.predict(eval_data, prediction_type='RawFormulaVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afff249d",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[16:18:04] /tmp/build/80754af9/xgboost-split_1619724447847/work/dmlc-core/src/io/local_filesys.cc:104: LocalFileSystem.ListDirectory demo/data error: No such file or directory\nStack trace:\n  [bt] (0) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::LocalFileSystem::ListDirectory(dmlc::io::URI const&, std::vector<dmlc::io::FileInfo, std::allocator<dmlc::io::FileInfo> >*)+0xb14) [0x7f54efde1ae4]\n  [bt] (1) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::ConvertToURIs(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0x783) [0x7f54efddd373]\n  [bt] (2) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::InitInputFileInfo(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool)+0x46) [0x7f54efdde156]\n  [bt] (3) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::Init(dmlc::io::FileSystem*, char const*, unsigned long, bool)+0x77) [0x7f54efdde847]\n  [bt] (4) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::InputSplit::Create(char const*, char const*, unsigned int, unsigned int, char const*, bool, int, unsigned long, bool)+0x646) [0x7f54efdb7bc6]\n  [bt] (5) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::InputSplit::Create(char const*, unsigned int, unsigned int, char const*)+0x1f) [0x7f54efdb8cef]\n  [bt] (6) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::Parser<unsigned int, float>* dmlc::data::CreateLibSVMParser<unsigned int, float>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, unsigned int, unsigned int)+0x1b) [0x7f54efd983ab]\n  [bt] (7) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(+0x32345c) [0x7f54efd7c45c]\n  [bt] (8) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(xgboost::DMatrix::Load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x2da) [0x7f54efb9175a]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2811ed2153e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# read in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'demo/data/agaricus.txt.train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'demo/data/agaricus.txt.test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# specify parameters via map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch_data_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                  feature_types)\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_uri\u001b[0;34m(data, missing, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0m_warn_unused_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     _check_call(_LIB.XGDMatrixCreateFromFile(c_str(os.fspath(data)),\n\u001b[0m\u001b[1;32m    490\u001b[0m                                              \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                                              ctypes.byref(handle)))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \"\"\"\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [16:18:04] /tmp/build/80754af9/xgboost-split_1619724447847/work/dmlc-core/src/io/local_filesys.cc:104: LocalFileSystem.ListDirectory demo/data error: No such file or directory\nStack trace:\n  [bt] (0) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::LocalFileSystem::ListDirectory(dmlc::io::URI const&, std::vector<dmlc::io::FileInfo, std::allocator<dmlc::io::FileInfo> >*)+0xb14) [0x7f54efde1ae4]\n  [bt] (1) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::ConvertToURIs(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0x783) [0x7f54efddd373]\n  [bt] (2) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::InitInputFileInfo(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool)+0x46) [0x7f54efdde156]\n  [bt] (3) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::io::InputSplitBase::Init(dmlc::io::FileSystem*, char const*, unsigned long, bool)+0x77) [0x7f54efdde847]\n  [bt] (4) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::InputSplit::Create(char const*, char const*, unsigned int, unsigned int, char const*, bool, int, unsigned long, bool)+0x646) [0x7f54efdb7bc6]\n  [bt] (5) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::InputSplit::Create(char const*, unsigned int, unsigned int, char const*)+0x1f) [0x7f54efdb8cef]\n  [bt] (6) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(dmlc::Parser<unsigned int, float>* dmlc::data::CreateLibSVMParser<unsigned int, float>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&, unsigned int, unsigned int)+0x1b) [0x7f54efd983ab]\n  [bt] (7) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(+0x32345c) [0x7f54efd7c45c]\n  [bt] (8) /scratch/cinthiasouza/anaconda3/lib/libxgboost.so(xgboost::DMatrix::Load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x2da) [0x7f54efb9175a]\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "        'silent': [False],\n",
    "        'max_depth': [6, 10, 15, 20],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'n_estimators': [100]}\n",
    "\n",
    "fit_params = {'eval_metric': 'mlogloss',\n",
    "              'early_stopping_rounds': 10,\n",
    "              'eval_set': [(x_valid, y_valid)]}\n",
    "\n",
    "rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=20,\n",
    "                            n_jobs=1, verbose=2, cv=2,\n",
    "                            fit_params=fit_params,\n",
    "                            scoring='neg_log_loss', refit=False, random_state=42)\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "rs_clf.fit(x_train, y_train)\n",
    "print(\"Randomized search time:\", time.time() - search_time_start)\n",
    "\n",
    "best_score = rs_clf.best_score_\n",
    "best_params = rs_clf.best_params_\n",
    "print(\"Best score: {}\".format(best_score))\n",
    "print(\"Best params: \")\n",
    "for param_name in sorted(best_params.keys()):\n",
    "    print('%s: %r' % (param_name, best_params[param_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
