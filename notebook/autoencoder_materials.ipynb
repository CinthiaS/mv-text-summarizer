{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec32170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, concatenate, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874dfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "from src import mlp_classifier\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a44768",
   "metadata": {},
   "outputs": [],
   "source": [
    "section='materials'\n",
    "\n",
    "with open('dataset/dataset_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "X_features = dataset[section][0]\n",
    "y_features = dataset[section][2]\n",
    "\n",
    "columns = list(range(0, 383))\n",
    "columns = list(map(str, columns))\n",
    "\n",
    "folder_to_save = 'models_v1'\n",
    "path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(folder_to_save)\n",
    "\n",
    "X_embedd = pd.read_csv(\"dataset/embed_bert_{}_train.csv\".format(section))\n",
    "\n",
    "y_embedd = X_embedd['label']\n",
    "X_embedd = X_embedd[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_dim=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bf29b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder vision 1\n",
    "sequence_input = Input(shape=(X_embedd.shape[1],), dtype='int32')\n",
    "\n",
    "e_1 = Dense(X_embedd.shape[1]*2)(sequence_input)\n",
    "e_1 = BatchNormalization()(e_1)\n",
    "e_1 = LeakyReLU()(e_1)\n",
    "\n",
    "e_2 = Dense(X_embedd.shape[1])(e_1)\n",
    "e_2 = BatchNormalization()(e_2)\n",
    "e_2 = LeakyReLU()(e_2)\n",
    "\n",
    "\n",
    "#encoder vision 2\n",
    "sequence_input2 = Input(shape=(X_features.shape[1],), dtype='int32')\n",
    "\n",
    "e_3 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_3 = BatchNormalization()(e_3)\n",
    "e_3 = LeakyReLU()(e_3)\n",
    "\n",
    "e_4 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_4 = BatchNormalization()(e_4)\n",
    "e_4 = LeakyReLU()(e_4)\n",
    "\n",
    "\n",
    "\n",
    "#Concatenate visions\n",
    "v_1 = e_2\n",
    "\n",
    "v_2_concat = concatenate([v_1, e_3])\n",
    "v_2 = Dense(256, activation='relu')(v_2_concat)\n",
    "\n",
    "v_3_concat = concatenate([v_1, v_2, e_4])\n",
    "v_3 = Dense(256, activation='relu')(v_3_concat)\n",
    "\n",
    "out_concat = concatenate([v_1, v_2, v_3])\n",
    "\n",
    "#Shared Inputs\n",
    "\n",
    "shared_input = Dense(bottleneck_dim)(out_concat)\n",
    "bottleneck = Dense(bottleneck_dim)(shared_input)\n",
    "\n",
    "# decoder  vision 1\n",
    "d_1 = Dense(X_embedd.shape[1])(bottleneck)\n",
    "d_1 = BatchNormalization()(d_1)\n",
    "d_1 = LeakyReLU()(d_1)\n",
    "dropout1 = Dropout(.2)(d_1)\n",
    "\n",
    "d_2 = Dense(X_embedd.shape[1])(dropout1)\n",
    "d_2 = BatchNormalization()(d_2)\n",
    "d_2 = LeakyReLU()(d_2)\n",
    "dropout2 = Dropout(.2)(d_2)\n",
    "\n",
    "d_v1 = Dense(X_embedd.shape[1])(dropout2)\n",
    "d_v1 = BatchNormalization()(d_v1)\n",
    "d_v1 = LeakyReLU()(d_v1)\n",
    "\n",
    "#decoder vision 2\n",
    "d_5 = Dense(X_features.shape[1])(bottleneck)\n",
    "d_5 = BatchNormalization()(d_5)\n",
    "d_5 = LeakyReLU()(d_5)\n",
    "dropout3 = Dropout(.2)(d_5)\n",
    "\n",
    "d_4 = Dense(X_embedd.shape[1])(dropout3)\n",
    "d_4 = BatchNormalization()(d_4)\n",
    "d_4 = LeakyReLU()(d_4)\n",
    "dropout4 = Dropout(.2)(d_4)\n",
    "\n",
    "d_v2 = Dense(X_features.shape[1])(dropout4)\n",
    "d_v2 = BatchNormalization()(d_v2)\n",
    "d_v2 = LeakyReLU()(d_v2)\n",
    "\n",
    "output_v1 = Dense(X_embedd.shape[1], activation='linear')(d_v1)\n",
    "output_v2 = Dense(X_features.shape[1], activation='linear')(d_v2)\n",
    "\n",
    "model = Model(inputs=[sequence_input, sequence_input2], outputs=[output_v1, output_v2])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=0.0001) ,loss=keras.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00a576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12153/12153 [==============================] - 938s 76ms/step - loss: 1.0446 - dense_30_loss: 0.1205 - dense_31_loss: 0.9241 - val_loss: 0.8112 - val_dense_30_loss: 0.0601 - val_dense_31_loss: 0.7511\n",
      "Epoch 2/5\n",
      "12153/12153 [==============================] - 922s 76ms/step - loss: 0.7777 - dense_30_loss: 0.0582 - dense_31_loss: 0.7196 - val_loss: 0.5764 - val_dense_30_loss: 0.0593 - val_dense_31_loss: 0.5171\n",
      "Epoch 3/5\n",
      "12153/12153 [==============================] - 940s 77ms/step - loss: 0.6968 - dense_30_loss: 0.0577 - dense_31_loss: 0.6392 - val_loss: 0.4760 - val_dense_30_loss: 0.0594 - val_dense_31_loss: 0.4165\n",
      "Epoch 4/5\n",
      "12153/12153 [==============================] - 943s 78ms/step - loss: 0.6613 - dense_30_loss: 0.0579 - dense_31_loss: 0.6034 - val_loss: 0.4543 - val_dense_30_loss: 0.0601 - val_dense_31_loss: 0.3942\n",
      "Epoch 5/5\n",
      "12153/12153 [==============================] - 703s 58ms/step - loss: 0.6433 - dense_30_loss: 0.0578 - dense_31_loss: 0.5854 - val_loss: 0.4273 - val_dense_30_loss: 0.0591 - val_dense_31_loss: 0.3682\n"
     ]
    }
   ],
   "source": [
    "one_hot_label = to_categorical(y_embedd)\n",
    "X_train_embedd, X_valid_embedd, y_train_embedd, y_valid_embedd = train_test_split(\n",
    "    X_embedd, y_embedd, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "one_hot_label = to_categorical(y_features)\n",
    "X_train_features, X_valid_features, y_train_features, y_valid_features = train_test_split(\n",
    "    X_features, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "history = model.fit(\n",
    "\tx=[X_train_embedd, X_train_features], y=[X_train_embedd, X_train_features],\n",
    "        epochs=5, validation_data=([X_valid_embedd, X_valid_features], [X_valid_embedd, X_valid_features]),\n",
    "    shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec5a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12153/12153 [==============================] - 1286s 106ms/step - loss: 0.6597 - dense_30_loss: 0.0576 - dense_31_loss: 0.6021 - val_loss: 0.4177 - val_dense_30_loss: 0.0591 - val_dense_31_loss: 0.3586\n",
      "Epoch 2/5\n",
      "12153/12153 [==============================] - 1596s 131ms/step - loss: 0.6466 - dense_30_loss: 0.0575 - dense_31_loss: 0.5891 - val_loss: 0.4218 - val_dense_30_loss: 0.0583 - val_dense_31_loss: 0.3635\n",
      "Epoch 3/5\n",
      "12153/12153 [==============================] - 2347s 193ms/step - loss: 0.6432 - dense_30_loss: 0.0574 - dense_31_loss: 0.5858 - val_loss: 0.4449 - val_dense_30_loss: 0.0583 - val_dense_31_loss: 0.3866\n",
      "Epoch 4/5\n",
      "12153/12153 [==============================] - 1806s 149ms/step - loss: 0.6345 - dense_30_loss: 0.0573 - dense_31_loss: 0.5772 - val_loss: 0.4644 - val_dense_30_loss: 0.0588 - val_dense_31_loss: 0.4055\n",
      "Epoch 5/5\n",
      "12153/12153 [==============================] - 1946s 160ms/step - loss: 0.6318 - dense_30_loss: 0.0572 - dense_31_loss: 0.5746 - val_loss: 0.4527 - val_dense_30_loss: 0.0591 - val_dense_31_loss: 0.3936\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tx=[X_train_embedd, X_train_features], y=[X_train_embedd, X_train_features],\n",
    "\tepochs=5, validation_data=([X_valid_embedd, X_valid_features], [X_valid_embedd, X_valid_features]),\n",
    "    shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c30458",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[sequence_input, sequence_input2], outputs=bottleneck)\n",
    "encoder.save('autoencoder_{}dim/encoder_{}.h5'.format(bottleneck_dim, section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd1ce39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open('autoencoder_{}dim/autoencoder_{}.json'.format(bottleneck_dim, section), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights('autoencoder_{}dim/autoencoder_{}.h5'.format(bottleneck_dim, section))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfa347",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46541778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9edc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(range(0, 383))\n",
    "columns = list(map(str, columns))\n",
    "\n",
    "sections = ['introduction', 'materials', 'conclusion']\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc6937bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Write dataset\n"
     ]
    }
   ],
   "source": [
    "for section in sections:\n",
    "    \n",
    "    X_features_test = dataset[section][1]\n",
    "    X_features_train = dataset[section][0]\n",
    "    \n",
    "    X_embedd_test = pd.read_csv(\"dataset/embed_bert_{}_test.csv\".format(section))\n",
    "    X_embedd_train = pd.read_csv(\"dataset/embed_bert_{}_train.csv\".format(section))\n",
    "\n",
    "    X_embedd_test = X_embedd_test[columns]\n",
    "    X_embedd_train = X_embedd_train[columns]\n",
    "\n",
    "    encoder = load_model('autoencoder_oneoutput64dim/encoder_{}.h5'.format(section))\n",
    "\n",
    "    X_test_encode = encoder.predict([X_embedd_test, X_features_test])\n",
    "    X_train_encode = encoder.predict([X_embedd_train, X_features_train])\n",
    "\n",
    "    dataset[section][6] = X_train_encode\n",
    "    dataset[section][7] = X_test_encode\n",
    "\n",
    "if verbose:\n",
    "    print(\"Write dataset\")\n",
    "    \n",
    "    with open('autoencoder_oneoutput64dim/dataset_{}.pkl'.format('features'), 'wb') as fp:\n",
    "        pickle.dump(dataset, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7d38fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84e54fb2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da99cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoder/history_{}.pkl'.format(section), 'wb') as fp:\n",
    "    pickle.dump(history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb6b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477f1f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder vision 1\n",
    "sequence_input = Input(shape=(X_embedd.shape[1],), dtype='int32')\n",
    "\n",
    "e_1 = Dense(X_embedd.shape[1]*2)(sequence_input)\n",
    "e_1 = BatchNormalization()(e_1)\n",
    "e_1 = LeakyReLU()(e_1)\n",
    "\n",
    "e_2 = Dense(X_embedd.shape[1])(e_1)\n",
    "e_2 = BatchNormalization()(e_2)\n",
    "e_2 = LeakyReLU()(e_2)\n",
    "\n",
    "\n",
    "#encoder vision 2\n",
    "sequence_input2 = Input(shape=(X_features.shape[1],), dtype='int32')\n",
    "\n",
    "e_3 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_3 = BatchNormalization()(e_3)\n",
    "e_3 = LeakyReLU()(e_3)\n",
    "\n",
    "e_4 = Dense(X_embedd.shape[1])(e_3)\n",
    "e_4 = BatchNormalization()(e_4)\n",
    "e_4 = LeakyReLU()(e_4)\n",
    "\n",
    "e_5 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_5 = BatchNormalization()(e_5)\n",
    "e_5 = LeakyReLU()(e_5)\n",
    "\n",
    "e_6 = Dense(X_embedd.shape[1])(e_5)\n",
    "e_6 = BatchNormalization()(e_6)\n",
    "e_6 = LeakyReLU()(e_6)\n",
    "\n",
    "\n",
    "#Concatenate visions\n",
    "v_1 = e_2\n",
    "\n",
    "v_2_concat = concatenate([v_1, e_4])\n",
    "v_2 = Dense(256, activation='relu')(v_2_concat)\n",
    "\n",
    "v_3_concat = concatenate([v_1, v_2, e_6])\n",
    "v_3 = Dense(256, activation='relu')(v_3_concat)\n",
    "\n",
    "out_concat = concatenate([v_1, v_2, v_3])\n",
    "\n",
    "#Shared Inputs\n",
    "\n",
    "shared_input = Dense(bottleneck_dim)(out_concat)\n",
    "bottleneck = Dense(bottleneck_dim)(shared_input)\n",
    "\n",
    "#decoder vision 2\n",
    "d_5 = Dense(X_features.shape[1])(bottleneck)\n",
    "d_5 = BatchNormalization()(d_5)\n",
    "d_5 = LeakyReLU()(d_5)\n",
    "dropout2 = Dropout(.2)(d_5)\n",
    "\n",
    "d_v2 = Dense(X_features.shape[1])(dropout2)\n",
    "d_v2 = BatchNormalization()(d_v2)\n",
    "d_v2 = LeakyReLU()(d_v2)\n",
    "\n",
    "#output_v1 = Dense(X_embedd.shape[1], activation='linear')(d_v1)\n",
    "output_v2 = Dense(X_features.shape[1], activation='linear')(d_v2)\n",
    "\n",
    "model = Model(inputs=[sequence_input, sequence_input2], outputs=output_v2)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=0.0001) ,loss=keras.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f01943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12153/12153 [==============================] - 921s 76ms/step - loss: 0.7083 - val_loss: 0.4429\n",
      "Epoch 2/5\n",
      "12153/12153 [==============================] - 917s 75ms/step - loss: 0.6776 - val_loss: 0.4184\n",
      "Epoch 3/5\n",
      "12153/12153 [==============================] - 926s 76ms/step - loss: 0.6595 - val_loss: 0.4434\n",
      "Epoch 4/5\n",
      "12153/12153 [==============================] - 961s 79ms/step - loss: 0.6473 - val_loss: 0.3444\n",
      "Epoch 5/5\n",
      "12153/12153 [==============================] - 984s 81ms/step - loss: 0.6417 - val_loss: 0.4115\n"
     ]
    }
   ],
   "source": [
    "one_hot_label = to_categorical(y_embedd)\n",
    "X_train_embedd, X_valid_embedd, y_train_embedd, y_valid_embedd = train_test_split(\n",
    "    X_embedd, y_embedd, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "one_hot_label = to_categorical(y_features)\n",
    "X_train_features, X_valid_features, y_train_features, y_valid_features = train_test_split(\n",
    "    X_features, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "history= model.fit(\n",
    "\tx=[X_train_embedd, X_train_features], y=X_train_features,\n",
    "\tepochs=5, validation_data=([X_valid_embedd, X_valid_features], X_valid_features),\n",
    "    shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68191ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[sequence_input, sequence_input2], outputs=bottleneck)\n",
    "encoder.save('autoencoder_oneoutput{}dim/encoder_{}.h5'.format(bottleneck_dim, section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031f626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open('autoencoder_oneoutput{}dim/autoencoder_{}.json'.format(bottleneck_dim, section), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights('autoencoder_oneoutput{}dim/autoencoder_{}.h5'.format(bottleneck_dim, section))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255c294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
