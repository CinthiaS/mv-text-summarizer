{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec32170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, concatenate, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874dfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "#from src import ensemble_tree_models\n",
    "from src import tunning_hyperparametrs as th\n",
    "from src import mlp_classifier\n",
    "#from src import summarization\n",
    "#from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from src import pipeline_extract_features as pef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a44768",
   "metadata": {},
   "outputs": [],
   "source": [
    "section='concat'\n",
    "\n",
    "with open('dataset/dataset_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "X_features = dataset[section][0]\n",
    "y_features = dataset[section][2]\n",
    "\n",
    "columns = list(range(0, 383))\n",
    "columns = list(map(str, columns))\n",
    "\n",
    "folder_to_save = 'models_v1'\n",
    "path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(folder_to_save)\n",
    "\n",
    "X_embedd = pd.read_csv(\"dataset/embed_bert_{}_train.csv\".format(section))\n",
    "\n",
    "y_embedd = X_embedd['label']\n",
    "X_embedd = X_embedd[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053dd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf29b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder vision 1\n",
    "sequence_input = Input(shape=(X_embedd.shape[1],), dtype='int32')\n",
    "\n",
    "e_1 = Dense(X_embedd.shape[1]*2)(sequence_input)\n",
    "e_1 = BatchNormalization()(e_1)\n",
    "e_1 = LeakyReLU()(e_1)\n",
    "\n",
    "e_2 = Dense(X_embedd.shape[1])(e_1)\n",
    "e_2 = BatchNormalization()(e_2)\n",
    "e_2 = LeakyReLU()(e_2)\n",
    "\n",
    "\n",
    "#encoder vision 2\n",
    "sequence_input2 = Input(shape=(X_features.shape[1],), dtype='int32')\n",
    "\n",
    "e_3 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_3 = BatchNormalization()(e_3)\n",
    "e_3 = LeakyReLU()(e_3)\n",
    "\n",
    "e_4 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "e_4 = BatchNormalization()(e_4)\n",
    "e_4 = LeakyReLU()(e_4)\n",
    "\n",
    "\n",
    "#Concatenate visions\n",
    "v_1 = e_2\n",
    "\n",
    "v_2_concat = concatenate([v_1, e_3])\n",
    "v_2 = Dense(256, activation='relu')(v_2_concat)\n",
    "\n",
    "v_3_concat = concatenate([v_1, v_2, e_4])\n",
    "v_3 = Dense(256, activation='relu')(v_3_concat)\n",
    "\n",
    "out_concat = concatenate([v_1, v_2, v_3])\n",
    "\n",
    "#Shared Inputs\n",
    "\n",
    "shared_input = Dense(bottleneck_dim)(out_concat)\n",
    "bottleneck = Dense(bottleneck_dim)(shared_input)\n",
    "\n",
    "# decoder  vision 1\n",
    "d_1 = Dense(X_embedd.shape[1])(bottleneck)\n",
    "d_1 = BatchNormalization()(d_1)\n",
    "d_1 = LeakyReLU()(d_1)\n",
    "dropout1 = Dropout(.2)(d_1)\n",
    "\n",
    "d_2 = Dense(X_embedd.shape[1])(dropout1)\n",
    "d_2 = BatchNormalization()(d_2)\n",
    "d_2 = LeakyReLU()(d_2)\n",
    "dropout2 = Dropout(.2)(d_2)\n",
    "\n",
    "d_v1 = Dense(X_embedd.shape[1])(dropout2)\n",
    "d_v1 = BatchNormalization()(d_v1)\n",
    "d_v1 = LeakyReLU()(d_v1)\n",
    "\n",
    "#decoder vision 2\n",
    "d_5 = Dense(X_features.shape[1])(bottleneck)\n",
    "d_5 = BatchNormalization()(d_5)\n",
    "d_5 = LeakyReLU()(d_5)\n",
    "dropout3 = Dropout(.2)(d_5)\n",
    "\n",
    "d_4 = Dense(X_embedd.shape[1])(dropout3)\n",
    "d_4 = BatchNormalization()(d_4)\n",
    "d_4 = LeakyReLU()(d_4)\n",
    "dropout4 = Dropout(.2)(d_4)\n",
    "\n",
    "d_v2 = Dense(X_features.shape[1])(dropout4)\n",
    "d_v2 = BatchNormalization()(d_v2)\n",
    "d_v2 = LeakyReLU()(d_v2)\n",
    "\n",
    "output_v1 = Dense(X_embedd.shape[1], activation='linear')(d_v1)\n",
    "output_v2 = Dense(X_features.shape[1], activation='linear')(d_v2)\n",
    "\n",
    "model = Model(inputs=[sequence_input, sequence_input2], outputs=[output_v1, output_v2])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(\n",
    "                learning_rate=0.0001) ,loss=keras.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23058/23058 [==============================] - 2635s 114ms/step - loss: 0.9704 - dense_14_loss: 0.0882 - dense_15_loss: 0.8822 - val_loss: 0.5330 - val_dense_14_loss: 0.0592 - val_dense_15_loss: 0.4739\n",
      "Epoch 2/10\n",
      "23058/23058 [==============================] - 2538s 110ms/step - loss: 0.6772 - dense_14_loss: 0.0566 - dense_15_loss: 0.6206 - val_loss: 0.4503 - val_dense_14_loss: 0.0589 - val_dense_15_loss: 0.3913\n",
      "Epoch 3/10\n",
      "23058/23058 [==============================] - 2410s 104ms/step - loss: 0.6829 - dense_14_loss: 0.0564 - dense_15_loss: 0.6264 - val_loss: 0.4181 - val_dense_14_loss: 0.0586 - val_dense_15_loss: 0.3596\n",
      "Epoch 4/10\n",
      "23058/23058 [==============================] - 2807s 122ms/step - loss: 0.6319 - dense_14_loss: 0.0563 - dense_15_loss: 0.5756 - val_loss: 0.4394 - val_dense_14_loss: 0.0594 - val_dense_15_loss: 0.3800\n",
      "Epoch 5/10\n",
      "15674/23058 [===================>..........] - ETA: 11:23 - loss: 0.6276 - dense_14_loss: 0.0560 - dense_15_loss: 0.5716 ETA: 11:02 - loss: 0.6276 - dense_14_loss: 0.0560 - dense_1"
     ]
    }
   ],
   "source": [
    "one_hot_label = to_categorical(y_embedd)\n",
    "X_train_embedd, X_valid_embedd, y_train_embedd, y_valid_embedd = train_test_split(\n",
    "    X_embedd, y_embedd, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "one_hot_label = to_categorical(y_features)\n",
    "X_train_features, X_valid_features, y_train_features, y_valid_features = train_test_split(\n",
    "    X_features, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "history = model.fit(\n",
    "\tx=[X_train_embedd, X_train_features], y=[X_train_embedd, X_train_features],\n",
    "\tepochs=10, validation_data=([X_valid_embedd, X_valid_features], [X_valid_embedd, X_valid_features]),\n",
    "    shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c30458",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[sequence_input, sequence_input2], outputs=bottleneck)\n",
    "encoder.save('autoencoder_{}dim/encoder_{}.h5'.format(bottleneck_dim, section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b7fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open('autoencoder_{}dim/autoencoder_{}.json'.format(bottleneck_dim, section), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights('autoencoder_{}dim/autoencoder_{}.h5'.format(bottleneck_dim, section))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224dabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoder_16dim/history_{}.pkl'.format(section), 'wb') as fp:\n",
    "    pickle.dump(history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e98209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
