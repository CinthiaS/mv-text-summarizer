{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc25715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "#import smogn\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from pysbd.utils import PySBDFactory\n",
    "import math\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#from tensorflow.keras.models import model_from_json\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "#import en_core_web_sm\n",
    "#nlp_md = en_core_web_sm.load()\n",
    "\n",
    "#import en_core_web_md\n",
    "#nlp_md = en_core_web_md.load()\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "#import en_core_web_sm\n",
    "#nlp_md = en_core_web_sm.load()y\n",
    "\n",
    "#import en_core_web_md\n",
    "#nlp_md = en_core_web_md.load()\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = \"/scratch/cinthiasouza/mv-text-summarizer\"\n",
    "path_to_read=\"/scratch/cinthiasouza/mv-text-summarizer/result_/{}/{}_*.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5dea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "from src import ensemble_tree_models\n",
    "from src import tunning_hyperparametrs as th\n",
    "#from src import mlp_regressor\n",
    "#from src import mlp_classifier\n",
    "from src import summarization\n",
    "from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from src import pipeline_extract_features as pef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49cba1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, InputLayer, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d9b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset6_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords', 'tf-isf', 'pos_score', 'ner_score']\n",
    "\n",
    "sections=['introduction', 'materials', 'conclusion']\n",
    "\n",
    "folder_to_save = 'dataset6'\n",
    "path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(folder_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89046a",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98febf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(sentence):\n",
    "    \n",
    "    try:\n",
    "        doc = nlp_md(sentence)\n",
    "        embeddings = doc.vector\n",
    "    except TypeError:\n",
    "        embeddings = [0] *300\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3523ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6908ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"introduction\"\n",
    "\n",
    "features = dataset[section][4][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = RandomUnderSampler(random_state=47).fit_resample(features[['sentences', 'articles', 'rouge_1']],\n",
    "                         features['bin'])\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in X_train.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = y_train\n",
    "\n",
    "df.to_csv(\"dataset6/embed_{}_train.csv\".format (section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a3bea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"materials\"\n",
    "\n",
    "features = dataset[section][4][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = RandomUnderSampler(random_state=47).fit_resample(\n",
    "    features[['sentences', 'articles', 'rouge_1']],features['bin'])\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in X_train.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = y_train\n",
    "\n",
    "df.to_csv(\"{}/embed_{}_train.csv\".format (path_to_save, section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d6733324",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"conclusion\"\n",
    "\n",
    "features = dataset[section][4][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "X_train, y_train = RandomUnderSampler(random_state=47).fit_resample(features[['sentences', 'articles', 'rouge_1']],\n",
    "                                                     features['bin'])\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in X_train.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = y_train\n",
    "\n",
    "df.to_csv(\"{}/embed_{}_train.csv\".format (path_to_save, section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860670f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc503b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"introduction\"\n",
    "\n",
    "features = dataset[section][5][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad36da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"introduction\"\n",
    "\n",
    "features = dataset[section][5][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in features.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = features['bin']\n",
    "\n",
    "df.to_csv(\"{}/embed_{}_test.csv\".format (path_to_save, section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "792e4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"materials\"\n",
    "\n",
    "features = dataset[section][5][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in features.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = features['bin']\n",
    "\n",
    "df.to_csv(\"{}/embed_{}_test.csv\".format (path_to_save, section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ca2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = \"conclusion\"\n",
    "\n",
    "features = dataset[section][5][['sentences', 'articles', 'rouge_1', 'bin']]\n",
    "features = features.reset_index(drop=True)\n",
    "\n",
    "embeddings = [create_embeddings(row['sentences']) for idx, row in features.iterrows()]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df['label'] = features['bin']\n",
    "\n",
    "df.to_csv(\"{}/embed_{}_test.csv\".format (path_to_save, section), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f1d1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}/embed_introduction_train.csv\".format(path_to_save), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704764b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95db89a6",
   "metadata": {},
   "source": [
    "# Classificação com embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910e08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbed6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode(text):\n",
    "\n",
    "    text = str(text).encode(\"ascii\", \"ignore\")\n",
    "    text = text.decode()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb1f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z]+\", ' ', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f5884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddae3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "section='conclusion'\n",
    "\n",
    "with open('dataset6_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "X_features = dataset[section][0]\n",
    "y_features = dataset[section][2]\n",
    "\n",
    "columns = list(range(0, 300))\n",
    "columns = list(map(str, columns))\n",
    "\n",
    "folder_to_save = 'models_v6'\n",
    "path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(folder_to_save)\n",
    "\n",
    "X_embedd = pd.read_csv(\"dataset6/embed_{}_train.csv\".format(section))\n",
    "\n",
    "y_embedd = X_embedd['label']\n",
    "X_embedd = X_embedd[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1879b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b98f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = to_categorical(y_embedd)\n",
    "X_train_embedd, X_valid_embedd, y_train_embedd, y_valid_embedd = train_test_split(\n",
    "    X_embedd, y_embedd, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "one_hot_label = to_categorical(y_features)\n",
    "X_train_features, X_valid_features, y_train_features, y_valid_features = train_test_split(\n",
    "    X_features, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f55dec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61120, 18)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentences.apply(remove_unicode)\n",
    "sentences = sentences.apply(clean_text)\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0669d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset[section][5]['sentences']\n",
    "sentences = sentences.apply(remove_unicode)\n",
    "sentences = sentences.apply(clean_text)\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(sentences)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 300\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in sentences]\n",
    "\n",
    "max_length = 500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5abf059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_train = padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f482951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61120, 500)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46b50f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_test = padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "539fbb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61120, 500)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8894b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195892 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "\n",
    "f = open(r'../glove.840B.300d.txt', \"r\", encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a1784fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 48328 is out of bounds for axis 0 with size 48328",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-12d36d67c884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 48328 is out of bounds for axis 0 with size 48328"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4981ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48328, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88241851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e8c3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1919/1919 [==============================] - 848s 441ms/step - loss: 0.6997 - accuracy: 0.5032\n",
      "Epoch 2/6\n",
      "1919/1919 [==============================] - 845s 440ms/step - loss: 0.6943 - accuracy: 0.5062\n",
      "Epoch 3/6\n",
      "1919/1919 [==============================] - 866s 451ms/step - loss: 0.6879 - accuracy: 0.5427\n",
      "Epoch 4/6\n",
      "1919/1919 [==============================] - 851s 443ms/step - loss: 0.5487 - accuracy: 0.7166\n",
      "Epoch 5/6\n",
      "1919/1919 [==============================] - 852s 444ms/step - loss: 0.2217 - accuracy: 0.9056\n",
      "Epoch 6/6\n",
      "1919/1919 [==============================] - 870s 454ms/step - loss: 0.0776 - accuracy: 0.9709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f066aa6d430>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, y_features, epochs=6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecca41d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61120,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = dataset[section][5]['bin']\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dc0fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910/1910 [==============================] - 295s 154ms/step - loss: 1.9350 - accuracy: 0.5017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9349699020385742, 0.5016524791717529]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(padded_docs, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
