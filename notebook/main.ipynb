{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mv-summarization': conda)"
  },
  "interpreter": {
   "hash": "5dd7fce1a8cb8d97b2536bbe54fd7faa274378c3acb961864d5bd989f2d52777"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/media/cinthia/Dados/Mestrado/mv-text-summarizer')\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from bs4 import BeautifulSoup\n",
    "from pysbd.utils import PySBDFactory\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Flatten, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import tunning_hyperparametrs as th\n",
    "from src import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introdução"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esse trabalho tem como objetivo explorar uma abordagem de sumarização extrativa de textos. A abordagem aqui utiliza modela o problema de sumarização de textos como um problema de regressão. A sumarização de textos tem como objetivo condensar as informações de um texto em um resumo simples e descritivo. Um dos benefícios da utilização dessa técnica é o de facilitar o acesso à informação, propiciando ao leitor uma visão geral dos textos sem a necessidade de analisar todo seu conteúdo. Com o aumento da quantidade de dados disponíveis nas bases digitais, a sumarização automática do conteúdo desses documentos torna-se cada vez mais importante. Contudo, gerar bons resumos é uma tarefa desafiante, pois envolve a compreensão da estrutura sintática e semântica do texto, a extração das informações mais relevantes e a elaboração de um resumo coerente e fluente. Compreendendo as dificuldades atreladas à tarefa e a sua importância nos dias atuais, o estudo e o desenvolvimento de modelos computacionais para sumarização automática de textos vem ganhando destaque. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapas Metodológicas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Coleta  dos dados\n",
    "\n",
    "---\n",
    "\n",
    "A primeira etapa é a coleta dos documentos completos em formato XML. Ao todo, foram coletados xxx documentos. Após a coleta, os documentos são submetidos a uma etapa de preparação dos dados. Nessa etapa é realizada uma refatoração dos arquivos XML a fim de corrigir inconsistências para facilitar o processo de segmentação. Após, todos os documentos são segmentados em quatro seções e convertidos para o formato texto. Ao final, tem-se um conjunto de dados $D = \\{D_{1}, D_{2}, ... D_{m}\\}$, onde cada documento possui quatro seções $D_{m} = \\{D_{m2}, D_{m3}, D_{m4}\\}$. A seção $D_{m1}$ é o abstract do texto que é utilizado como ground-truth. Cada seção é composta por um conjunto de parágrafos $P$, onde $P = \\{P_{1}, P_{2}, ... P_{j}\\}$, e um conjunto de sentenças $S$, onde $S = \\{S_{1}, S_{2}, ... S_{i}\\}$. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extração das Features\n",
    "\n",
    "---\n",
    "\n",
    "A terceira fase consiste na extração das features. Ao todo, foram extraídas 14 features, que são descritas a seguir.\n",
    "\n",
    "Posição da sentença no texto: Considera que a posição da sentença no texto permite identificar a sua importância. De acordo com ... as sentenças que ocorrem no inicio e no final do texto tendem a ser mais importantes, pois, em geral, os textos possuem uma estrutura hierárquica que contribui para que essas informações sejam colocadas nessa posição. A Equação apresentada a seguir define o calculo do score das sentenças.\n",
    "\n",
    "$SentPos(s_{i}) =$\n",
    "$$\n",
    "1,  se \\ pos(S_{i}) \\leq 3 \\\\\n",
    "1 - \\frac{i-3}{sent\\_count(D_{m})}, se \\ pos(S_{i}) > 3 \n",
    "$$\n",
    "    \n",
    "onde, a $pos(S_{i})$ representa a posição da sentença $i$ no documento e $sent\\_count(D_{m})$ representa a quantidade de sentenças no documento. \n",
    "\n",
    "Posição da sentença no parágrafo: De acordo com ..., a posição de uma sentença no parágrafo também é um indicativo de sua importância. Similar a feature de posição da sentença no texto, essa feature atribui uma importância para as sentenças considerando a sua posição no parágrafo. \n",
    "\n",
    "A Equação apresentada a seguir define o cálculo do score das sentenças.\n",
    "\n",
    "$SentPos(s_{i}) =$\n",
    "$$\n",
    "        1,  se \\ pos(S_{i}) \\leq 1 \\ or \\  pos(S_{i}) == sent-count(P_{j})\\\\\n",
    "        1 - \\frac{i-2}{sent-count(P_{j})}, se \\ pos(s_{i}) > 1 \\ or \\  pos(S_{i}) == sent-count(P_{j})\n",
    "$$\n",
    "    \n",
    "onde, $P_{j}$ representa um parágrafo do texto e $s_{i}$ representa as sentenças do parágrafo, $pos(s_{i})$ representa a posição da sentença $i$ no parágrafo e $sent-count(P_{j})$ representa a quantidade de sentenças no parágrafo. \n",
    "\n",
    "Name Entity (NE): De acordo com ..., sentenças que se referem a pessoas, objetos, localização geográfica e tempo, por exemplo, podem conter mais informações importantes. Sendo assim, defini-se o $NE_{score}$ de uma sentença como:\n",
    "\n",
    "\n",
    "$$NE_{score} = \\frac{Count(NE_{(s_{i})})}{Max(Count(NE(D_{m}))},$$\n",
    "\n",
    "onde, $NE$ retorna todas as name entity da sentença $i$ e $Count(NE(S_{i}))$ representa a quantidade de $NEs$ na sentença $i$, e $Max(Count(NE(D_{m})))$ representa o valor máximo de $NEs$ em uma sentença, considerando todas as sentenças do documento $D_{m}D$.\n",
    "\n",
    "Part-Of-Speech (POS): ..., afirma que algumas classes de palavras desempenham um papel mais importante nas sentenças. De acordo com os autores, os substantivos e verbos são informações básicas de uma sentença. O substantivo pode desempenhar o papel de sujeito, objeto e complemento em uma sentença e o verbo permite identificar as orações das sentenças. Sendo assim, quanto maior a quantidade de substantivos e verbos em uma sentença, mais informações ela traz. Os autores definem o score das sentenças utilizando a equação $POS_{score}$ de uma sentença como:\n",
    "\n",
    "$$POS_{score} = \\frac{Count(POS_{(s_{i})})}{Max(Count(POS(D_{m}))},$$\n",
    "\n",
    "onde, $POS$ retorna todos os substantivos e verbos da sentença. Nesse trabalho, o método $POS$ retorna também adjetivos e advérbios. $Count(POS(S_{i}))$ representa a quantidade de $POS$ na sentença $i$, e $Max(Count(POS(D_{m})))$ representa o valor máximo de $POS$ em uma sentença, considerando todas as sentenças do documento $D_{m}$.\n",
    "\n",
    "- Palavras-chave: Essa feature ranqueia as sentenças de acordo com a quantidade de palavras-chave contidas nessa sentença. Para isso, são extraídos $x$ conjuntos de $k$ palavras-chave do texto considerando todas as sentenças. Cada conjunto é composto por n-grams com diferentes valores $n$ tal que, $n \\in \\{1, 2, 3\\}$. Após, as sentenças são ponderas considerando a quantidade de n-grams que ocorre em cada sentença. Além das palavras-chave selecionadas por um algoritmo de extração de palavras-chaves, são utilizadas também as palavras-chaves definidas pelo autor do artigo. A hipótese testada é de que uma sentença que possui uma maior quantidade de palavras-chave é mais importante que uma sentença que possui poucas palavras chaves. \n",
    "\n",
    "$$ Keywords_{score} = CountMatch(Keywords(D_{m}, x), s_{i}), \\forall i \\in D_{m}$$\n",
    "\n",
    "onde, $Keywords(D_{m}, x)$ retorna todas as palavras-chave do documento $D_{m}$, $CountMatch$ retorna a quantidade de palavras-chave na sentença $i$.\n",
    "\n",
    "- Citação: Trabalhos como os de ..., afirmam que as citações representam um indicativo de embasamento das sentenças. Sendo assim, sentenças com uma maior quantidade de citação podem apresentar informações importantes. Essa feature, atribui um score para cada sentença baseado na quantidade de citações que ela possui. \n",
    "\n",
    "- Term Frequency - Inverse Sentence Frequency (TF-ISF): É uma medida estatística baseada no Term Frequency - Inverse Document Frequency (TF-IDF) que calcula a importância de uma palavra para um conjunto de sentenças. Para essa medida, quando uma palavra ocorre com frequência em uma sentença, o valor de TF-ISF aumenta proporcionalmente. No entanto, se essa palavra possui uma alta frequência em todas as sentenças, então o valor de TF-ISF dessa palavra diminuí. \n",
    "\n",
    "$$ TF-ISF (W, S) = TF(W, S) \\times ISF(W),$$\n",
    "\n",
    "$$ ISF(W) = \\frac{log|S|}{SF(W)},$$\n",
    "\n",
    "onde, $TF(W, S)$ representa a frequência da palavra $W$ na sentença $S$ e $ISF$ é a razão entre o $log$ da quantidade de sentenças pela frequência do termo $W$, representado por $SF(W)$. Para definir o valor de TF-ISF da sentença é calculada a média dos valores de TF-ISF das sentenças. \n",
    "\n",
    "- Centralidade: Essa feature pondera as sentenças de acordo com a sua representatividade semântica em relação a todas as outras sentenças de $D_{m}$. Inicialmente, todas as sentenças são representadas como vetores de embeddings. Após, todas as sentenças são agrupadas. O centroide de cada cluster é considerado a sentença que melhor representa, semanticamente, todas as outras sentenças do cluster. Após, é calculada a distância do cosseno entre cada sentença do cluster e seu centroid. A hipótese aqui é de que quanto mais próxima uma sentença é do centroid melhor ela representa todas as outras sentenças do cluster. Cada cluster representa um tópico e outliers possuem pesos de representatividade 0. O valor de score de cada sentença é definido utilizando as equações x, y:\n",
    "\n",
    "$$ labels, centroid = Cluster(S),$$\n",
    "\n",
    "$$ Centrality_{score} = cossine\\_distance(label, centroid_{j})$$\n",
    "\n",
    "onde, $Cluster(S)$ retorna o resultado da clusterização de todas as sentenças do texto, representado pela variável label, e o centroide de cada cluster. \n",
    "\n",
    "Tamanho da sentença: De acordo com ..., o tamanho da sentença é uma feature importante para o ranqueamento de sentenças. Sentenças curtas, em geral, não capturam todas as informações importantes do texto e sentenças muito longas tendem a conter informações muito específicas e/ou redundantes. A  Equação apresentada a seguir define o cálculo dessa feature.\n",
    "\n",
    "$$ Len_{score}(S_{i}) = \\frac{Len(S_{i})}{Max\\_len(D_{m})}$$\n",
    "\n",
    "onde, $Len(S_{i})$ retorna o tamanho da sentença $i$ e $Max\\_len(D_{m})$ retorna o tamanho da maior sentença de $D_{m}$.\n",
    "\n",
    "LexRank: É um algoritmo de sumarização baseado em grafo, onde os vértices representam as sentenças e as arestas representam a similaridade entre as sentenças. Basicamente, o algoritmo representa as sentenças como um vetor de TF-IDF e calcula a sentença centroide que é a média dos vetores encontra. Após, o algoritmo ranqueia as sentenças do texto com base na similaridade das sentenças com o centroide. Para calcular a similaridade, o algoritmo utiliza a similaridade do cosseno. A partir do grafo gerado, utiliza-se o algoritmo PageRank para ranquear as sentenças do texto.\n",
    "\n",
    "TextRank: É um algoritmo de sumarização baseado em grafos, similar ao LexRank. A diferença entre o LexRank e o TextRank, é que o TextRank calcula a similaridade entre as sentenças utilizando uma medida de sobreposição de palavras, normalizada pelo tamanho da sentença.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparação dos Dados\n",
    "\n",
    "-----\n",
    "\n",
    "A etapa de preparação dos dados envolveu a divisão do conjunto de dados em treino, teste e validação e a normalização dos dados. Além disso, foi realizado um trabalho árduo de experimentação até chegar a modelagem aqui apresentada. Inicialmente, o objetivo era trabalhar com um problema de classificação. Em geral, na literatura, os modelos de sumarização extrativa que utilizam aprendizado supervisionado modelam o problema de sumarização extrativa como um problema de classificação binária. Contudo, durantes os experimentos, foi encontrada uma grande dificuldade no ajuste dos modelos devido ao desbalanceamento dos dados. Sendo assim, a segunda alternativa foi a modelagem como um problema de regressão. Como o objetivo aqui é apenas identificar o melhor conjunto de sentenças para o resumo,  acredita-se que modelar o problema como um problema de regressão é um estratégia viável. \n",
    "\n",
    "O rótulo da sentença foi criado calculando o score de ROUGE-1 de cada sentença da seção $D_{mj}$, com todas as sentenças da seção $D_{m1}$. A seção $D_{m1}$, representa o abstract do documento, que foi escrito por um especialista, denominado aqui de resumo referência. Se o resumo referência possui 5 sentenças e a seção $D_{mj}$ possui 40 sentenças, esse processo irá gerar uma lista de com 40 itens onde cada item possui uma lista de com 5 valores de scores de ROUGE-1. Posteriormente, na segunda etapa, é utilizada a métrica Máximo, essa métrica define que o score das sentenças é o score máximo da sentença com todas as outras sentenças do resumo referência. Assim sendo, dado que $S_{1} \\in D_{mj}$, então $score_{S_{1}} = max(\\{score_{1}, ..., score_{5}\\})$, onde $score_{1}$ representa o score entre $S_{1}$ e a primeira sentença do resumo referência. Esse processo tem como objetivo definir, para cada seção, um conjunto de scores para suas sentenças baseado na sua similaridade com o resumo referência.\n",
    "\n",
    "Definir o rótulo também foi um grande desafio. O conjunto de métricas ROUGE é largamente utilizado na literatura para avaliar modelos de sumarização. As três principais métricas desse conjunto que são utilizadas na avaliação de resumos são as métricas ROUGE-1, ROUGE-2 e ROUGE-L. As métricas ROUGE-1 e ROUGE-2 avaliam as sentenças com base na sobreposição de n-grams de tamanho 1 e 2, respectivamente. A métrica ROUGE-L avalia as sentenças utilizando o conceito de Longest Common Substring (LCS). Essa métrica diz que quanto maior a sobreposição de LCS entre duas sentenças mais similares elas são.Inicialmente, os experimentos foram realizadas utilizando a métrica ROUGE-2 ao invés da métrica ROUGE-1, como apresentado anteriorme. Contudo, com a utilização da métrica ROUGE-2 a distribuição dos scores das sentenças eram valores muito baixos e, em sua maioria, zero. Nesse contexto, o ajuste dos modelos estava bastante complexo. Sendo assim, optou-se por utilizar a métrica ROUGE-1 que possui uma melhor distribuição de scores."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treinamento dos Algoritmos de Regressão\n",
    "\n",
    "---\n",
    "\n",
    "A quarta fase consiste no treinamento dos algoritmos de regressão. Foram testados os algoritmos 3 algoritmos: Random Forest (RF), Gradient Boosting (GB) e uma rede MultiLayer Perceptron (MLP). Cada algoritmo recebe como entrada a matriz de features e o conjunto de rótulos de cada seção. A matriz de features é normalizada usando o método StandartScale, disponível na biblioteca Scikit Learn. \n",
    "\n",
    "Para definir os hiperparâmetros dos modelos RF e GB, foi utilizado o k-fold cross-validation com k = 5. Para cada classificador são avaliados diferentes hiperparâmetros e os resultados obtidos são avaliados utilizando a métrica mean square error e mean absolute error. Todo esse processo foi realizado com auxílio do Randomized Search, disponível, também , na biblioteca Scikit Learn. Após esse processo, a base de dados é dividida em treinamento e teste e os hiperparâmetros obtidos são utilizado para treinar o modelo utilizando o conjunto de treino. Posteriormente, os resultados obtidos são avaliados na base de teste. Para a rede neural..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experimentos e Resultados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extração das Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def extract_features_batches(batches, path_base, save_batches=500, verbose=True):\n",
    "\n",
    "    all_scores_intro =[ ]\n",
    "    all_features_intro = []\n",
    "    all_scores_mat =[ ]\n",
    "    all_features_mat = []\n",
    "    all_scores_conc =[ ]\n",
    "    all_features_conc = []\n",
    "\n",
    "    files = []\n",
    "    cont = 0\n",
    "    count_texts = 0\n",
    "    count_features = 0\n",
    "  \n",
    "    for batch in batches:\n",
    "\n",
    "        if verbose:\n",
    "            \n",
    "            print(\"Batch: {} \\n\".format(cont))\n",
    "    \n",
    "        section_1, section_2, section_3, section_4, keywords = utils.load_batches(\n",
    "            batch, path_base)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Iniciando a extração de features...\")\n",
    "            print(\"Total de arquivos: {} \\n\".format(len(section_1)))\n",
    "\n",
    "        for i in range(len(section_1)):\n",
    "\n",
    "            features_intro, scores_intro = extract_features_file(\n",
    "                    section=section_2[i], reference=section_1[i], keywords=keywords[i],\n",
    "                    number_text=count_texts, verbose=True)\n",
    "            \n",
    "            features_mat, scores_mat = extract_features_file(\n",
    "                    section=section_3[i], reference=section_1[i], keywords=keywords[i],\n",
    "                    number_text=count_texts, verbose=True)\n",
    "\n",
    "            features_conc, scores_conc = extract_features_file(\n",
    "                    section=section_4[i], reference=section_1[i], keywords=keywords[i],\n",
    "                    number_text=count_texts, verbose=True)\n",
    "\n",
    "            if not((features_intro.empty) or (scores_intro.empty)) and not((features_mat.empty) or (scores_mat.empty)) and not((features_conc.empty) or (scores_conc.empty)):\n",
    "\n",
    "                if (features_intro.shape[0] == scores_intro.shape[0]) and (features_mat.shape[0] == scores_mat.shape[0]) and (features_conc.shape[0] == scores_conc.shape[0]):\n",
    "                    \n",
    "                    all_scores_intro.append(scores_intro)\n",
    "                    all_features_intro.append(features_intro)\n",
    "\n",
    "                    all_scores_mat.append(scores_mat)\n",
    "                    all_features_mat.append(features_mat)\n",
    "    \n",
    "                    all_scores_conc.append(scores_conc)\n",
    "                    all_features_conc.append(features_conc)\n",
    "\n",
    "                    count_features+=1\n",
    "\n",
    "            if (i % save_batches == 0) and (i !=0):\n",
    "                print(\"Quantidade de arquivos processados: {}\".format(count_features))\n",
    "                print(\"Saving Results\")\n",
    "\n",
    "                utils.save_results(all_features_intro, all_scores_intro, number_text=count_texts,batch=cont, name_section='introduction', verbose=False)\n",
    "\n",
    "                utils.save_results(all_features_mat, all_scores_mat, number_text=count_texts,batch=cont, name_section='materials', verbose=False)\n",
    "                \n",
    "                utils.save_results(all_features_conc, all_scores_conc, number_text=count_texts,batch=cont, name_section='conclusion', verbose=False)\n",
    "                \n",
    "                all_scores_intro =[]\n",
    "                all_features_intro = []\n",
    "\n",
    "                all_scores_mat =[ ]\n",
    "                all_features_mat = []\n",
    "    \n",
    "                all_scores_conc =[ ]\n",
    "                all_features_conc = []\n",
    "\n",
    "\n",
    "            count_texts+=1\n",
    "        \n",
    "        cont+=1\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def extract_features_file(section, reference, keywords, number_text, verbose=False):\n",
    "  \n",
    "  xml = preprocess.format_xml(str(section))\n",
    "  text = preprocess.format_text(str(section), post_processing=False)\n",
    "  reference = preprocess.format_text(str(reference), post_processing=True)\n",
    "\n",
    "\n",
    "  bibs = extract_features.get_citations(xml)\n",
    "  text = preprocess.replace_bib(text, bibs)\n",
    "  text = preprocess.format_text(text, post_processing=True)\n",
    "\n",
    "  soup = BeautifulSoup(text)\n",
    "  text = soup.get_text()\n",
    "\n",
    "  sentences = tokenizer.split_sentences([text])\n",
    "  sentences = list(map(str, sentences[0]))\n",
    "  sentences = preprocess.format_sentences(sentences)\n",
    "\n",
    "  try:\n",
    "\n",
    "    features = create_features_df.main(\n",
    "    sentences, xml, keywords, nlp_sm, nlp_md)\n",
    "    features_df = create_features_df.format_df (sentences, features)\n",
    "    features_df['number_text'] = [number_text]*len(features_df)\n",
    "\n",
    "    sentences_ref = tokenizer.split_sentences([reference])\n",
    "    sentences_ref = list(map(str, sentences_ref[0]))\n",
    "\n",
    "    scores_df, label = transform_data.main_create_label(sentences, sentences_ref, rouge)\n",
    "    scores_df['label'] = label\n",
    "    scores_df['number_text'] = [number_text]*len(scores_df)\n",
    "\n",
    "    return features_df, scores_df\n",
    "\n",
    "  except IndexError as error:\n",
    "    return pd.DataFrame(), pd.DataFrame()\n",
    "  except ValueError as error:\n",
    "    return pd.DataFrame(), pd.DataFrame()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "if os.path.isfile('batches.json'):\n",
    "\n",
    "    print(\"Load Batch Files\")\n",
    "    with open('batches.json') as f:\n",
    "        batches = json.load(f)\n",
    "\n",
    "    batch_files = [value for key, value in batches.items()]\n",
    "else: \n",
    "\n",
    "    print(\"Creating Batch Files\")\n",
    "    batch_files = utils.create_batches(path_base, tam=45)\n",
    "    save_batches(batch_files)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Batch Files\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch de teste é o 20!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "path_base = \"../../sumdata/dataset_articles\"\n",
    "batch_files = [[]]\n",
    "extract_features_batches(batch_files, path_base, save_batches=1, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch: 5 \n",
      "\n",
      "Iniciando a extração de features...\n",
      "Total de arquivos: 1054 \n",
      "\n",
      "Quantidade de arquivos processados: 308\n",
      "Saving Results\n",
      "501\n",
      "Quantidade de arquivos processados: 619\n",
      "Saving Results\n",
      "1001\n",
      "Batch: 6 \n",
      "\n",
      "Iniciando a extração de features...\n",
      "Total de arquivos: 1054 \n",
      "\n",
      "Quantidade de arquivos processados: 976\n",
      "Saving Results\n",
      "1555\n",
      "Quantidade de arquivos processados: 1297\n",
      "Saving Results\n",
      "2055\n",
      "Batch: 7 \n",
      "\n",
      "Iniciando a extração de features...\n",
      "Total de arquivos: 1054 \n",
      "\n",
      "Quantidade de arquivos processados: 1635\n",
      "Saving Results\n",
      "2609\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algoritmos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def simple_nn(epochs=100, learning_rate=0.001):\n",
    "\n",
    "    metrics = [\n",
    "        keras.metrics.MeanSquaredError(name=\"mean_squared_error\"),\n",
    "        keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\")]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(14,)))\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, activation='relu',kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer=keras.initializers.GlorotUniform()))\n",
    "\n",
    "\n",
    "    model.compile(loss='mae', optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate, clipnorm=1.0), metrics=metrics)\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_hiperparametrs_gb(X_train, y_train):\n",
    "\n",
    "  parameters = {'n_estimators': [50, 100, 200],\n",
    "               'min_samples_leaf':[5, 10, 20],\n",
    "               'min_samples_split':[10, 20, 40],\n",
    "               'max_depth':[5, 7, 20],\n",
    "              }\n",
    "\n",
    "  gbsearch = th.randomized_search (parameters, GradientBoostingRegressor(), X_train, y_train)\n",
    "\n",
    "  return gbsearch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_hiperparametrs_rf(X_train, y_train):\n",
    "  \n",
    "  parameters = {'n_estimators': [50,  100, 200],\n",
    "              'min_samples_leaf':[5, 10, 20],\n",
    "              'min_samples_split':[10, 20, 40],\n",
    "              'max_depth':[ 5, 7, 20]\n",
    "              }\n",
    "\n",
    "  rfsearch = th.randomized_search (parameters, RandomForestRegressor(), X_train, y_train)\n",
    "\n",
    "  return rfsearch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def plot_history(history,dataset):\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Loos x Epoch - {}'.format(dataset))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    return plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def pipeline_rf(dataset, X, y, X_train, y_train, X_test, y_test, train=False):\n",
    "\n",
    "    if train == True:\n",
    "        rfsearch = get_hiperparametrs_rf(X, y)\n",
    "        pickle.dump(rfsearch, open('rfsearch_{}'.format(dataset), 'wb'))\n",
    "\n",
    "        rf = RandomForestRegressor(\n",
    "        n_estimators=rfsearch.best_estimator_.n_estimators,\n",
    "        min_samples_leaf=rfsearch.best_estimator_.min_samples_leaf,\n",
    "        min_samples_split=rfsearch.best_estimator_.min_samples_split,\n",
    "        max_depth=rfsearch.best_estimator_.max_depth)\n",
    "        rf.fit(X_train, y_train)\n",
    "        pickle.dump(rf, open('rf_{}'.format(dataset), 'wb'))\n",
    "\n",
    "\n",
    "    infile = open('rfsearch_{}'.format(dataset),'rb')\n",
    "    rfsearch = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    print(\"\\n Random Forest Hiperparâmetros\")\n",
    "    print(\"Num estimators: {}\".format(rfsearch.best_estimator_.n_estimators))\n",
    "    print(\"Min samples leaf: {}\".format(rfsearch.best_estimator_.min_samples_leaf))\n",
    "    print(\"Min samples splot: {}\".format(rfsearch.best_estimator_.min_samples_split))\n",
    "    print(\"Max depth: {}\".format(rfsearch.best_estimator_.max_depth))\n",
    "    print(\"Best Score: {}\".format(rfsearch.best_score_))\n",
    "\n",
    "    infile = open('rf_{}'.format(dataset),'rb')\n",
    "    rf = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return mse, mae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def pipeline_gb(dataset, X, y, X_train, y_train, X_test, y_test, train=False):\n",
    "\n",
    "    if train:\n",
    "        gbsearch = get_hiperparametrs_gb(X, y)\n",
    "        pickle.dump(gbsearch, open('gbsearch_{}'.format(dataset), 'wb'))\n",
    "\n",
    "        gb = GradientBoostingRegressor(\n",
    "        n_estimators=gbsearch.best_estimator_.n_estimators, \n",
    "        min_samples_leaf=gbsearch.best_estimator_.min_samples_leaf, \n",
    "        min_samples_split=gbsearch.best_estimator_.min_samples_split, \n",
    "        max_depth=gbsearch.best_estimator_.max_depth)\n",
    "        gb.fit(X_train, y_train)\n",
    "        pickle.dump(gb, open('gb_{}'.format(dataset), 'wb'))\n",
    "\n",
    "    infile = open('gbsearch_{}'.format(dataset),'rb')\n",
    "    gbsearch = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    print(\"\\n Gradient Boost Hiperparâmetros\")\n",
    "    print(\"Num estimators: {}\".format(gbsearch.best_estimator_.n_estimators))\n",
    "    print(\"Min samples leaf: {}\".format(gbsearch.best_estimator_.min_samples_leaf))\n",
    "    print(\"Min samples splot: {}\".format(gbsearch.best_estimator_.min_samples_split))\n",
    "    print(\"Max depth: {}\".format(gbsearch.best_estimator_.max_depth))\n",
    "    print(\"Best Score: {}\".format(gbsearch.best_score_))\n",
    "\n",
    "    infile = open('gb_{}'.format(dataset),'rb')\n",
    "    gb = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    y_pred = gb.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return mse, mae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_neural_model(dataset, X_train, y_train, validation_split=0.2, learning_rate=0.001, epochs=100, batch_size=64, verbose=0):\n",
    "\n",
    "    model = simple_nn(epochs=1, learning_rate=learning_rate)\n",
    "\n",
    "    def step_decay(epoch):\n",
    "        initial_lrate = 0.001\n",
    "        drop = 0.5\n",
    "        epochs_drop = 10.0\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "        return lrate\n",
    "\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    early = keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "    callbacks_list = [lrate, early]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train, validation_split=validation_split, epochs=epochs,\n",
    "         batch_size=batch_size, verbose=verbose,  callbacks=callbacks_list)\n",
    "\n",
    "    with open('/history_{}'.format(dataset), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model_{}.json\".format(dataset), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model_{}.h5\".format(dataset))\n",
    "    print(\"Saved model to disk\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def pipeline_neural_model(\n",
    "    dataset,  X_train, y_train, X_test, y_test, validation_split=0.2, learning_rate=0.001,\n",
    "     epochs=100, batch_size=64, verbose=0, train=False):\n",
    "\n",
    "    if train:\n",
    "        train_neural_model(dataset, X_train, y_train, validation_split=validation_split, learning_rate=learning_rate,\n",
    "         epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    json_file = open('model_{}.json'.format(dataset), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    model.load_weights('model_{}.h5'.format(dataset))\n",
    "\n",
    "    infile = open('history_{}'.format(dataset),'rb')\n",
    "    history = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    plt = plot_history(history, dataset)\n",
    "    plt.show()\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return mse, mae\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparação dos dados "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Número de arquivos de treino e validação: 3848\n",
    "\n",
    "Número de arquivos de teste: 384"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def len_sentences_by_text(df):\n",
    "    \n",
    "    grouped = df.groupby(['number_text'])\n",
    "    tam = []\n",
    "    num = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) >= 10 and len(group) <= 40:\n",
    "            num.append(name)\n",
    "            tam.append(len(group))\n",
    "\n",
    "    return tam"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def  load_preparation(X, y, dataset, train=False):\n",
    "\n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "        'count_three_gram', 'count_article_keywords', 'tf-isf',\n",
    "        'position_score', 'paragraph_score', 'number_citations', 'length_score',\n",
    "        'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    X = X[columns_name]\n",
    "    y = y['rouge_1'] *100\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=47)\n",
    "\n",
    "    print(\"\\n{} - Tamanho da amostra de treino: {}\".format(dataset, len(X_train)))\n",
    "    print(\"{} - Tamanho da amostra de test: {}\".format(dataset, len(X_test)))\n",
    "\n",
    "    if train:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        pickle.dump(scaler, open('StandardScaler_{}', 'wb'))\n",
    "    else:\n",
    "        infile = open('StandardScaler_{}'.format(dataset),'rb')\n",
    "        scaler = pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def main(dataset, train=False): \n",
    "\n",
    "    features_df = loader.read_features(path=\"../result/{}/features_*.csv\".format(dataset))\n",
    "    scores_df = loader.read_features(path=\"../result/{}/scores_*.csv\".format(dataset))\n",
    "\n",
    "    print(\"Shape features para a {} - {}\".format(dataset, features_df.shape))\n",
    "    print(\"Shape features para a {} - {}\".format(dataset, scores_df.shape))\n",
    "\n",
    "    X, y, X_train, X_test, y_train, y_test = load_preparation(features_df, scores_df, dataset)\n",
    "\n",
    "    result = pd.DataFrame({'gb':[], 'rf':[], 'mlp':[]})\n",
    "\n",
    "    mse, mae = pipeline_gb(dataset, X, y, X_train, y_train, X_test, y_test, train=train)\n",
    "    result['gb'] = [mse, mae]\n",
    "    mse, mae = pipeline_rf(dataset, X, y, X_train, y_train, X_test, y_test, train=train)\n",
    "    result['rf'] = [mse, mae]\n",
    "    mse, mae = pipeline_neural_model(\n",
    "        dataset,  X_train, y_train, X_test, y_test, validation_split=0.2, learning_rate=0.001,\n",
    "        epochs=100, batch_size=64, verbose=0, train=train)\n",
    "    result['mlp'] = [mse, mae]\n",
    "\n",
    "    return features_df, scores_df, X_train, X_test, y_train, y_test, result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Avaliação dos resultados por seção\n",
    "\n",
    "São treinados diferentes algoritmos para cada uma das seções dos documentos, são gerados resumos para cada seção selecionando as sentenças com os 3 melhores scores. São avaliados os resultados de cada seção separadamente e os resumos gerados pela combinação dos resumos de cada seção. \n",
    "\n",
    "A busca de hiperparâmetros é realizada com todo dataset. \n",
    "Para normalizar os dados eu utilizo o StandartScale.\n",
    "\n",
    "A seguir é apresentado para cada seção o shape das features e scores, o tamanho da base de treino e teste, os hiperparâmetros obtidos com o RandomizedSearch e o bestscore que é a estimativa do erro esperado.\n",
    "\n",
    "## Introdução"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "path_base = \"../../sumdata/dataset_articles\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "dataset='introduction'\n",
    "features_intro, scores_intro, X_train, X_test, y_train, y_test, result_intro = main(dataset, train=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape features para a introduction - (76938, 16)\n",
      "Shape features para a introduction - (76938, 5)\n",
      "\n",
      "introduction - Tamanho da amostra de treino: 61550\n",
      "introduction - Tamanho da amostra de test: 15388\n",
      "\n",
      " Gradient Boost Hiperparâmetros\n",
      "Num estimators: 200\n",
      "Min samples leaf: 10\n",
      "Min samples splot: 20\n",
      "Max depth: 5\n",
      "Best Score: 0.2552171838114866\n",
      "\n",
      " Random Forest Hiperparâmetros\n",
      "Num estimators: 50\n",
      "Min samples leaf: 20\n",
      "Min samples splot: 40\n",
      "Max depth: 7\n",
      "Best Score: 0.024119128157676584\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-25T12:34:21.382837</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m449eb557b8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m449eb557b8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.144173\" xlink:href=\"#m449eb557b8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(129.781673 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.926414\" xlink:href=\"#m449eb557b8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(200.563914 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"277.708655\" xlink:href=\"#m449eb557b8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(271.346155 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.490896\" xlink:href=\"#m449eb557b8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(342.128396 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Epoch -->\n     <g transform=\"translate(202.232813 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m72d1581b87\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"218.565877\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 9.4 -->\n      <g transform=\"translate(27.240625 222.365095)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"193.657746\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 9.6 -->\n      <g transform=\"translate(27.240625 197.456965)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"168.749615\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 9.8 -->\n      <g transform=\"translate(27.240625 172.548834)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"143.841485\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10.0 -->\n      <g transform=\"translate(20.878125 147.640703)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"118.933354\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10.2 -->\n      <g transform=\"translate(20.878125 122.732573)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"94.025223\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10.4 -->\n      <g transform=\"translate(20.878125 97.824442)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"69.117093\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 10.6 -->\n      <g transform=\"translate(20.878125 72.916311)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m72d1581b87\" y=\"44.208962\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 10.8 -->\n      <g transform=\"translate(20.878125 48.008181)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p506f252623)\" d=\"M 65.361932 32.201761 \nL 68.901044 131.101014 \nL 72.440156 149.807482 \nL 75.979268 157.25325 \nL 79.51838 166.30718 \nL 83.057492 172.167115 \nL 86.596604 176.164479 \nL 90.135716 176.841119 \nL 93.674828 181.034337 \nL 97.21394 190.146346 \nL 100.753052 194.714169 \nL 104.292164 195.486894 \nL 107.831276 200.49809 \nL 111.370388 197.481894 \nL 114.909501 198.673645 \nL 118.448613 203.117351 \nL 121.987725 200.653086 \nL 125.526837 203.430195 \nL 129.065949 204.676936 \nL 132.605061 208.261452 \nL 136.144173 209.54525 \nL 139.683285 213.341653 \nL 143.222397 214.202745 \nL 146.761509 210.61621 \nL 150.300621 216.726752 \nL 153.839733 211.735748 \nL 157.378845 215.785015 \nL 160.917957 216.184561 \nL 164.457069 215.314206 \nL 167.996181 215.270142 \nL 171.535293 218.928295 \nL 175.074405 220.571733 \nL 178.613517 218.817125 \nL 182.152629 221.501711 \nL 185.691742 219.500416 \nL 189.230854 220.078476 \nL 192.769966 221.226043 \nL 196.309078 219.887254 \nL 199.84819 222.668401 \nL 203.387302 222.108632 \nL 206.926414 221.317378 \nL 210.465526 220.293808 \nL 214.004638 227.02196 \nL 217.54375 220.542515 \nL 221.082862 223.993888 \nL 224.621974 223.17496 \nL 228.161086 224.97577 \nL 231.700198 223.671068 \nL 235.23931 222.842638 \nL 238.778422 224.447238 \nL 242.317534 225.732342 \nL 245.856646 226.765414 \nL 249.395758 224.542492 \nL 252.934871 227.607502 \nL 256.473983 226.753775 \nL 260.013095 226.594265 \nL 263.552207 225.361301 \nL 267.091319 224.944058 \nL 270.630431 226.214316 \nL 274.169543 224.501279 \nL 277.708655 229.522807 \nL 281.247767 226.435349 \nL 284.786879 224.617437 \nL 288.325991 227.29549 \nL 291.865103 227.128498 \nL 295.404215 226.532385 \nL 298.943327 229.874489 \nL 302.482439 226.746292 \nL 306.021551 227.16983 \nL 309.560663 226.272157 \nL 313.099775 225.437671 \nL 316.638887 225.925227 \nL 320.177999 226.413852 \nL 323.717112 226.547113 \nL 327.256224 228.331532 \nL 330.795336 226.201964 \nL 334.334448 228.967195 \nL 337.87356 229.011259 \nL 341.412672 228.166083 \nL 344.951784 228.728465 \nL 348.490896 227.674133 \nL 352.030008 227.527569 \nL 355.56912 225.971191 \nL 359.108232 228.074036 \nL 362.647344 225.977011 \nL 366.186456 227.343355 \nL 369.725568 228.612544 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p506f252623)\" d=\"M 65.361932 181.899823 \nL 68.901044 195.613979 \nL 72.440156 214.174477 \nL 75.979268 179.417266 \nL 79.51838 207.634458 \nL 83.057492 212.534128 \nL 86.596604 193.968879 \nL 90.135716 206.405176 \nL 93.674828 215.662087 \nL 97.21394 223.489704 \nL 100.753052 220.715802 \nL 104.292164 225.49385 \nL 107.831276 224.385002 \nL 111.370388 219.424521 \nL 114.909501 224.283096 \nL 118.448613 224.979214 \nL 121.987725 216.711549 \nL 125.526837 229.51378 \nL 129.065949 227.176719 \nL 132.605061 228.265376 \nL 136.144173 225.751464 \nL 139.683285 223.786632 \nL 143.222397 224.148053 \nL 146.761509 223.038967 \nL 150.300621 224.093774 \nL 153.839733 221.367144 \nL 157.378845 227.091916 \nL 160.917957 226.064308 \nL 164.457069 227.86001 \nL 167.996181 224.762932 \nL 171.535293 228.407783 \nL 175.074405 226.275602 \nL 178.613517 227.139069 \nL 182.152629 228.453747 \nL 185.691742 228.539025 \nL 189.230854 227.25012 \nL 192.769966 228.32013 \nL 196.309078 225.829378 \nL 199.84819 224.181071 \nL 203.387302 229.53314 \nL 206.926414 228.872772 \nL 210.465526 225.894346 \nL 214.004638 227.535646 \nL 217.54375 229.016604 \nL 221.082862 224.115272 \nL 224.621974 228.159076 \nL 228.161086 226.182485 \nL 231.700198 225.44931 \nL 235.23931 226.352684 \nL 238.778422 227.526619 \nL 242.317534 225.482448 \nL 245.856646 226.776341 \nL 249.395758 225.80895 \nL 252.934871 227.007826 \nL 256.473983 224.393553 \nL 260.013095 226.318003 \nL 263.552207 226.813992 \nL 267.091319 228.677512 \nL 270.630431 228.468594 \nL 274.169543 227.680428 \nL 277.708655 226.725151 \nL 281.247767 226.12013 \nL 284.786879 226.295674 \nL 288.325991 226.274533 \nL 291.865103 226.694864 \nL 295.404215 226.020956 \nL 298.943327 225.989363 \nL 302.482439 226.909128 \nL 306.021551 226.262299 \nL 309.560663 227.202374 \nL 313.099775 226.36278 \nL 316.638887 225.697424 \nL 320.177999 226.261112 \nL 323.717112 226.178922 \nL 327.256224 226.245909 \nL 330.795336 226.965425 \nL 334.334448 226.704603 \nL 337.87356 226.685719 \nL 341.412672 227.307724 \nL 344.951784 227.278981 \nL 348.490896 226.99975 \nL 352.030008 227.04904 \nL 355.56912 227.074457 \nL 359.108232 226.837508 \nL 362.647344 227.003907 \nL 366.186456 226.888461 \nL 369.725568 226.572174 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Loos x Epoch - introduction -->\n    <g transform=\"translate(135.893125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" id=\"DejaVuSans-78\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"176.326172\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"228.425781\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"260.212891\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"319.392578\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"351.179688\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"414.363281\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"477.839844\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"539.021484\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"594.001953\" xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"657.380859\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"689.167969\" xlink:href=\"#DejaVuSans-2d\"/>\n     <use x=\"725.251953\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"757.039062\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"784.822266\" xlink:href=\"#DejaVuSans-6e\"/>\n     <use x=\"848.201172\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"887.410156\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"926.273438\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"987.455078\" xlink:href=\"#DejaVuSans-64\"/>\n     <use x=\"1050.931641\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"1114.310547\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"1169.291016\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"1208.5\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"1236.283203\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"1297.464844\" xlink:href=\"#DejaVuSans-6e\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.14375 59.674375 \nL 113.13125 59.674375 \nQ 115.13125 59.674375 115.13125 57.674375 \nL 115.13125 29.318125 \nQ 115.13125 27.318125 113.13125 27.318125 \nL 57.14375 27.318125 \nQ 55.14375 27.318125 55.14375 29.318125 \nL 55.14375 57.674375 \nQ 55.14375 59.674375 57.14375 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 59.14375 35.416562 \nL 79.14375 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- Train -->\n     <g transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 59.14375 50.094687 \nL 79.14375 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- Test -->\n     <g transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"44.083984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"105.607422\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"157.707031\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p506f252623\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Materials and Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "dataset='materials'\n",
    "features_mat, scores_mat, X_train, X_test, y_train, y_test, result_mat = main(dataset, train=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape features para a materials - (172234, 16)\n",
      "Shape features para a materials - (172234, 5)\n",
      "\n",
      "materials - Tamanho da amostra de treino: 137787\n",
      "materials - Tamanho da amostra de test: 34447\n",
      "\n",
      " Gradient Boost Hiperparâmetros\n",
      "Num estimators: 50\n",
      "Min samples leaf: 10\n",
      "Min samples splot: 20\n",
      "Max depth: 5\n",
      "Best Score: 0.009325638415237725\n",
      "\n",
      " Random Forest Hiperparâmetros\n",
      "Num estimators: 100\n",
      "Min samples leaf: 20\n",
      "Min samples splot: 40\n",
      "Max depth: 7\n",
      "Best Score: 0.008664772857182302\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-25T12:34:28.777677</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mbaca83d53e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"120.487035\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(114.124535 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.974638\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(175.612138 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.462242\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(237.099742 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.949845\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(298.587345 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.437448\" xlink:href=\"#mbaca83d53e\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(356.893698 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epoch -->\n     <g transform=\"translate(195.870313 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m943eeea89e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"223.837264\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 7.3 -->\n      <g transform=\"translate(20.878125 227.636483)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"195.719004\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 7.4 -->\n      <g transform=\"translate(20.878125 199.518223)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"167.600744\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 7.5 -->\n      <g transform=\"translate(20.878125 171.399963)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"139.482484\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 7.6 -->\n      <g transform=\"translate(20.878125 143.281703)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"111.364224\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 7.7 -->\n      <g transform=\"translate(20.878125 115.163443)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"83.245964\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 7.8 -->\n      <g transform=\"translate(20.878125 87.045183)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"55.127705\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 7.9 -->\n      <g transform=\"translate(20.878125 58.926923)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m943eeea89e\" y=\"27.009445\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 8.0 -->\n      <g transform=\"translate(20.878125 30.808663)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pb9cdc34675)\" d=\"M 58.999432 32.201761 \nL 62.073812 111.145596 \nL 65.148192 126.856088 \nL 68.222572 137.011314 \nL 71.296952 144.518895 \nL 74.371333 151.803235 \nL 77.445713 148.645959 \nL 80.520093 151.159525 \nL 83.594473 157.079887 \nL 86.668853 168.18251 \nL 89.743233 177.086919 \nL 92.817614 177.543321 \nL 95.891994 180.075524 \nL 98.966374 183.143504 \nL 102.040754 183.533404 \nL 105.115134 182.594722 \nL 108.189514 184.061539 \nL 111.263895 183.835885 \nL 114.338275 187.584044 \nL 117.412655 191.467354 \nL 120.487035 192.801567 \nL 123.561415 193.197635 \nL 126.635795 198.399605 \nL 129.710176 198.012789 \nL 132.784556 194.973234 \nL 135.858936 203.095698 \nL 138.933316 201.777306 \nL 142.007696 196.105391 \nL 145.082076 199.395941 \nL 148.156457 206.168102 \nL 151.230837 205.491007 \nL 154.305217 208.101512 \nL 157.379597 207.416908 \nL 160.453977 206.658829 \nL 163.528357 209.035367 \nL 166.602738 208.598808 \nL 169.677118 208.384417 \nL 172.751498 211.637827 \nL 175.825878 212.13861 \nL 178.900258 211.401313 \nL 181.974638 214.316846 \nL 185.049019 213.386208 \nL 188.123399 212.935839 \nL 191.197779 215.541919 \nL 194.272159 213.723952 \nL 197.346539 214.549472 \nL 200.420919 215.336109 \nL 203.4953 215.48802 \nL 206.56968 212.172263 \nL 209.64406 212.747057 \nL 212.71844 217.487798 \nL 215.79282 213.80051 \nL 218.8672 213.739236 \nL 221.941581 215.977003 \nL 225.015961 215.572489 \nL 228.090341 218.702413 \nL 231.164721 217.557921 \nL 234.239101 216.759887 \nL 237.313481 212.827504 \nL 240.387862 215.100265 \nL 243.462242 215.429025 \nL 246.536622 219.39895 \nL 249.611002 218.794927 \nL 252.685382 218.943754 \nL 255.759762 214.77955 \nL 258.834143 217.255172 \nL 261.908523 215.116757 \nL 264.982903 219.874794 \nL 268.057283 217.607664 \nL 271.131663 219.548045 \nL 274.206043 216.239931 \nL 277.280424 219.427509 \nL 280.354804 216.101294 \nL 283.429184 217.81039 \nL 286.503564 216.383261 \nL 289.577944 220.480292 \nL 292.652324 218.610033 \nL 295.726705 215.259819 \nL 298.801085 218.974458 \nL 301.875465 215.327796 \nL 304.949845 217.337228 \nL 308.024225 217.135574 \nL 311.098605 216.758814 \nL 314.172986 220.128336 \nL 317.247366 218.179642 \nL 320.321746 220.526817 \nL 323.396126 217.019328 \nL 326.470506 219.094458 \nL 329.544886 218.808335 \nL 332.619267 216.59068 \nL 335.693647 222.171421 \nL 338.768027 221.378616 \nL 341.842407 220.029923 \nL 344.916787 218.63457 \nL 347.991167 217.879977 \nL 351.065548 217.45629 \nL 354.139928 224.420987 \nL 357.214308 220.659152 \nL 360.288688 217.061027 \nL 363.363068 217.888692 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pb9cdc34675)\" d=\"M 58.999432 169.702824 \nL 62.073812 203.784592 \nL 65.148192 200.425528 \nL 68.222572 201.783876 \nL 71.296952 206.110449 \nL 74.371333 206.205376 \nL 77.445713 192.383243 \nL 80.520093 203.782581 \nL 83.594473 205.485376 \nL 86.668853 216.14916 \nL 89.743233 228.672342 \nL 92.817614 209.715144 \nL 95.891994 229.874489 \nL 98.966374 228.115918 \nL 102.040754 214.28574 \nL 105.115134 216.06764 \nL 108.189514 218.795732 \nL 111.263895 227.866532 \nL 114.338275 218.879531 \nL 117.412655 225.228675 \nL 120.487035 226.63328 \nL 123.561415 222.280427 \nL 126.635795 226.379067 \nL 129.710176 227.077079 \nL 132.784556 224.878731 \nL 135.858936 224.519535 \nL 138.933316 223.065456 \nL 142.007696 220.566504 \nL 145.082076 217.329585 \nL 148.156457 221.600248 \nL 151.230837 228.069392 \nL 154.305217 227.484945 \nL 157.379597 221.420583 \nL 160.453977 227.115693 \nL 163.528357 222.352695 \nL 166.602738 224.252049 \nL 169.677118 221.238907 \nL 172.751498 229.604991 \nL 175.825878 222.703176 \nL 178.900258 224.788362 \nL 181.974638 222.269165 \nL 185.049019 223.234931 \nL 188.123399 224.52168 \nL 191.197779 224.350462 \nL 194.272159 223.110372 \nL 197.346539 224.484406 \nL 200.420919 223.257322 \nL 203.4953 228.036275 \nL 206.56968 222.841947 \nL 209.64406 224.636719 \nL 212.71844 225.99453 \nL 215.79282 226.18948 \nL 218.8672 225.412094 \nL 221.941581 225.589882 \nL 225.015961 224.37835 \nL 228.090341 224.207535 \nL 231.164721 223.372763 \nL 234.239101 224.704161 \nL 237.313481 224.247624 \nL 240.387862 221.070504 \nL 243.462242 223.748853 \nL 246.536622 225.251603 \nL 249.611002 226.034218 \nL 252.685382 225.087625 \nL 255.759762 224.436406 \nL 258.834143 221.6516 \nL 261.908523 223.547199 \nL 264.982903 222.88083 \nL 268.057283 224.869748 \nL 271.131663 223.767624 \nL 274.206043 223.691199 \nL 277.280424 223.192562 \nL 280.354804 222.736025 \nL 283.429184 223.920205 \nL 286.503564 224.8672 \nL 289.577944 224.578798 \nL 292.652324 224.868139 \nL 295.726705 224.112071 \nL 298.801085 224.193188 \nL 301.875465 223.794708 \nL 304.949845 223.901166 \nL 308.024225 223.659557 \nL 311.098605 223.6794 \nL 314.172986 224.369367 \nL 317.247366 224.23368 \nL 320.321746 224.449412 \nL 323.396126 224.235825 \nL 326.470506 223.967669 \nL 329.544886 223.814283 \nL 332.619267 223.64883 \nL 335.693647 223.972093 \nL 338.768027 224.198551 \nL 341.842407 224.074663 \nL 344.916787 224.060049 \nL 347.991167 223.905188 \nL 351.065548 223.834797 \nL 354.139928 224.019959 \nL 357.214308 223.899557 \nL 360.288688 223.99368 \nL 363.363068 223.91578 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Loos x Epoch - materials -->\n    <g transform=\"translate(137.590313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" id=\"DejaVuSans-78\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"176.326172\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"228.425781\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"260.212891\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"319.392578\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"351.179688\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"414.363281\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"477.839844\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"539.021484\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"594.001953\" xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"657.380859\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"689.167969\" xlink:href=\"#DejaVuSans-2d\"/>\n     <use x=\"725.251953\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"757.039062\" xlink:href=\"#DejaVuSans-6d\"/>\n     <use x=\"854.451172\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"915.730469\" xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"954.939453\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"1016.462891\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"1057.576172\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"1085.359375\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"1146.638672\" xlink:href=\"#DejaVuSans-6c\"/>\n     <use x=\"1174.421875\" xlink:href=\"#DejaVuSans-73\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.674375 \nL 106.76875 59.674375 \nQ 108.76875 59.674375 108.76875 57.674375 \nL 108.76875 29.318125 \nQ 108.76875 27.318125 106.76875 27.318125 \nL 50.78125 27.318125 \nQ 48.78125 27.318125 48.78125 29.318125 \nL 48.78125 57.674375 \nQ 48.78125 59.674375 50.78125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 52.78125 35.416562 \nL 72.78125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- Train -->\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 52.78125 50.094687 \nL 72.78125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- Test -->\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"44.083984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"105.607422\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"157.707031\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb9cdc34675\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABCWElEQVR4nO3dd3xUVdrA8d+T3kgCSWih996l2VDEtnZRsayuHbu+a9nyurq7r211beuuiK5rR1eKbdVVUQQUQUAQEJDeS0hIQnqZ5/3jTMiQRoDMDMk8389nPszce+fe584N97nnnHvPEVXFGGNM6AoLdgDGGGOCyxKBMcaEOEsExhgT4iwRGGNMiLNEYIwxIc4SgTHGhDhLBMY0ABHZKCKnBDsOfxGRT0Tkqnou26R/i6bIEoGpt8byH1xEZolIkYjk+bw+DHZcwdIQx01Vz1DVVxsqJnN0iQh2AMb4ya2q+lKwg2jsREQAUVVPsGMx/mMlAnPERCRaRJ4Wke3e19MiEu0z/3oRWSsiWSLygYi09U4XEXlKRHaLSK6ILBORfjWsv4WIbBWRs72fE7zru/IwYh3jXdfvRGSP92r5cp/5SSLymohkiMgmEflfEQnzmX+9iKwUkX0i8pOIDPFZ/SAR+VFEckTkHRGJOdT4vNvoJCIqIleLyBYR2SsiE0XkGO/6s0XkOZ/lu4rIlyKS6d2nN0Uk2TvvdaAD8KG3ZHSvd/pIEfnWu66lIjLGZ32zROQhEfkGKAC6eKddd7Dt1bAvw0Vkoff47hKRJw/nNzF+pqr2sle9XsBG4JQapv8J+A5oCaQB3wJ/9s47GdgDDAGigb8Bs73zTgMWAcmAAL2BNrVs+1Rgp3cbLwJT64hzFnBdLfPGAGXAk954TgTygZ7e+a8B7wPNgE7Az8C13nkXAduAY7zxdgM6+vw2C4C2QAtgJTDxMH/nToACk4AY774XAe959z8d2A2c6F2+GzDOuz9pwGzg6dqOm/f7mcCZuIvBcd7PaT6/32agL67WINL3Nz2U7QHzgF963ycAI4P9d2yvGv7mgh2AvRrPq45EsA440+fzacBG7/t/An/xmZcAlHpPdid7T7QjgbB6bP9vwDLvyTiljuVm4a5ks31eFYlpDC4RxPss/2/gfiAcKAH6+My7EZjlff9f4I46fpsrfD7/BZh0mL9zRSJI95mWCVzi83kacGct3z8P+KG24wbcB7xe5Tv/Ba7y+f3+VMNvWltyrXV73iTxRyA12H+/9qr9ZVVDpiG0BTb5fN7knVZtnqrm4U5q6ar6JfAc8Hdgt4hMFpHEOrYzGegHvKKqmQeJ6XZVTfZ53e8zb6+q5tcQbyru6rfqvqR737fHJb3a7PR5X4BLetWIyAqfRuzj61jfLp/3hTV8TvCur5WIvC0i20QkF3jDuy+16Qhc5K0WyhaRbOA4oI3PMltq+/Ihbu9aoAewSkS+F5Gz6ojLBIklAtMQtuNOLhU6eKdVmyci8UAK7qoeVX1WVYcCfXAnjHtq2oCIhOMSwWvAzSLS7Qjibe6No2q8e3Cllar7ss37fgvQ9Qi2C4Cq9lXVBO9rzpGuD3gYV4Lor6qJwBW4qqv9m6yy/BZcicA3Ucar6qN1fOdQtle5EtU1qnoprkrrMWBqld/eHAUsEZhDFSkiMT6vCGAK8L8ikiYiqcAfcFeJeOddLSKDvA3IDwPzVXWjt/FzhIhE4urpi4Da7k75He7kcw3wOPCaNzkcrj+KSJT3ivws4F1VLcdVEz0kIs1EpCPwPz778hJwt4gM9TZ0d/MuE2zNgDwgR0TSqZ5MdwFdfD6/AZwtIqeJSLj3OI4RkXYNtL39ROQKEUlTd9dRtney3YF0lLFEYA7Vx7hqiYrXg8D/AQuBH3F1+Iu901DVL3D179OAHbgr6gnedSXiGn734qpgMnEn+QOIyFDcCflK78n6MVxS+E0dcT4nBz5HsMhn3k7vNrcDb+IadVd5592GS0rrgbnAW8DL3n15F3jIO20frvG2RR0xBMofcY3xOcB/gOlV5j+CS9TZInK3qm4BzsUl1wxcCeEe6n8+ONj2fJ0OrBCRPOAZYIKqFtZzOyZARNUGpjGhw3ub5BuqWt+rX2OaPCsRGGNMiLNEYIwxIc6qhowxJsRZicAYY0Jco+t0LjU1VTt16hTsMIwxplFZtGjRHlVNq2leo0sEnTp1YuHChcEOwxhjGhUR2VTbPL9WDYnIXd7H6ZeLyJSqvTGK67XyHXE9Sc4XkU7+jMcYY0x1fksE3icObweGqWo/XIdeE6osdi2u35duwFO4B4WMMcYEkL8biyOAWG83BHFU9j9T4VygYtSjqcBYEamxzxJjjDH+4bc2AlXdJiJP4Po1LwQ+U9XPqiyWjreXQ1UtE5EcXIdke3wXEpEbgBsAOnToUG1bpaWlbN26laKiogbfj6NNTEwM7dq1IzIyMtihGGOaCL8lAhFpjrvi74zrbOpdEblCVd+o84s1UNXJuJ4nGTZsWLUHH7Zu3UqzZs3o1KkTTblAoapkZmaydetWOnfuHOxwjDFNhD+rhk4BNqhqhqqW4jqmGl1lmW24Pt7xVh8l4ToeOyRFRUWkpKQ06SQAICKkpKSERMnHGBM4/kwEm4GRIhLnrfcfixu+z9cHwFXe9+OBL/UwH3Vu6kmgQqjspzEmcPyWCFR1Pq4BeDGua+IwYLKI/ElEzvEu9k8gRUTW4roZrqtb4SNSVFrOzpwiysqtK3RjjPHl1wfKVPUB4IEqk//gM78INyC43xWXlrN7XxFJcZFEHMlwJjXIzMxk7NixAOzcuZPw8HDS0twDfAsWLCAqKqrW7y5cuJDXXnuNZ599tmGDMsaYemp0TxYfLglzVSrqafhO9lJSUliyZAkADz74IAkJCdx9993755eVlRERUfNPPWzYMIYNG9bgMRljTH2FTKdzYd66dU+Aelv91a9+xcSJExkxYgT33nsvCxYsYNSoUQwePJjRo0ezevVqAGbNmsVZZ7nxvB988EGuueYaxowZQ5cuXayUYIwJiCZXIvjjhyv4aXtutekeVQpLyomJDCc87NAaXPu0TeSBs/secixbt27l22+/JTw8nNzcXObMmUNERARffPEFv/vd75g2bVq176xatYqvvvqKffv20bNnT2666SZ7ZsAY41dNLhEcTCBHX7jooosID3cNEjk5OVx11VWsWbMGEaG0tLTG7/ziF78gOjqa6OhoWrZsya5du2jXzkZVNMb4T5NLBLVduZeUlbNq5z7aNY+jRXztjbcNKT4+fv/7+++/n5NOOokZM2awceNGxowZU+N3oqOj978PDw+nrKzM32EaY0JcyLQRVNx/H6wR2XJyckhPTwfglVdeCUoMxhhTk5BJBJWNxcHZ/r333stvf/tbBg8ebFf5xpijSqMbs3jYsGFadWCalStX0rt37zq/p6os25ZDq8QYWiXG1Lns0a4++2uMMb5EZJGq1nivesiUCEQEEQnY7aPGGNNYhEwiAAgTsDxgjDEHCrFEYCUCY4ypKsQSQfAai40x5mgVUolARIJ2+6gxxhytQioRuKqhYEdhjDFHlyb3ZHFdwgQ8fsgER9INNbiO56Kiohg9uuoAbsYY438hlgiEUk/DD0xzsG6oD2bWrFkkJCRYIjDGBEVIVQ1JABuLFy1axIknnsjQoUM57bTT2LFjBwDPPvssffr0YcCAAUyYMIGNGzcyadIknnrqKQYNGsScOXMCE6Axxng1vRLBJ7+BnctqnNWqrNxVDUUd4m637g9nPFrvxVWV2267jffff5+0tDTeeecdfv/73/Pyyy/z6KOPsmHDBqKjo8nOziY5OZmJEycecinCGGMaStNLBHUQAtMNdXFxMcuXL2fcuHEAlJeX06ZNGwAGDBjA5Zdfznnnncd5550XgGiMMaZufksEItITeMdnUhfgD6r6tM8yzYGXga5AEXCNqi4/og3XceWelV1IZn4J/dKTjmgTB6Oq9O3bl3nz5lWb95///IfZs2fz4Ycf8tBDD7FsWc2lF2OMCRS/tRGo6mpVHaSqg4ChQAEwo8pivwOWqOoA4ErgGX/FA+zva8jfzxJER0eTkZGxPxGUlpayYsUKPB4PW7Zs4aSTTuKxxx4jJyeHvLw8mjVrxr59+/wakzHG1CZQjcVjgXWquqnK9D7AlwCqugroJCKt/BVEmHdv/f1MWVhYGFOnTuW+++5j4MCBDBo0iG+//Zby8nKuuOIK+vfvz+DBg7n99ttJTk7m7LPPZsaMGdZYbIwJikC1EUwAptQwfSlwATBHRIYDHYF2wC7fhUTkBuAGgA4dOhx2EL4D2IdxaOMW19eDDz64//3s2bOrzZ87d261aT169ODHH3/0SzzGGHMwfi8RiEgUcA7wbg2zHwWSRWQJcBvwA1BedSFVnayqw1R1WMWDWoejYsx6e7rYGGMqBaJEcAawWFV3VZ2hqrnA1QDixpLcAKz3VyBhQR6u0hhjjkaBaCO4lJqrhRCRZG+JAeA6YLY3ORyy+pzcxadqqLGyJGaMaWh+TQQiEg+MA6b7TJsoIhO9H3sDy0VkNa7kcMfhbCcmJobMzMyDniQbe9WQqpKZmUlMTOMeatMYc3Txa9WQquYDKVWmTfJ5Pw/ocaTbadeuHVu3biUjI6PO5YrLPGTsK6Y8K4qYyPAj3WxQxMTE0K5du2CHYYxpQprEk8WRkZF07tz5oMst35bD9W/O5cUrhzGut9/uUjXGmEYlpDqdqygFFJZWuzHJGGNCVkglgtgolwiKSiwRGGNMhdBKBFYiMMaYaiwRGGNMiAupRBAd4Xa30KqGjDFmv5BKBGFhQkxkGEVWIjDGmP1CKhGAqx6yqiFjjKkUmonAqoaMMWa/kEsEMVFWIjDGGF8hlwhiI8OtjcAYY3yEZCKwEoExxlQKvUQQZW0ExhjjK+QSQUxkOIWlnmCHYYwxR42QSwTWRmCMMQcKyURgVUPGGFMp9BJBVDhFZZYIjDGmQsglghgrERhjzAFCMBGEUVzmwdNYBy42xpgG5rdEICI9RWSJzytXRO6sskySiHwoIktFZIWIXO2veCpUdEVt1UPGGOP4bcxiVV0NDAIQkXBgGzCjymK3AD+p6tkikgasFpE3VbXEX3FVjFJWWFJOXFSTGLLZGGOOSKCqhsYC61R1U5XpCjQTEQESgCygzJ+B2LjFxhhzoEAlggnAlBqmPwf0BrYDy4A7VLXa014icoOILBSRhRkZGUcUyP6qIUsExhgDBCARiEgUcA7wbg2zTwOWAG1x1UjPiUhi1YVUdbKqDlPVYWlpaUcUz/7hKkvs6WJjjIHAlAjOABar6q4a5l0NTFdnLbAB6OXPYPa3EViJwBhjgMAkgkupuVoIYDOu/QARaQX0BNb7MxhrIzDGmAP59bYZEYkHxgE3+kybCKCqk4A/A6+IyDJAgPtUdY8/Y6qsGrJEYIwx4OdEoKr5QEqVaZN83m8HTvVnDFVVVA1ZY7Exxjgh92RxrFUNGWPMAUI3EVjVkDHGACGYCGKi3C5bicAYY5yQSwRR4WGEibURGGNMhZBLBCJig9MYY4yPkEsE4B3A3koExhgDhGgicAPYWyIwxhgI0URgA9gbY0yl0EwEUdZGYIwxFUIyEVjVkDHGVArJRBAbGU5hqXVDbYwxEMKJoMiqhowxBgjVRGC3jxpjzH4hmQisjcAYYyqFZCKwqiFjjKkUmokgKsxKBMYY4xWaiSAynDKPUlpudw4ZY0xIJgIbt9gYYyqFdCKwdgJjjPFjIhCRniKyxOeVKyJ3VlnmHp/5y0WkXERa+CumCjZcpTHGVPLb4PWquhoYBCAi4cA2YEaVZR4HHvcuczZwl6pm+SumChUD2FsiMMaYwFUNjQXWqeqmOpa5FJgSiGBs3GJjjKkUqEQwgTpO8iISB5wOTKtl/g0islBEFmZkZBxxMC3iowDYlVt0xOsyxpjGzu+JQESigHOAd+tY7Gzgm9qqhVR1sqoOU9VhaWlpRxxTz9bNCA8TVmzPPeJ1GWNMYxeIEsEZwGJV3VXHMnWWGBpaTGQ43VsmsHxbTqA2aYwxR61AJII66/5FJAk4EXg/ALHs17dtEsutRGCMMf5NBCISD4wDpvtMmygiE30WOx/4TFXz/RlLVf3SE8nYV8xuaycwxoQ4v90+CuA9uadUmTapyudXgFf8GUdN+qUnAbB8ew4nJ8YEevPGGHPUCMkniwH6tElEBJZvs+ohY0xoC9lEEB8dQZfUeJZZg7ExJsSFbCIAVz20whKBMSbEhXYiaJvE9pwiMvOKgx2KMcYETUgngr7piQD2YJkxJqSFdiJoW3nnkDHGhKp6JQIRiReRMO/7HiJyjohE+jc0/0uKjaRDizhW2J1DxpgQVt8SwWwgRkTSgc+AXxKEe//9oV96opUIjDEhrb6JQFS1ALgA+IeqXgT09V9YgdO3bRKbMgvIKSwNdijGGBMU9U4EIjIKuBz4j3dauH9CCqyKJ4ztNlJjTKiqbyK4E/gtMENVV4hIF+Arv0UVQIM7JBMRJsxesyfYoRhjTFDUKxGo6teqeo6qPuZtNN6jqrf7ObaASIyJZHjnFnyxsq5eso0xpumq711Db4lIorc30eXATyJyj39DC5xTerdi7e48Nu4JaAeoxhhzVKhv1VAfVc0FzgM+ATrj7hxqEk7p3QrASgXGmJBU30QQ6X1u4DzgA1UtBdRvUQVYh5Q4erRKYObK3cEOxRhjAq6+ieAFYCMQD8wWkY5Ak3oK65TerViwMYucAruN1BgTWurbWPysqqar6pnqbAJO8nNsATW2dyvKPcqsn61UYIwJLfVtLE4SkSdFZKH39Vdc6aDJGNQ+mdSEKL6w6iFjTIipb9XQy8A+4GLvKxf4l7+CCobwMOGkni2ZtXo3peWeYIdjjDEBU99E0FVVH1DV9d7XH4EudX1BRHqKyBKfV66I3FnDcmO881eIyNeHsQ8N5pQ+rdhXVMb3G7KCGYYxxgRUfRNBoYgcV/FBRI4FCuv6gqquVtVBqjoIGAoUADN8lxGRZOAfwDmq2he4qP6hN7zju6eSEB3B1EVbgxmGMcYEVH0TwUTg7yKyUUQ2As8BNx7CdsYC67yNzL4uA6ar6mYAVQ1qBX1cVATnD07no2U72JtfEsxQjDEmYOp719BSVR0IDAAGqOpg4ORD2M4EYEoN03sAzUVklogsEpErD2GdfnHFyI6UlHl4d9GWYIdijDEBcUgjlKlqrvcJY4D/qc93RCQKOAd4t4bZEbhqo18ApwH3i0iPGtZxQ8UdSxkZGYcS8iHr2boZx3RqzpvzN+PxNJln5owxplZHMlSl1HO5M4DFqlpT/w1bgf+qar6q7sENgDOw6kKqOllVh6nqsLS0tMOPuJ6uGNmRTZkFzF1rPZIaY5q+I0kE9b1cvpSaq4UA3geOE5EIEYkDRgArjyCmBnF6v9akxEfxxndVmzSMMabpiahrpojso+YTvgCxB1u5t7fScfg0LIvIRABVnaSqK0XkU+BHwAO8pKrL6x++f0RHhHPRsPZMnr2OHTmFtEk66K4aY0yjVWeJQFWbqWpiDa9mqlpnEvF+P19VU1Q1x2faJFWd5PP5cVXto6r9VPXpI9qbBnT5iA4oMGX+5mCHYowxfnUkVUNNWvsWcZzUsyVvLdhCSZk9aWyMabosEdThylEd2ZNXzCfLdwQ7FGOM8RtLBHU4oXsanVLieG3egY3G5R5F1W4tNcY0DZYI6hAWJvxyVCcWbdrL8m2umWNLVgEnPTGLy1+aT5Y9fWyMaQIsERzE+KHtiI0M57V5G9meXchlL33H3oISFm7ayznPzWXVziY1Po8xJgRZIjiIpNhIzh+SzvtLtnPZi9+RnV/KG9eO4N83jqKkzMMF//iWr1bbGAbGmMbLEkE9XDmqI8VlHjL2FfPKNcMZ2D6ZQe2T+fC24+jQIo57p/5IYUl5sMM0xpjDYomgHnq1TuSxC/sz5YaRDO3YfP/0Vokx/Pm8fmTsK+bVeRuDF6AxxhwBSwT1dMkxHRjQLrna9GM6teDEHmlM+nod+4ps4HtjTONjiaAB3H1qT7ILSvnn3A3BDsUYYw6ZJYIG0L9dEqf3bc1LczbYgDbGmEbHEkED+Z9Te5BfUsYLs9cHOxRjjDkklggaSI9WzTi1Tytm/LDVnjo2xjQqlgga0Mm9WrIrt5ifd+UFOxRjjKk3SwQN6IQebvS02T/7dzhNY4xpSJYIGlCbpFi6t0xg9hpLBMaYxsMSQQM7oUca8zdk2ZPGxphGwxJBAzuhRxolZR7mb8gMdijGGFMvlgga2IjOLYiKCGP2z3uCHYoxxtSL3xKBiPQUkSU+r1wRubPKMmNEJMdnmT/4K55AiYkMZ0TnFtZOYIxpNA46AP3hUtXVwCAAEQkHtgEzalh0jqqe5a84guGE7mk89PFKtmcX0jY5NtjhGGNMnQJVNTQWWKeqmw66ZBNgt5EaYxqTQCWCCcCUWuaNEpGlIvKJiPQNUDx+1aNVAq0TY6x6yBjTKPg9EYhIFHAO8G4NsxcDHVV1IPA34L1a1nGDiCwUkYUZGUdwci0rAY/n8L9fTyLCmJ5pzP55D0WldhupMeboFogSwRnAYlXdVXWGquaqap73/cdApIik1rDcZFUdpqrD0tLSDi+K5dPg/9IgKzCdwp01oC15xWXMsmEsjTFHuUAkgkuppVpIRFqLiHjfD/fG458b8GO9I4vlB+bEPLJLC1ITovhg6faAbM8YYw6XXxOBiMQD44DpPtMmishE78fxwHIRWQo8C0xQf3XdGd/S/ZsXmEQQER7Gmf3bMHPlbvKKywKyTWOMORx+TQSqmq+qKaqa4zNtkqpO8r5/TlX7qupAVR2pqt/6LZgEbyLID1wD7jkD21Jc5uGLn6rVihljzFEjdJ4sjksBCQtYiQBgSIfmtE2KseohY8xRLXQSQVi4SwZ5gbs6DwsTzhrYltk/Z5BdYENYGmOOTqGTCAASWgW0aghc9VCZR/l0+c6AbtcYY+rLb11MHJXi0wJaNQTQt20inVPjeWnuBgpKyumcFk/ftom0bBYT0DiMMaY2oZUIElpC1rqAblJEuP74LjzyyUr+9NFPAERHhPGX8QM4d1B6QGMxxpiahFYiiE+DvAxQBff4QkBcNqIDlw5vT2Z+Cesz8nnis9Xc8fYSVu/cx92n9iQsLHCxGGNMVSHWRtASygqhJPCDy4sIqQnRDO/cgjeuHcGlwzvwj1nruOH1RdYNhTEmqEIrEQT4obLaREWE8fD5/Xjw7D7MXLWL619baMnAGBM0oZUIErz9FAX4zqGaiAi/OrYzj104gLlr99SZDPLtyWRjjB+FWBvB0VEi8HXxsPYIcO+0H7nynws4vV9r2reIIz4qnDlr9/D5T7tYuzuPY7ulcMuYbozqmoIEsH3DGNP0hVYi2N/NxNGTCAAuGtaeMBEe/GAFCzZm7Z8eESaM6NKCsb1aMv2HbVz20nz6pyfRPD6K7IISikrL+Z9xPTi9X5sgRm+MaexCKxHEpQLi7hw6ylw4tB0XDEknK7+ELXsL2VtQwpAOzUmKjQTgrnE9mLpoK+98v4WcghKax0WxLbuQO95ewtSJcfRvlxTkPTDGNFahlQjCIyCuRUC7mTgUIkJKQjQpCdHV5sVEhnPFyI5cMbLj/ml78oo597lvuP61hXxw27H2kJox5rCEVmMxuHaCo6CxuCGkJkQz+cqh5BSWcuPriyguszuPjDGHLvQSQULLo6qx+Ej1bZvEkxcP5IfN2dz/3nL8NZyDMabpCs1EcJQ1Fh+pM/q34baTu/HvhVt5a8HmYIdjjGlkQi8RxLc8KhuLj9Sdp/TgxB5pPPjBChZt2hvscIwxjUjoJYKENCjNh5L8YEfSoMLDhGcnDKZNUiw3v7mIXblFwQ7JGNNIhF4iaIiHyjLXQUHWwZcLsKS4SF745VByC8sY8/gs/ve9ZazdHfh+lYwxjUto3T4KB45d3KLzoX9fFf51BnQ6Hsb/s2FjawC92yTy/q3H8uLs9fx74Vbe+G4zXdPiad8ijvbN4zhvcDpDOzYPdpjGmKOI30oEItJTRJb4vHJF5M5alj1GRMpEZLy/4tkv3tvf0OGWCDLXuecQ1s0Ez9F5u2aPVs14/KKBzPvNydxzWk+6t2xGxr5ipi/eyjWvfE9Wvg2baYyp5LcSgaquBgYBiEg4sA2YUXU577zHgM/8FcsBjrSbia0L3L+Fe2HHUkgf0jBx+UFKQjS3nNRt/+efd+3jjGfm8MRnq3n4/P41fueTZTtYvj2He07rFagwjTFBFqg2grHAOlXdVMO824BpQGDu6dxfIjjMO4e2LICIWPd+/VcNE1OA9GjVjKtGdWLKgs0s35ZTbf7URVu5+a3F/P2rdTXOr5CZV8xzX65hX1GpP8M1xgRIoBLBBGBK1Ykikg6cDzxf15dF5AYRWSgiCzMyjvDWz/BIiG1+BCWChdBhJLTqB+saVyIAuHNcd1Lio/jD+8vxeCofPvv391u4Z+pSRnRuQVR4GFMXba11HY98soonPvuZ6161cRSMaQr8nghEJAo4B3i3htlPA/epqqeudajqZFUdpqrD0tLSjjyo+JaH199Q8T7YvQLaD4cuY2DLfCgpOPJ4AigxJpL7Tu/F4s3ZPPzxSp7+4mfuePsH7pv+I8d1S+WVq4czrm8r3l+yjZKy6odl1c5cpi3eyjGdmrNgYxa3vLmY0vI6D58x5igXiBLBGcBiVa3pzDsMeFtENgLjgX+IyHl+jyjhMB8q27YY1APthkPXk6C8BDZ92/Dx+dmFQ9oxtGNzXpq7gWdmrmHBhiwuGNyOF68cRkxkOBcNbcfeglJmrqx+yB77ZBXNoiN48cph/PncfsxctZu73116QOnCGNO4BOL20UupoVoIQFX3378pIq8AH6nqe36PKKElbP+hIgjYsQTSekPkQXrvrGgobjcUwqMhPMq1E3Q/xa/hNrSwMOG1a4aTsa+YNskxREeEHzD/+O5ptE6M4d1FWzmjf+VYB/PWZfLV6gx+c0YvkuOiuGJkR3IKS3n8v6s5uVdLzh2UHuhdMcY0AL+WCEQkHhgHTPeZNlFEJvpzuwfl283EnCdg8hh4qi989Ujdt5Vu+R5Se7o2hqg411bQCNsJAOKjI+iUGl8tCYB7SvmCIenMWr2b3d4nlFWVRz9dRZukGH41utP+ZW86sStd0+KZPHu9dXhnTCPl10SgqvmqmqKqOT7TJqnqpBqW/ZWqTvVnPPslpEHJPvjuefjy/6DXWdBuGHz9KDzVD1Z+VP07qrD1e2h/TOW0Lie5NoN9R+f4Bkdi/NB2eBSm/7CNpVuyuf3tJSzdks1d43oQE1mZPMLChOuP78KK7bnMW5cZxIiNMYcr9LqYgMpuJj79DXQ/FS56BS57B25dCC17w/s3Q06Vu2Yy10FhlmsfqND1JPfv+lmBiDqguqQlMKxjc/762WrO/fs3fLlyFzec0IULh7Srtux5g9NJTYjihdnrgxCpMeZIhWYiSGjl/m0/Ai561d1SCpDaHca/DOVlMGPigU8O728f8CkRtB4IsS1g3nOwc3lgYg+gm8Z0pU+bRB44uw/f/W4svzuzN+FhUm25mMhwrhrVia9/zmD1zn1BiNQYcyRCMxF0OhZOvt+VAqLiDpyX0hXO/AtsnAPf/q1y+pYFEJ0IaT5P3IaFwZmPQ/ZmmHQcvHcL7NsZmH0IgLG9W/H+rcdx9bGdaRYTWeeyV4zsSGxkOC/OcaWCkjIPG/bkU253Exlz1Au9TucAouLhhLtrnz/ocljzmWs/yN4MkbHuc/pQd/L31X88dBsLs5+ABZMhax1c86l/4z8KNY+P4uJh7XhrwWY27sln2bYciss8nNgjjecuG1xrIlFVRKqXMowxgROaieBgROCspyF3B6yYDmXF7pmB4/+n5uVjm8NpD0FMEnz1MOTvgfjUgIZ8NLju+C58sXI3HlV+ObIjCTERPPflWi58/lv+edUxtG8RR2FJOUu3ZrNgQxbzN2SyeFM2Qzom89Qlg2jZ7CC37xpj/EIa2y1/w4YN04ULFwY7jJpt/8Hdinr+ZBh4SbCjOSp8s3YPE99YRFR4GG2SY1i5Yx/lHkUEerVOpH96Ih8s3U6zmEj+ftkQhnduAUB+cRm7covIKSwlu7CUzinxdEqND/LeGNN4icgiVR1W4zxLBA3I44G/9oTOJxyVYxUEy9rdefx2+o9EhocxpENzhnRMZmiHFiTFueqiVTtzmfj6IrbsLWRguyS27C0kY1/xAeuIiQzjtWtG7E8UxphDY4kgkGbcBKs/hnvXQ1j1h7VY+jbMfwGu/9JVQRkAcotK+fOHP7Eps4BOqXF0TImnbXIMybFRxEaF87sZy9idW8xb149gQLvkYIdrTKNTVyKwNoKG1n0cLH3L20vpiOrz13wO2xfDvh2Q2Dbw8R2lEmMiefyigbXOf/O6EYx/fh5XvbyAd24cRY9WzfbPU1U2ZxVQUFJO7zaJgQjXmCbFEkFD63oSSLi7y6imRLBntfffNZYIDkGbpFjeun4EF02ax2lPz6Zd81i6t2xGbGQ4CzdlsSvXVSXdc1rPAwbj8Zdv1u5hxfYcerdJpE+bRFISov2+TWP8xRJBQ4tt7h5UW/MZjL3/wHmecpcAADLXQJcTAx9fI9YxJZ5pN41m2uKtrN2dx9rdeeQVlzGicwrHdG7B9xuyePy/q8kvLuOe03oiImTll/Dd+kzW7c5jQ2Y+e/NLuPXkbgztePhtDZ8u38HNby7G9xGJsb1a8uKVwwir4YG7hpRdUMLNby5mROcUbj25W40P+BlzqCwR+EP3cTDzj+7hsmatK6dnb4Yy14nb/oRQk/Iy187Q9WSITvBvrI1M+xZx3HlKjxrnXT68AwkxEfxj1jrWZ+STmV/Mok1795+w2yTFUFquXPrifB4fP4BzB6VT7lGmLNjMP75aS0pCNMd1T+X4bqkM6dj8gD6VKnyzdg+3T1nCwPbJPHfZEDbtyefLVbt5ae4G3py/iV+O6rR/2X/O3cDzs9ZRXFZOWbmSkhDFw+f354Qehzemhsej3PnOEuatz+TbdZl8u24Pz0wYTOsku+3WHBlLBP5QkQjWfgGDr6icnuGtFgqPqjsR/PQeTLsWmrWBU/4IAy62huV6CAsTHjqvnxsvYc56erdJ5NaTu3Nyr5b0bNWM2Khw9uaXcOMbi7jj7SX8sDmb7zdmsWJ7LsM6NidMhBdnr+f5WeuIighjaIfmjOqaQqfUeBKiwykoKefeqT/SOTWef/3qGJLjokhPjmVU1xRW79rHo5+s4uTerUhPjuWTZTv480c/MaJzC3q3SSQyXJi1OoMrX17AjSd24dfjehIVEUZRaTnFZR6SYut+chvg2S/XMGt1Bn8+rx+xkeHc/95yznx2DuOHtqN1YgxtkmI4tnsqiQd5CrzC87PW8cXKXTxx0UA6+9yau2xrDt+s28P1x3cJaolj4558tmcXMrrbgc/k5BSWsnRLNsd3T7WHERuI3TXkD6rwZG/XL9Elr1dO/+YZ+PwP0O0U2PMz3Lms5u9/fA/88IbrzmL7YtfR3QWToUXnmpc31ZSUeYiKqLkHlZIyD7+bsYypi7bSOjGG3/+iN2cNaIOIkFdcxvz1mcxb5666f9qRe8B327eIZdrE0bRMPPAqfEtWAac9PZvhnVtw96k9uWjSPHq1acaU60fuL1kUlpTz5//8xFvzN5OeHEu5R9mZW0SYwKMXDODiY9rXuj9frdrNNa9+z/mD0/nrRQMREdbuzuO+aT+ybFvO/tHkerVuxoybjyU2qoY71nzMWeOSEkBCdATPXjqY47ul8o9Z63h25hrKPMr//qI31x3fpe4fGij3KB8u3c6JPdJoHh+1f7rHo0yes54uqfGM69PqgJP2yh25tIiPolVizaWZ9Rl5XDRpHlkFJfzzqmGc3Mv1D1ZUWs7lL81n0aa9jO6awqMXDKBDSlyN6/CHco9S7tFa/7aOZnb7aDC8f4vrzvreDZXdUrx3syslDLsGZj0Kv9/huq+o6oUTIboZXPkBLJ0C//2d6xjv8neh7eDA7sfBlJfCpm+g43EQ3ngKmKrKgg1Z9EtPIj669rhzCkrJyCsir7icguIy+qYn1Xr1/so3G3jww5+IjwonKTaS9249tsanpT9ZtoO3v99CakI0HVrEsWCjSzpPjB/IhUMre3ddn5HHN2v38M3aTL7+OYNOqfFMv2l0tZO8qrK3oJS5a/dwx9s/cMHgdjxx0YBar5Yz9hVzxjNzaB4XyT8uH8Ltby9h1c5cuqTGsy4jn3MHtWVvQSnfb8jis7tOoH2LyhNtUWn5AVVmqspvpy/j7e+30KdNIlOuH0lSXCSqyh/eX8Hr320CYHjnFvz+zN7sLShh0tfr+G59Fh1T4vjwtuOqlWB25hRx4fPfUlRaTlqzaLbuLWT6zaPp3jKBu95ZwntLtnPlqI5MX7yNco9y/QldaBYdQU5hKVkFJWzPLmTr3kIKS8p5+IL+nFiPqrhyj/Lk56vpnJrAhUPSa/ztlmzJ5o63fyA5LorpN40+7NJSuUfZnl1ITmEp/dKTal2upMzDXz9fTXJsFMd3T6VPm8QjaoOyRBAMS6bAexNh4jfQup+b9uJYd+If+itX9XPTt9Cq74HfKymAR9vD6NvhlAfctIyf4Y0LoSATLn7t6BkRzVMO06+H5dPg+LurN46HmHKPcvEL8/hpey5TbxpF37a1/yf3VVRazrWvfs+8dZn833n9KS4rZ9rirSzf5koj6cmxHNsthTtO6UF6cg0XDj6e+vxnnpm5hofP789lIzpUm+/xKFf9awELNmTxwa3H0bN1MwpKyvjNtGXMWZPBH8/txzkD27I9u5BTn5rNoPbJvH7tcErKPTzy8Spem7eRCcM7cN/pvUiKjeSRT1bywtfrOWtAGz5bsYvebRN549rhvDhnA8/OXMP1x3emU2o8T33+M3vySgBonRjDuYPb8tKcDZzetzXPXTZ4/4k3u6CEi1+Yx7a9hbx9wyhSm0VxznPfEBsZzhn9WvPC7PX77wzbnl3I72YsY9ZqN8hUmEBSbCRtk2Np1zyWtbvz2J5dxFvXj2Bwh+aAe2L9ne+3cHKvlvufVFdVHvxgBa/Oc0nrpJ5pPHrhgP2lFY9HmTR7HU9+9jNxUeHkFpXx+PgBXDSs9hKcr02Z+cxfn8V3GzJZsiWbLVkFlJa78+4/rxrG2N6tqn1HVbln6o9MXVTZHX6L+ChuOakb1x53eDUDlgiCIXszPN0fzvgLjLjRVRc90h4GToAhv4QXTnBdYPc978DvbfoW/nUGXPo29Dyjcvq+nfDmeNj1E1z9sRsdrYLHA9OugQGXHPgdf1KFD2+Hxa+5YT4zVsGvPoJOxwVm+0ep/OIycgpLaXuQE3ZVhSXlXP3KAr5bnwVAv/RELhjcjrG9W9KhRVy968I9HuXqV1xSue3kbmzIzGfFtlyyC0uIj4ogLMxVKT1yQX8uHd6h2nd9rzhf/24T97+3nDtP6c7nP+1ixfZcjuuWyrfr9pCSEM1JPdP498KtXDGygxu/euVuJr6xiDbJMWzJKuTiYe147EJXMtlXVMrbC7bQPD6Kcwa2JSoijBe+Xscjn6ziz+f25ZfebswfeH8527OLeOXqY/a3DfyweS+XTP6OkjIPFwxO568XD9z/e6gqGfuKiYkKJ8G7fxV27yti/PPzyC0qZerEUWzKLOD+95azPaeIhOgIHh8/gDP6t+HF2et56OOVXHdcZ9Kbx/LYp6uICg9jVNcU9uS5EsaOnCJ+0b8ND53fj6teXsCu3GK+untMrVVw5R7l8592MXn2OhZvzgYgJT6KoR2b07VlAp1S4nh25lrat4jl7RtGVfv+379ay+P/Xc2dp3TnsuEdmLt2D3PX7OHEnmmHPSSsJYJgeao/tB3k2glyt7t2gzOfgEGXwcNt4eT/hRPuOfA7c5+GLx6Ae9ZV77iuKNeNoNbrF3D+85XTK5JH885ucB1/V9Gouuqq7/7h4j/2Tph8IpQWwsS5EGfdQByO/OIypi7ayoguLejV+vAfjNubX8JZf5vLtuxCWjaLpl96EqkJURSUlJNfXMag9s25fWy3gyYXj0eZMPk7FmzMIjkuksfHD2Rcn1Ys25rDb6b/yIrtuZw9sC3PXDJo/wn442U7uPWtxYzr04q/XzaEiPDa69I9HuW61xYyd80eju+eysxVu+mcGs/D5/dnVNeUA5b974qdzFqdwYPn9KlxeNXabMrM58Ln51FQUkZBSTk9WiXw61N78vysdSzZks24Pq34/KddnNm/Nc9dOoSwMGF9Rh4PfLCCnTlFpDWLJq1ZNGN6pnHeIFdlNH99JpdM/q7GZ1byi8uYvngr/5y7gY2ZBbRvEctVozoxpmcaXdMSDvjNJ89ex8Mfr+I/tx93QOnxw6XbuW3KD5w3qC1PXTKowRrE60oEqGqjeg0dOlQbjek3qj7WRdXjUV07U/WBRNX1X7t5f+2tOu2G6t+ZcpnqM4PqWOdE1Yfbq5YWVU776Ndu3Q8kqi59p2H3oarSItX3bnHb+vg+t2+qqtsWq/4xRfXtyyun1WbPWtWiffXf5s4Vqpu+O/yYm6ryMtXZT6iu+aLarOyCEt2VU3jo66tic2a+PvD+ct22t+CA6aVl5frt2j1aUlZe7Ts7cwq1rPwgfwNeWXnFOurhL7TH7z/WZ7/4WYtKq8dwpJZvy9ZTn/xa/zbzZy0udfEWl5brA+8v1473faQX/uMbLSw5tO1e+8r32vcPn2rGviItL/foT9tz9OH//KT9H/hUO973kZ7z3Fz9aOn2On+H7IIS7X3/J3rXOz/sn7Zk817t/vuPdfzz3zT4bwEs1FrOq34rEYhIT+Adn0ldgD+o6tM+y5wL/BnwAGXAnao6t671NqoSweLX4YNb4eb5bjjLT++Du9dAQkt49RwoyXN9DlVQhSd6uOcHLnih5nWu+dxVEU2YAr3OdM8cPNkLOoxyt6SqB27+rvq4CeBGUQuLgJa9qs+rj3274J0r3Ghtx9/tSjS+VyvfPAuf3++G/ux7fs3ryN0Bzw6GHqe69g5fe9aAhLnBgSpsmANvXezivvvnmhvXqyrIgnVfuvEjmuqdVh6Pq5r74XX3m535OBxz3eGtq7wUPrgdVv3HHdNjrq25nyw/ydhXjEe11juI/Gn5thw6p8bXecNATdbuzuO0p2fTJTWezPwSsvJLCA8TTu/XmmuO7cyQDsn1upJ/4P3lvLVgM9/cdzIAZz83l8jwMN6/5dgGf1o9KH0NqepqYJA3gHBgGzCjymIzgQ9UVUVkAPBv4DDPUkehjqPdv5vmujr0mGSI997BkNodfnzXnfwr/mCyN0H+bmh/TI2rA6DLGPf08ooZLhFsmgv5GW6AnPJS1wi96kPoc+6B39u7yVUficCNc6B5x/rvh8fj1vnJb6Aou/YT/ahbXZvB7L9Cn/NqfvZh7lNQVgg/ve/6Y2rn/bvM2w0vneKS4+jb4IR7Yct8mHKpu4Mqf7c7UfUfX7muhS/DnKfg9Eeg91lu2u6V7jt7N7jPrfpB77NddVxy9cZTwCXIn9531Xg9Tq88CZYUwM+fuDu1WlS5jfK/v4fcbTD2D9XnNYT8TNixxP0+MVUanVXhk3tcEjj2Tve39Z9fu2N8yh9rvgioTXEe/PtKWDfT/Vaf3ANL3oQT73XtXFu/d8fm+F9XjtFdlaobsyNrvYtlz8+Quda9sja49Q75JfQ+p/qIgEBaswY64alCaYF7L2Hu4iG87mcq6rprp8b1lxWBhNOtZQI3nNCFD5ZsZ0zPNEZ3TeX47qmHnMyuPrYzr323iZfmbmDBhiz2FZUx7abRAe+yJFD3+40F1qnqJt+Jqprn8zEeaFwNFgfToot7KGzTt66xN61X5ckxpTsU57iTeEJLN23L9+7fdsNrX2d4pDuxLZ/u6uSXT4OoBOh+qntQ7auHYfbj7j9dxbY85TDD22ANMPUauPoTiIiqfTuecsjb5a6s5z7tusRI6Q6X/xta96/5O2FhcNxd8P7NruTS49QD5+dsg0X/gn7jYcPX8PkDroFZxD07UVrgEtjcp2DZVHcCSukGV74Hk09yPbdWJIKyEvj6L26Zdy6HARPcg3wf3ulONhOmuGSw8iN3q+7Xj0GPM2DIlS6xFO9zJ/KlU9zJzveYHXO9O6n9+G93jFJ7wsQ5EOH9z7lhthunGnHJadSt7kR5OE+BF2a79RVmuZj27XTDpO74EVB3bAdd5q72wyIgcx2s/MAlgdG3wykPumP1yb3w7bMuuZ70W+h0fN0PIXo87pi+d5MbR+PsZ91vs3yaa/95+zK3XGK6O6m+fp57OHLcn2HXcvjxHVj9qYu5vIQD/+sKJLd3x67PALd/M250x7jXWe4CpuvJbqRAT7krwQHEpbi/ofJS2L7EXeRkb3Z/1+FRbnr2JpfwCvZAeLQ7JmERULjX3VXnKT0wjrSe0H44tB3ilistcK/IeHdBFZvsTu6Fe92xKNgDeRnu/2VRNpTku2RZnOsuUjxl3lhTuS+xDfe1awXhLWB3C8iKcesqLXCJUdWV0D1lbj0lee7f8lL3m3nK6BTdjE+Twtg6L5zjKKN/y0iaf1DuYo2Icfut5e7vvbwEBl4KI2449L+zgwhUIpgATKlphoicDzwCtAR+UcsyNwA3AHToUMtV3dFIBDoeCxvnuj/QXj67l+ptZNqzpjIRbF3g/kBb9ql7vX3Pd1feqz+GlR9CzzMrq0yO/x/3DMPSKe6PRgTmPgmb58H5L7g/rnevck8+n/bQgestL4M5T7grwtztlX/0rfrD+JfdVf7BqgwGXAyzHnHbrJoI5vzV/ecY+wfXF9PHd7uE4Sl1T1NXNJ4Pu9Zd4bZKhcunQXyKW+83T7vqqWatXIlo3w53d9X2JS75/fg2tBkEE96CJO+dFaNugewtLgEtehVW/+fAmFK6w2kPQ/+L3Ql43nPw39+6k0yfc6HNAPjsf2HOk+4EW14KH9/rShdXfuCSzNwnYfGr7vmQY6538VW1d5M78UYluLGvC7PcQ4Mr3nMlpAphEe5C4KTfu22vmAGLXnHDoPoaeQuM+5M7vuER8Iu/ugQ961F49WxoPxKGX+8SQrNW7nfPWO32f8Mc96BiUY77e7jkjcq/zf7jXULdutD9HSa2cRccXz/mqv6WvOVOblHN3B1qiW3cySoiGpI7uRNvavcDq/BU3bMmP7zhtr/0LbfdmGR3wtVyt5yEuxJz8T4ozXfTYlu4ZFFe7OYnd4DmnaDdUHcsSgvd32lcC5dIYpJc4lKPK9Ft/8GV9hZXqYasTXgUxLd0N2rEJkNCK3fhEBXvjlt0M7fdfdtdNWfeLvd/uHCviyUyzl2IhEe5OERc3NEJ7tgnpruLuYr5JXm0IZOSgl2kJDWjeWIz99touUsmJXnu+xFR3nX45+E5v981JCJRwHagr6ruqmO5E3BtCHXeJN+o2gjAVV98dJd7f9rD7sQE7sTwzAA4+xn3XAG40c2iEtxVcl3Ky9wAOBHR7qr20neg5+neeaUw6ThXRG8zCPpd6E76vc9xJ3MR+M/d8P2LcM5z0O8C90eetcE9E7D1e+g2zp1Uktq5UkzH0YfWxcX8F9wV6tWfVFaPZW9xbQODr4Czn3ZxPneM24eiHPef+IZZlUV5VfeqqObIWA1/Hw6nPuR+wxdOcFdIN3/nYtv+gyu9jLy59naEsmJ3QpIw9586JsmVAHz3TdX9dgmtKu9+mna9OyFPnONt6/mNSzYVJ88t37tksPoTF3/nE92JJCbJndQ2znFXtlVFNXMn3oET3G8d3cxNq1q1k7fbbT8q3l1lp3SrfSjU0iJXWpj7NOR670FP7emSbdZ697lVP1fllD7UDaLUvFPN66pqx1J3Mu8w0pWuDuekVF7qLkpWfwol+9zvHO+9EMrb5V6Rce7vpuPoyoukI+HxQM5md9wj4yEyxiWJomx3Ao+IcSf9mGR3DILQbcWevGJS/VwdFNTbR70Nwreo6qn1WHY9MFxV99S2TKNLBBk/w9+9df6XT6t8GMzjgYfbuCL/aQ+5q4lH2h34IFldPrrLJZmYJLh77YHVPKWFrkTw7XOQtQ4S28FNc11RGNzJ4l+nu5OnhLuTfuY671jNTx1YD384SgrcMxRtB8EV09wJ+KO7YNm7cNtiV20Arhpi6jXuP+h1MyF9SN3rnXySO5Gc/gi8eparzhh61ZHFWh/5me4YJrVzCbPdMW6/qp4wMtfB/EmwaZ5LbkU57mq942h3Zd6qn6s6KM51+9x1rP86FSwvg51L3dX/xjluez1O917FW/fnoSjYA9NcSu3VQt1wbQcqIkOAaCAzADEFTmp3V9zNz3DF5gphYdCiq2tQA28VSZmrz6yPvhe4RND77Op1/ZGxrppiyK/cVXLzjpVJANwV0a8+dlVWW76DzfPdg2BnPHZojci1iYqDUTfDzD/BP0a5xkNPGQy/oTIJAPQ5H/p+5BpjD5YEwFV1fXKPq2uOS3XVRYEQnwKnPwbTr4OwSPc71XTVmNLV3b1zNAiPcFf86UPhuDuDHY05yvk1EYhIPDAOuNFn2kQAVZ0EXAhcKSKlQCFwifq7iBJoFe0Ea2e6K0pfqd1g6yKYfqOr305sV1mVcjAdR7tbOAdeWvsyYWG1d0cRFefq8KvW4zeUY66DNV+46oyeZ0DrAQe2kVTEd9G/6r/Ofhe6+vuMlXDib+p3K2lD6T/elaBSurjkbkwTYk8WB8LeTa6OuPPxB06f+WfXOBse5eq9j/+1q6M0tXv7cld6umt5w9QfGxMigl01ZJp3rLnKZdBlrs54+PX1b7ALdb/4K+RstSRgTAOyRBBMKV2r38Jp6tas9YGjvhljjljjG13BGGNMg7JEYIwxIc4SgTHGhDhLBMYYE+IsERhjTIizRGCMMSHOEoExxoQ4SwTGGBPiGl0XEyKSAWw66II1SwVq7dm0CQvF/Q7FfYbQ3O9Q3Gc49P3uqKppNc1odIngSIjIwtr62mjKQnG/Q3GfITT3OxT3GRp2v61qyBhjQpwlAmOMCXGhlggmH3yRJikU9zsU9xlCc79DcZ+hAfc7pNoIjDHGVBdqJQJjjDFVWCIwxpgQFzKJQEROF5HVIrJWRH4T7Hj8QUTai8hXIvKTiKwQkTu801uIyOcissb7b/ODrasxEpFwEflBRD7yfu4sIvO9x/wdEYkKdowNSUSSRWSqiKwSkZUiMioUjrWI3OX9+14uIlNEJKYpHmsReVlEdovIcp9pNR5fcZ717v+PIjLkULYVEolARMKBvwNnAH2AS0WkT3Cj8osy4Neq2gcYCdzi3c/fADNVtTsw0/u5KboDWOnz+THgKVXtBuwFrg1KVP7zDPCpqvYCBuL2vUkfaxFJB24HhqlqPyAcmEDTPNavAKdXmVbb8T0D6O593QA8fygbColEAAwH1qrqelUtAd4Gzg1yTA1OVXeo6mLv+324E0M6bl9f9S72KnBeUAL0IxFpB/wCeMn7WYCTganeRZrUfotIEnAC8E8AVS1R1WxC4FjjhtiNFZEIIA7YQRM81qo6G8iqMrm243su8Jo63wHJItKmvtsKlUSQDmzx+bzVO63JEpFOwGBgPtBKVXd4Z+0EWgUrLj96GrgX8Hg/pwDZqlrm/dzUjnlnIAP4l7c67CURiaeJH2tV3QY8AWzGJYAcYBFN+1j7qu34HtE5LlQSQUgRkQRgGnCnqub6zlN3v3CTumdYRM4CdqvqomDHEkARwBDgeVUdDORTpRqoiR7r5rir385AWyCe6tUnIaEhj2+oJIJtQHufz+2805ocEYnEJYE3VXW6d/KuimKi99/dwYrPT44FzhGRjbhqv5Nx9efJ3uoDaHrHfCuwVVXnez9PxSWGpn6sTwE2qGqGqpYC03HHvykfa1+1Hd8jOseFSiL4HujuvbMgCte49EGQY2pw3nrxfwIrVfVJn1kfAFd5318FvB/o2PxJVX+rqu1UtRPu2H6pqpcDXwHjvYs1qf1W1Z3AFhHp6Z00FviJJn6scVVCI0Ukzvv3XrHfTfZYV1Hb8f0AuNJ799BIIMenCungVDUkXsCZwM/AOuD3wY7HT/t4HK6o+COwxPs6E1dfPhNYA3wBtAh2rH78DcYAH3nfdwEWAGuBd4HoYMfXwPs6CFjoPd7vAc1D4VgDfwRWAcuB14HopnisgSm4dpBSXAnw2tqOLyC4OyPXActwd1XVe1vWxYQxxoS4UKkaMsYYUwtLBMYYE+IsERhjTIizRGCMMSHOEoExxoQ4SwTGVCEi5SKyxOfVYB23iUgn394kjTkaRBx8EWNCTqGqDgp2EMYEipUIjKknEdkoIn8RkWUiskBEunmndxKRL739wM8UkQ7e6a1EZIaILPW+RntXFS4iL3r71P9MRGKDtlPGYInAmJrEVqkausRnXo6q9geew/V4CvA34FVVHQC8CTzrnf4s8LWqDsT1A7TCO7078HdV7QtkAxf6dW+MOQh7stiYKkQkT1UTapi+EThZVdd7O/fbqaopIrIHaKOqpd7pO1Q1VUQygHaqWuyzjk7A5+oGFkFE7gMiVfX/ArBrxtTISgTGHBqt5f2hKPZ5X4611Zkgs0RgzKG5xOffed733+J6PQW4HJjjfT8TuAn2j6ecFKggjTkUdiViTHWxIrLE5/OnqlpxC2lzEfkRd1V/qXfabbiRwu7BjRp2tXf6HcBkEbkWd+V/E643SWOOKtZGYEw9edsIhqnqnmDHYkxDsqohY4wJcVYiMMaYEGclAmOMCXGWCIwxJsRZIjDGmBBnicAYY0KcJQJjjAlx/w8qpVdOoR+U/AAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "dataset='conclusion'\n",
    "features_conc, scores_conc, X_train, X_test, y_train, y_test, result_conc = main(dataset, train=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape features para a conclusion - (297889, 16)\n",
      "Shape features para a conclusion - (297889, 5)\n",
      "\n",
      "conclusion - Tamanho da amostra de treino: 238311\n",
      "conclusion - Tamanho da amostra de test: 59578\n",
      "\n",
      " Gradient Boost Hiperparâmetros\n",
      "Num estimators: 200\n",
      "Min samples leaf: 10\n",
      "Min samples splot: 20\n",
      "Max depth: 5\n",
      "Best Score: 0.2552171838114866\n",
      "\n",
      " Random Forest Hiperparâmetros\n",
      "Num estimators: 200\n",
      "Min samples leaf: 20\n",
      "Min samples splot: 40\n",
      "Max depth: 7\n",
      "Best Score: 0.2191141161210636\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-25T12:34:42.550104</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc596cf2769\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"120.487035\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(114.124535 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.974638\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(175.612138 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"243.462242\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(237.099742 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.949845\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(298.587345 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.437448\" xlink:href=\"#mc596cf2769\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(356.893698 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epoch -->\n     <g transform=\"translate(195.870313 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m20092bdced\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m20092bdced\" y=\"222.234123\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 9.0 -->\n      <g transform=\"translate(20.878125 226.033342)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m20092bdced\" y=\"175.67862\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 9.2 -->\n      <g transform=\"translate(20.878125 179.477839)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m20092bdced\" y=\"129.123116\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 9.4 -->\n      <g transform=\"translate(20.878125 132.922335)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m20092bdced\" y=\"82.567613\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 9.6 -->\n      <g transform=\"translate(20.878125 86.366832)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m20092bdced\" y=\"36.01211\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 9.8 -->\n      <g transform=\"translate(20.878125 39.811328)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p4362bcfc53)\" d=\"M 58.999432 32.201761 \nL 62.073812 126.793823 \nL 65.148192 144.658119 \nL 68.222572 155.928085 \nL 71.296952 160.402373 \nL 74.371333 166.37512 \nL 77.445713 170.850962 \nL 80.520093 172.530568 \nL 83.594473 174.389101 \nL 86.668853 188.22443 \nL 89.743233 193.26791 \nL 92.817614 193.664835 \nL 95.891994 195.133103 \nL 98.966374 196.374715 \nL 102.040754 197.944212 \nL 105.115134 198.069639 \nL 108.189514 199.158741 \nL 111.263895 200.739338 \nL 114.338275 200.557525 \nL 117.412655 205.331061 \nL 120.487035 208.282692 \nL 123.561415 210.145221 \nL 126.635795 211.443442 \nL 129.710176 211.725818 \nL 132.784556 211.902747 \nL 135.858936 213.023151 \nL 138.933316 211.436338 \nL 142.007696 211.546447 \nL 145.082076 214.858819 \nL 148.156457 216.414552 \nL 151.230837 218.093936 \nL 154.305217 217.842195 \nL 157.379597 217.424847 \nL 160.453977 218.655359 \nL 163.528357 217.476127 \nL 166.602738 219.458977 \nL 169.677118 219.959795 \nL 172.751498 218.976362 \nL 175.825878 220.315874 \nL 178.900258 219.48606 \nL 181.974638 221.47779 \nL 185.049019 223.376282 \nL 188.123399 222.093601 \nL 191.197779 221.868943 \nL 194.272159 223.315234 \nL 197.346539 223.214892 \nL 200.420919 223.051949 \nL 203.4953 223.358523 \nL 206.56968 223.850017 \nL 209.64406 226.263535 \nL 212.71844 224.727115 \nL 215.79282 224.691152 \nL 218.8672 225.261677 \nL 221.941581 225.49255 \nL 225.015961 225.837751 \nL 228.090341 223.945253 \nL 231.164721 224.967535 \nL 234.239101 225.029915 \nL 237.313481 226.890002 \nL 240.387862 225.359132 \nL 243.462242 225.681467 \nL 246.536622 226.079058 \nL 249.611002 226.335461 \nL 252.685382 225.699449 \nL 255.759762 226.461332 \nL 258.834143 227.114216 \nL 261.908523 225.592226 \nL 264.982903 227.282265 \nL 268.057283 228.172683 \nL 271.131663 228.093209 \nL 274.206043 227.650997 \nL 277.280424 227.202126 \nL 280.354804 227.668757 \nL 283.429184 228.141826 \nL 286.503564 227.959347 \nL 289.577944 226.558787 \nL 292.652324 228.300108 \nL 295.726705 225.88215 \nL 298.801085 227.282487 \nL 301.875465 225.912341 \nL 304.949845 226.707967 \nL 308.024225 226.688654 \nL 311.098605 227.185698 \nL 314.172986 225.628189 \nL 317.247366 227.364181 \nL 320.321746 226.751922 \nL 323.396126 227.473402 \nL 326.470506 225.949636 \nL 329.544886 227.088243 \nL 332.619267 225.816661 \nL 335.693647 227.335766 \nL 338.768027 228.665288 \nL 341.842407 229.874489 \nL 344.916787 228.720786 \nL 347.991167 227.173932 \nL 351.065548 227.858784 \nL 354.139928 227.691844 \nL 357.214308 227.885201 \nL 360.288688 226.90088 \nL 363.363068 226.169854 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p4362bcfc53)\" d=\"M 58.999432 198.313388 \nL 62.073812 223.397371 \nL 65.148192 218.135005 \nL 68.222572 168.252523 \nL 71.296952 206.260105 \nL 74.371333 218.113694 \nL 77.445713 185.189107 \nL 80.520093 205.773495 \nL 83.594473 218.63205 \nL 86.668853 222.079171 \nL 89.743233 221.066879 \nL 92.817614 198.003707 \nL 95.891994 218.9717 \nL 98.966374 217.895696 \nL 102.040754 211.823717 \nL 105.115134 199.585636 \nL 108.189514 201.211963 \nL 111.263895 216.103539 \nL 114.338275 215.672648 \nL 117.412655 228.29145 \nL 120.487035 214.632829 \nL 123.561415 225.316953 \nL 126.635795 219.518471 \nL 129.710176 215.76522 \nL 132.784556 215.950363 \nL 135.858936 216.027395 \nL 138.933316 216.886511 \nL 142.007696 216.608575 \nL 145.082076 208.172583 \nL 148.156457 213.437169 \nL 151.230837 215.016434 \nL 154.305217 217.419297 \nL 157.379597 221.575245 \nL 160.453977 213.740191 \nL 163.528357 210.144333 \nL 166.602738 213.124158 \nL 169.677118 214.114473 \nL 172.751498 218.90932 \nL 175.825878 219.600831 \nL 178.900258 214.151324 \nL 181.974638 214.773795 \nL 185.049019 219.290484 \nL 188.123399 216.119078 \nL 191.197779 216.137504 \nL 194.272159 215.54034 \nL 197.346539 215.170498 \nL 200.420919 214.972035 \nL 203.4953 220.754534 \nL 206.56968 214.947172 \nL 209.64406 213.702452 \nL 212.71844 217.779593 \nL 215.79282 216.975309 \nL 218.8672 214.926749 \nL 221.941581 219.739133 \nL 225.015961 213.649174 \nL 228.090341 216.634104 \nL 231.164721 218.352781 \nL 234.239101 214.318485 \nL 237.313481 211.216564 \nL 240.387862 214.635715 \nL 243.462242 213.883155 \nL 246.536622 214.867254 \nL 249.611002 215.747238 \nL 252.685382 215.680418 \nL 255.759762 214.038773 \nL 258.834143 215.53146 \nL 261.908523 214.655916 \nL 264.982903 213.464253 \nL 268.057283 216.77285 \nL 271.131663 214.856599 \nL 274.206043 216.046264 \nL 277.280424 216.26182 \nL 280.354804 215.575637 \nL 283.429184 215.390494 \nL 286.503564 215.104566 \nL 289.577944 215.106564 \nL 292.652324 215.25308 \nL 295.726705 215.917286 \nL 298.801085 216.363715 \nL 301.875465 215.993208 \nL 304.949845 215.856681 \nL 308.024225 216.002531 \nL 311.098605 216.410778 \nL 314.172986 216.327974 \nL 317.247366 216.694486 \nL 320.321746 216.034054 \nL 323.396126 216.604801 \nL 326.470506 216.403008 \nL 329.544886 215.971674 \nL 332.619267 215.779871 \nL 335.693647 216.046264 \nL 338.768027 216.168583 \nL 341.842407 215.98788 \nL 344.916787 216.202992 \nL 347.991167 216.229409 \nL 351.065548 216.113972 \nL 354.139928 216.093327 \nL 357.214308 216.119522 \nL 360.288688 216.12929 \nL 363.363068 216.169693 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Loos x Epoch - conclusion -->\n    <g transform=\"translate(133.950938 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" id=\"DejaVuSans-78\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"176.326172\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"228.425781\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"260.212891\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"319.392578\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"351.179688\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"414.363281\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"477.839844\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"539.021484\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"594.001953\" xlink:href=\"#DejaVuSans-68\"/>\n     <use x=\"657.380859\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"689.167969\" xlink:href=\"#DejaVuSans-2d\"/>\n     <use x=\"725.251953\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"757.039062\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"812.019531\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"873.201172\" xlink:href=\"#DejaVuSans-6e\"/>\n     <use x=\"936.580078\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"991.560547\" xlink:href=\"#DejaVuSans-6c\"/>\n     <use x=\"1019.34375\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"1082.722656\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"1134.822266\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"1162.605469\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"1223.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 50.78125 59.674375 \nL 106.76875 59.674375 \nQ 108.76875 59.674375 108.76875 57.674375 \nL 108.76875 29.318125 \nQ 108.76875 27.318125 106.76875 27.318125 \nL 50.78125 27.318125 \nQ 48.78125 27.318125 48.78125 29.318125 \nL 48.78125 57.674375 \nQ 48.78125 59.674375 50.78125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 52.78125 35.416562 \nL 72.78125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_15\">\n     <!-- Train -->\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 52.78125 50.094687 \nL 72.78125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_16\">\n     <!-- Test -->\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"44.083984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"105.607422\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"157.707031\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4362bcfc53\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resultados da Avaliação dos modelos na base de teste\n",
    "\n",
    "Cada algoritmo é treinado com o conjunto de hiperparâmetros obtidos anteriormente na base de treino e os resultados na base de teste são apresentados nas tabelas abaixo.\n",
    "\n",
    "É possível ver que a rede MLP e o GB apresentam resultados superiores aos do RF e similares entre si. Esse comportamente se mantém nos testes de sumarização que são apresentados na próxima seção.\n",
    "\n",
    "Linha 0 - MAE\n",
    "\n",
    "Linha 1 - MSE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "result_intro"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           gb          rf         mlp\n",
       "0  163.947292  173.243343  167.101167\n",
       "1    9.447179    9.781518    9.250687"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb</th>\n",
       "      <th>rf</th>\n",
       "      <th>mlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163.947292</td>\n",
       "      <td>173.243343</td>\n",
       "      <td>167.101167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.447179</td>\n",
       "      <td>9.781518</td>\n",
       "      <td>9.250687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "result_mat"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           gb          rf         mlp\n",
       "0  118.328920  121.440611  129.537783\n",
       "1    7.554799    7.652398    7.323716"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb</th>\n",
       "      <th>rf</th>\n",
       "      <th>mlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118.328920</td>\n",
       "      <td>121.440611</td>\n",
       "      <td>129.537783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.554799</td>\n",
       "      <td>7.652398</td>\n",
       "      <td>7.323716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "result_conc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           gb          rf         mlp\n",
       "0  163.125807  171.181866  172.586313\n",
       "1    9.081223    9.327374    9.056795"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb</th>\n",
       "      <th>rf</th>\n",
       "      <th>mlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163.125807</td>\n",
       "      <td>171.181866</td>\n",
       "      <td>172.586313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.081223</td>\n",
       "      <td>9.327374</td>\n",
       "      <td>9.056795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliação da Tarefa de Sumarização\n",
    "\n",
    "Os dados aqui apresentados são relativos a tarefa de sumarização, os testes foram feitos em 71 arquivos selecionados aleatoriamente e esses arquivos não fazem parte da base de treino e teste. Todos or arquivos estão completos e a sumarização é realizada para cada um, separadamente.  \n",
    "\n",
    "Para avaliar os modelos são utilizadas as métricas ROUGE-1, ROUGE-2 e ROUGE-L. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Suppress code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def create_label(df):\n",
    "\n",
    "    label = [0 for i in range(len(df))]\n",
    "\n",
    "    j = 0\n",
    "    for index, row in df.sort_values('predictions', ascending=False).iterrows():\n",
    "      label[index] = 1\n",
    "      j +=1\n",
    "\n",
    "      if j==3:\n",
    "        break\n",
    "\n",
    "    return label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def summarize(X_val, y_val, scaler, model):\n",
    "\n",
    "        columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "                'count_three_gram', 'count_article_keywords', 'tf-isf',\n",
    "                'position_score', 'paragraph_score', 'number_citations', 'length_score',\n",
    "                'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "        #sum_x_test = X_val.loc[X_val['number_text'] == n]\n",
    "        try:\n",
    "                sentences = X_val[0]\n",
    "        except KeyError:\n",
    "                sentences = X_val['0']\n",
    "                \n",
    "        sum_x_test = X_val[columns_name]\n",
    "        sum_y_test = y_val['rouge_1']*100\n",
    "\n",
    "        sum_x_test = scaler.transform(sum_x_test)\n",
    "        y_pred = model.predict(sum_x_test)\n",
    "\n",
    "        summ_df = pd.DataFrame({'sentences': sentences, 'predictions': y_pred.reshape(1, -1)[0]})\n",
    "        summ_df.reset_index(inplace=True, drop=True)\n",
    "        summ_df['label'] = create_label(summ_df)\n",
    "        summary = ' '.join(summ_df.loc[summ_df['label'] == 1]['sentences'].values)\n",
    "\n",
    "        return summ_df, summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import re\n",
    "\n",
    "def get_ref_summary(file_name):\n",
    "\n",
    "    text = loader.load_files(path_base, [file_name])\n",
    "    reference = preprocess.format_xml(text[0].get('sec_abstract'))\n",
    "    reference = re.sub('(?<=<title>)(.*?)(?=</title>)', '', reference)\n",
    "    reference = re.sub(r'[\\t\\n\\r]', '', reference)\n",
    "    reference = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", reference)\n",
    "    reference = preprocess.xml_to_text(reference)\n",
    "    reference = preprocess.format_text(str(reference), post_processing=True)\n",
    "    reference = re.sub(r' +', \" \", reference)\n",
    "    reference = reference.strip()\n",
    "    \n",
    "    return reference"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "def evaluate_summaries(candidate, reference):\n",
    "\n",
    "    rouge_1 = rouge.rouge_n(summary=candidate, references=reference, n=1)\n",
    "    rouge_2 = rouge.rouge_n(summary=candidate, references=reference, n=2)\n",
    "    rouge_l = rouge.rouge_l(summary=candidate, references=reference)\n",
    "\n",
    "    return [rouge_1, rouge_2, rouge_l]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def load_summarize_models(model, dataset):\n",
    "\n",
    "    if model == 'mlp':\n",
    "        json_file = open('model_{}.json'.format(dataset), 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        model.load_weights('model_{}.h5'.format(dataset))\n",
    "    else: \n",
    "        infile = open('{}_{}'.format(model, dataset),'rb')\n",
    "        model = pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "    infile = open('StandardScaler_{}'.format(dataset),'rb')\n",
    "    scaler = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    return model, scaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def summarizer_all_texts(model_intro, scaler_intro, model_mat, scaler_mat, model_conc, scaler_conc, path):\n",
    "\n",
    "    summaries_intro = []\n",
    "    summaries_mat = []\n",
    "    summaries_conc = []\n",
    "    summaries_merged = []\n",
    "    references = []\n",
    "\n",
    "    files = glob.glob(path)\n",
    "    files = [i.split(\"/\")[-1] for i in files]\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        name = file.split('_')[1]\n",
    "\n",
    "        X_intro = pd.read_csv(\"../sumdata/{}/{}\".format('introduction', file))\n",
    "        y_intro =  pd.read_csv(\"../sumdata/{}/scores_{}\".format('introduction', name))\n",
    "\n",
    "        X_mat = pd.read_csv(\"../sumdata/{}/{}\".format('materials', file))\n",
    "        y_mat =  pd.read_csv(\"../sumdata/{}/scores_{}\".format('materials', name))\n",
    "\n",
    "        X_conc = pd.read_csv(\"../sumdata/{}/{}\".format('conclusion', file))\n",
    "        y_conc =  pd.read_csv(\"../sumdata/{}/scores_{}\".format('conclusion', name))\n",
    "\n",
    "        _, summary_intro = summarize(X_intro, y_intro, scaler_intro, model_intro)\n",
    "        _, summary_mat = summarize(X_mat, y_mat, scaler_mat, model_mat)\n",
    "        _, summary_conc = summarize(X_conc, y_conc, scaler_conc, model_conc)\n",
    "        merged = summary_intro + \" \" + summary_mat + \" \" + summary_conc\n",
    "\n",
    "        summaries_intro.append(summary_intro)\n",
    "        summaries_mat.append(summary_mat)\n",
    "        summaries_conc.append(summary_conc)\n",
    "        summaries_merged.append(merged)\n",
    "\n",
    "        name_articles = name.replace(\".csv\", \".json\")\n",
    "\n",
    "        references.append(get_ref_summary(name_articles))\n",
    "\n",
    "\n",
    "    names = [i.split('_')[1].replace(\".csv\", \"\") for i in files]\n",
    "    summaries = pd.DataFrame({\"article\": names, \"reference\": references, \"intro\": summaries_intro, \"mat\": summaries_mat,\"conc\": summaries_conc,\"merged\": summaries_merged})\n",
    "\n",
    "    return summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "  def evaluation_all_summaries(summaries):\n",
    "\n",
    "    all_scores = []\n",
    "    i = 0\n",
    "    for index, row in summaries.iterrows():\n",
    "\n",
    "        scores_intro = evaluate_summaries(row['intro'], row['reference'])\n",
    "        scores_mat = evaluate_summaries(row['mat'], row['reference'])\n",
    "        scores_conc = evaluate_summaries(row['conc'], row['reference'])\n",
    "        scores_merged = evaluate_summaries(row['merged'], row['reference'])\n",
    "\n",
    "        scores = pd.DataFrame({\n",
    "                            \"article\": row['article'],\n",
    "                            \"intro_r1\": scores_intro[0],\n",
    "                            \"intro_r2\": scores_intro[1], \n",
    "                            \"intro_r3\": scores_intro[2],  \n",
    "                            \"mat_r1\": scores_mat[0],\n",
    "                            \"mat_r2\": scores_mat[1],\n",
    "                            \"mat_rl\": scores_mat[2],\n",
    "                            \"conc_r1\": scores_conc[0],\n",
    "                            \"conc_r2\": scores_conc[1],\n",
    "                            \"conc_rl\": scores_conc[2],\n",
    "                            \"merged_r1\": scores_merged[0],\n",
    "                            \"merged_r2\": scores_merged[1],\n",
    "                            \"merged_rl\": scores_merged[2]},  index=[i])\n",
    "\n",
    "        i+=1\n",
    "\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    df = pd.concat(all_scores)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resultados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "mlp_model_intro, scaler_intro = load_summarize_models(model='mlp', dataset='introduction')\n",
    "mlp_model_mat, scaler_mat = load_summarize_models(model='mlp', dataset='materials')\n",
    "mlp_model_conc, scaler_conc = load_summarize_models(model='mlp', dataset='conclusion')\n",
    "\n",
    "path = \"../sumdata/introduction/features_*\"\n",
    "summaries_mlp = summarizer_all_texts(mlp_model_intro, scaler_intro, mlp_model_mat, scaler_mat, mlp_model_conc, scaler_conc, path)\n",
    "summaries_mlp.to_csv(\"summaries_mlp.csv\", index = False)\n",
    "scores_mlp = evaluation_all_summaries(summaries_mlp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "gb_model_intro, _ = load_summarize_models(model='gb', dataset='introduction')\n",
    "gb_model_mat, _ = load_summarize_models(model='gb', dataset='materials')\n",
    "gb_model_conc, _ = load_summarize_models(model='gb', dataset='conclusion')\n",
    "\n",
    "path = \"../sumdata/introduction/features_*\"\n",
    "summaries_gb = summarizer_all_texts(gb_model_intro, scaler_intro, gb_model_mat, scaler_mat, gb_model_conc, scaler_conc, path)\n",
    "summaries_gb.to_csv(\"summaries_gb.csv\", index = False)\n",
    "scores_gb = evaluation_all_summaries(summaries_gb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "rf_model_intro, _ = load_summarize_models(model='rf', dataset='introduction')\n",
    "rf_model_mat, _ = load_summarize_models(model='rf', dataset='materials')\n",
    "rf_model_conc, _ = load_summarize_models(model='rf', dataset='conclusion')\n",
    "\n",
    "path = \"../sumdata/introduction/features_*\"\n",
    "summaries_rf = summarizer_all_texts(rf_model_intro, scaler_intro, rf_model_mat, scaler_mat, rf_model_conc, scaler_conc, path)\n",
    "summaries_rf.to_csv(\"summaries_rf.csv\", index = False)\n",
    "scores_rf = evaluation_all_summaries(summaries_rf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "scores_mlp.describe().T"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           count      mean       std       min       25%       50%       75%  \\\n",
       "intro_r1    71.0  0.284745  0.092789  0.083770  0.216374  0.294118  0.357375   \n",
       "intro_r2    71.0  0.108754  0.079902  0.000000  0.054670  0.091324  0.144107   \n",
       "intro_r3    71.0  0.183563  0.072115  0.041885  0.133894  0.173469  0.218081   \n",
       "mat_r1      71.0  0.141949  0.078060  0.000000  0.086800  0.120482  0.184820   \n",
       "mat_r2      71.0  0.041478  0.050602  0.000000  0.010211  0.024691  0.055972   \n",
       "mat_rl      71.0  0.097280  0.056947  0.000000  0.058968  0.090000  0.129454   \n",
       "conc_r1     71.0  0.257959  0.070991  0.071006  0.216448  0.255319  0.314650   \n",
       "conc_r2     71.0  0.089224  0.051907  0.000000  0.046465  0.083721  0.131373   \n",
       "conc_rl     71.0  0.167098  0.047582  0.047337  0.136054  0.163743  0.205670   \n",
       "merged_r1   71.0  0.367071  0.076714  0.226721  0.312228  0.367647  0.423158   \n",
       "merged_r2   71.0  0.135148  0.069655  0.012121  0.085050  0.120755  0.178731   \n",
       "merged_rl   71.0  0.202441  0.063514  0.107784  0.161001  0.189189  0.236979   \n",
       "\n",
       "                max  \n",
       "intro_r1   0.500000  \n",
       "intro_r2   0.430769  \n",
       "intro_r3   0.469697  \n",
       "mat_r1     0.364372  \n",
       "mat_r2     0.233333  \n",
       "mat_rl     0.299595  \n",
       "conc_r1    0.406504  \n",
       "conc_r2    0.185792  \n",
       "conc_rl    0.278607  \n",
       "merged_r1  0.544850  \n",
       "merged_r2  0.354515  \n",
       "merged_rl  0.431894  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intro_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.284745</td>\n",
       "      <td>0.092789</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.216374</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.357375</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.108754</td>\n",
       "      <td>0.079902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054670</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>0.144107</td>\n",
       "      <td>0.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r3</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.183563</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.133894</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.218081</td>\n",
       "      <td>0.469697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.184820</td>\n",
       "      <td>0.364372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.041478</td>\n",
       "      <td>0.050602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.055972</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.097280</td>\n",
       "      <td>0.056947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>0.299595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.257959</td>\n",
       "      <td>0.070991</td>\n",
       "      <td>0.071006</td>\n",
       "      <td>0.216448</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>0.406504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.089224</td>\n",
       "      <td>0.051907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046465</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.131373</td>\n",
       "      <td>0.185792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.167098</td>\n",
       "      <td>0.047582</td>\n",
       "      <td>0.047337</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.205670</td>\n",
       "      <td>0.278607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.367071</td>\n",
       "      <td>0.076714</td>\n",
       "      <td>0.226721</td>\n",
       "      <td>0.312228</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.423158</td>\n",
       "      <td>0.544850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.135148</td>\n",
       "      <td>0.069655</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.085050</td>\n",
       "      <td>0.120755</td>\n",
       "      <td>0.178731</td>\n",
       "      <td>0.354515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.202441</td>\n",
       "      <td>0.063514</td>\n",
       "      <td>0.107784</td>\n",
       "      <td>0.161001</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.236979</td>\n",
       "      <td>0.431894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "scores_gb.describe().T"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           count      mean       std       min       25%       50%       75%  \\\n",
       "intro_r1    71.0  0.288521  0.089273  0.115942  0.222813  0.287234  0.354921   \n",
       "intro_r2    71.0  0.110267  0.075131  0.000000  0.050651  0.101266  0.147990   \n",
       "intro_r3    71.0  0.187197  0.074604  0.072464  0.131602  0.186667  0.219219   \n",
       "mat_r1      71.0  0.139153  0.071048  0.000000  0.093455  0.125786  0.167015   \n",
       "mat_r2      71.0  0.040277  0.049092  0.000000  0.010211  0.024845  0.052185   \n",
       "mat_rl      71.0  0.095860  0.056819  0.000000  0.061576  0.082474  0.113402   \n",
       "conc_r1     71.0  0.258114  0.070387  0.083333  0.208825  0.259414  0.306164   \n",
       "conc_r2     71.0  0.087115  0.050833  0.000000  0.044806  0.083721  0.124578   \n",
       "conc_rl     71.0  0.171213  0.051194  0.070175  0.134961  0.167401  0.206897   \n",
       "merged_r1   71.0  0.367378  0.084124  0.193548  0.305329  0.379447  0.427123   \n",
       "merged_r2   71.0  0.135957  0.074043  0.021739  0.081989  0.122867  0.173681   \n",
       "merged_rl   71.0  0.202566  0.071084  0.086022  0.155705  0.187845  0.233705   \n",
       "\n",
       "                max  \n",
       "intro_r1   0.528571  \n",
       "intro_r2   0.420290  \n",
       "intro_r3   0.514286  \n",
       "mat_r1     0.364372  \n",
       "mat_r2     0.242424  \n",
       "mat_rl     0.300429  \n",
       "conc_r1    0.435644  \n",
       "conc_r2    0.241206  \n",
       "conc_rl    0.308458  \n",
       "merged_r1  0.555556  \n",
       "merged_r2  0.421053  \n",
       "merged_rl  0.490196  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intro_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>0.089273</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.222813</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.354921</td>\n",
       "      <td>0.528571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.110267</td>\n",
       "      <td>0.075131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050651</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.147990</td>\n",
       "      <td>0.420290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r3</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.187197</td>\n",
       "      <td>0.074604</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.131602</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.219219</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.071048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093455</td>\n",
       "      <td>0.125786</td>\n",
       "      <td>0.167015</td>\n",
       "      <td>0.364372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>0.049092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.052185</td>\n",
       "      <td>0.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.095860</td>\n",
       "      <td>0.056819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061576</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.300429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.258114</td>\n",
       "      <td>0.070387</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.259414</td>\n",
       "      <td>0.306164</td>\n",
       "      <td>0.435644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.087115</td>\n",
       "      <td>0.050833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.124578</td>\n",
       "      <td>0.241206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.171213</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>0.167401</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.308458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.367378</td>\n",
       "      <td>0.084124</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.305329</td>\n",
       "      <td>0.379447</td>\n",
       "      <td>0.427123</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.135957</td>\n",
       "      <td>0.074043</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.081989</td>\n",
       "      <td>0.122867</td>\n",
       "      <td>0.173681</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.202566</td>\n",
       "      <td>0.071084</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.155705</td>\n",
       "      <td>0.187845</td>\n",
       "      <td>0.233705</td>\n",
       "      <td>0.490196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "scores_rf.describe().T"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           count      mean       std       min       25%       50%       75%  \\\n",
       "intro_r1    71.0  0.283802  0.084546  0.115942  0.222222  0.275000  0.346786   \n",
       "intro_r2    71.0  0.110161  0.072852  0.000000  0.064161  0.100000  0.137533   \n",
       "intro_r3    71.0  0.187552  0.070517  0.072464  0.137934  0.181034  0.219092   \n",
       "mat_r1      71.0  0.134302  0.070506  0.000000  0.088254  0.133333  0.168182   \n",
       "mat_r2      71.0  0.037262  0.044938  0.000000  0.007766  0.025641  0.048812   \n",
       "mat_rl      71.0  0.092474  0.051349  0.000000  0.054487  0.086957  0.113402   \n",
       "conc_r1     71.0  0.251143  0.075040  0.094737  0.200000  0.247934  0.306592   \n",
       "conc_r2     71.0  0.087336  0.055779  0.000000  0.047603  0.084848  0.116212   \n",
       "conc_rl     71.0  0.163517  0.057974  0.063158  0.120798  0.155689  0.195033   \n",
       "merged_r1   71.0  0.354518  0.084451  0.151351  0.290412  0.359551  0.424450   \n",
       "merged_r2   71.0  0.133894  0.072551  0.021858  0.084414  0.125000  0.165076   \n",
       "merged_rl   71.0  0.199752  0.068669  0.086486  0.157365  0.188976  0.227567   \n",
       "\n",
       "                max  \n",
       "intro_r1   0.528571  \n",
       "intro_r2   0.420290  \n",
       "intro_r3   0.514286  \n",
       "mat_r1     0.321951  \n",
       "mat_r2     0.232759  \n",
       "mat_rl     0.271186  \n",
       "conc_r1    0.477064  \n",
       "conc_r2    0.251429  \n",
       "conc_rl    0.305085  \n",
       "merged_r1  0.541254  \n",
       "merged_r2  0.418605  \n",
       "merged_rl  0.475248  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intro_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.283802</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.346786</td>\n",
       "      <td>0.528571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.110161</td>\n",
       "      <td>0.072852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064161</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.137533</td>\n",
       "      <td>0.420290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intro_r3</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.187552</td>\n",
       "      <td>0.070517</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.137934</td>\n",
       "      <td>0.181034</td>\n",
       "      <td>0.219092</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.134302</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088254</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.168182</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.037262</td>\n",
       "      <td>0.044938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.048812</td>\n",
       "      <td>0.232759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mat_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.092474</td>\n",
       "      <td>0.051349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054487</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.113402</td>\n",
       "      <td>0.271186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.251143</td>\n",
       "      <td>0.075040</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.306592</td>\n",
       "      <td>0.477064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.087336</td>\n",
       "      <td>0.055779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047603</td>\n",
       "      <td>0.084848</td>\n",
       "      <td>0.116212</td>\n",
       "      <td>0.251429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conc_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.163517</td>\n",
       "      <td>0.057974</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.120798</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>0.195033</td>\n",
       "      <td>0.305085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.354518</td>\n",
       "      <td>0.084451</td>\n",
       "      <td>0.151351</td>\n",
       "      <td>0.290412</td>\n",
       "      <td>0.359551</td>\n",
       "      <td>0.424450</td>\n",
       "      <td>0.541254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_r2</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.133894</td>\n",
       "      <td>0.072551</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>0.084414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.165076</td>\n",
       "      <td>0.418605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merged_rl</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.199752</td>\n",
       "      <td>0.068669</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.157365</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>0.227567</td>\n",
       "      <td>0.475248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exemplos de resumos gerados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "summaries_mlp =pd.read_csv(\"summaries_mlp.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "n = 30"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "#Ground Truth\n",
    "summaries_mlp['reference'][n]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The present research examined developmental and gender differences in the relative accessibility of different gender stereotype domains. A Northeastern US sample of children ages to years old provided open ended descriptions of girls and boys. Responses were coded by domain to examine differences by grade, gender of participant, and gender of target. Analyses revealed that girls and older children provided a higher proportion of stereotypes, and that appearance stereotypes were particularly prevalent in descriptions of girls and activity/trait stereotypes were more prevalent in descriptions of boys. Results are discussed in terms of implications for research on the stereotype knowledgebehavior link and the need for more attention to the role of appearance stereotypes in the gender stereotype literature.'"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "summaries_mlp['intro'][n]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'To the best of our knowledge, no study to date has examined accessibility differences of gender stereotype domains in young children. Children’s gender concepts may be similarly differentiated in structure, which could have important implications for how we understand gender stereotype development in children . Therefore, boys may endorse stronger gender stereotypical preferences than girls, but only in the domain of activities and toys.'"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "summaries_mlp['mat'][n]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Prior research on gender stereotype domains guided the development of the coding scheme that was used to categorize children’s responses. For the present study, children’s responses were coded into general categories based on their stereotyped nature and into sub categories based on their domains. These domain specific proportions were calculated by dividing the number of stereotyped responses for a given domain divided by the total number of statements .'"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "summaries_mlp['conc'][0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'We conclude that the specific movement of grk RNA particles in the nurse cells along straight paths to the ring canals is MT dependent . We conclude that the transport of grk RNA within the nurse cells towards ring canals and from the nurse cell to the oocyte is Dynein dependent . Within nurse cells, we have identified a new path of Dynein dependent transport to the ring canals that link the nurse cells to the oocyte.'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusão \n",
    "\n",
    "O maior desafio encontrado na etapa de treinamento foi o ajuste da rede MLP. Inicialmente, foi utilizada uma arquitetura apenas com duas camadas. Quando o treinamento era realizado por poucas epócas o modelo não convergia e todas as predições era a média de todos os valores. Quando realizava o treinamento por muitas epócas (+ 500) o modelo convergia na base de treino porém não generalizava, ou seja, ele só decorava os dados de treino. Posteriormente, eu utilizei a técnica Dropout com intuito evitar overfitting. A partir desse momento o modelo não sofria overfitting, mas também não aprendia. Diante disso, foram feitos diversos ajustes a fim de obter um melhor resultado sem gerar overfitting, como: \n",
    "\n",
    "- Número de camadas da rede;\n",
    "- Número de neurônios nas camadas\n",
    "- Alterar o otimizador utilizado;\n",
    "- Alterar função de ativação;\n",
    "- Regular taxa de aprendizagem;\n",
    "- Alterar o modo de inicialização dos pesos;\n",
    "- Remover features;\n",
    "- Ajuste do Dropout;\n",
    "- Alterar número o tamanho dos batches;\n",
    "- Alterar função de perda;\n",
    "- Adicionar o clip;\n",
    "- Adicionar um Learning Rate Scheduler;\n",
    "- Adicionar EarlyStopping.\n",
    "\n",
    "Dentre todos esses ajustes, os que fizeram maior diferença o aumento da complexidade do modelo juntamente com o Dropout. Desde o incío eu utilizei o Adam Optimizer e acabei voltando pra ele, isso também ocorreu para as funções de ativação eu comecei utilizando relu e acabei voltando pra ela. A melhor estratégia que eu encontrei pra ajustar a taxa de aprendizagem foi a utilização do Learning Rate Scheduler. Ao iniciar o treinamento, a função de perda diminuia e o erro na validação também diminuia, contudo, no decorrer do processo de aprendizagem começava a aumentar. Nesse momente, eu adicionei o Early Stopping porém mesmo com um patience=20, o treinamento parava precocemente e muitas vezes com um erro alto. Sendo assim, eu adicionei o Learning Rate Scheduler como uma tentativa de realizar um ajuste fino no final e tentar previnir a parada precoce com um erro alto. Outro ajuste realizado que ajudou a melhorar o desempenho foi a alteração do modo de inicialização dos pesos. Com a configuração default, a maioria das predições eram o mesmo valor sendo esse aproximadamente a média para todas as predições. Esse predição possui um erro baixo, porém não é util. Para evitar isso, foi alterado o kernel de inicialização de Normal para GlorotUniform. A diferença nos resultados foi muito grande. Antes desse ajuste a maioria das predições era o valor médio e alguns possuiam variações muito pequenas. Com a mudança do kernel já é possível ver um mudança muito mais significativa no valor das predições. Os resultados obtidos estão longe do que considero satisfatório, contudo eu acredito que há diversos outros ajustes que podem ser realizados para gerar melhores resultados.\n",
    "\n",
    "Além disso, foi possível ver uma superioridade nos resultados obtidos com a rede MLP e com GB em relação ao RF. A rede MLP e o GB apresentaram uma diferença muito pequena tanto na avaliação do modelos com as métricas MAE e MSE quanto na avaliação do desempenho do modelo na tarefa de sumarização."
   ],
   "metadata": {}
  }
 ]
}