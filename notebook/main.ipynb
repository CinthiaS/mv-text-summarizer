{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/media/cinthia/Dados/Mestrado/mv-text-summarizer')\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import smogn\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pysbd.utils import PySBDFactory\n",
    "import math\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = \"../../sumdata/dataset_articles\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "from src import ensemble_tree_models\n",
    "from src import tunning_hyperparametrs as th\n",
    "from src import mlp_regressor\n",
    "from src import mlp_classifier\n",
    "from src import summarization\n",
    "from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experimentos e Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extração das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name_section, initial_batch, embed_dim, batch_len, path_basem verbose):\n",
    "    \n",
    "    features_columns = ['sentences', 'text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords', 'tf-isf',\n",
    "       'position_score', 'paragraph_score', 'number_citations', 'length_score',\n",
    "       'pos_score', 'ner_score', 'dist_centroid', 'articles']\n",
    "    \n",
    "    scores_columns = ['rouge_1', 'rouge_2', 'rouge_l', 'label', 'articles']\n",
    "    \n",
    "    embeddings_columns = [i for i in range(embed_dim)]\n",
    "    embeddings_columns.append(\"article\")\n",
    "    \n",
    "\n",
    "    if os.path.isfile('batches_{}.json'.format(batch_len)):\n",
    "\n",
    "        print(\"Load Batch Files\")\n",
    "        with open('batches_{}.json'.format(batch_len) as f:\n",
    "            batches = json.load(f)\n",
    "\n",
    "        batch_files = [value for key, value in batches.items()]\n",
    "    else: \n",
    "\n",
    "        print(\"Creating Batch Files\")\n",
    "        batch_files = create_batches(path_base, tam=batch_len)\n",
    "        utils.save_batches(batch_files)\n",
    "        \n",
    "\n",
    "    batche_files = batch_files[args.initial_batch:]\n",
    "    extract_features_batches(\n",
    "        batch_files, path_base, name_section=name_section, initial_batch=initial_batch, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name section: conclusion\n",
      "Iniciando a extração de features...\n",
      "Batch: 36 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1003.707691265\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 37 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1044.7837589849996\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 38 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1038.4883319669998\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 39 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 984.2681289150005\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 40 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 987.2068271289991\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 41 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1069.3134625249986\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 42 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1026.3326052150005\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 43 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1074.9373005910002\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 44 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n",
      "Time: 1365.2643348950005\n",
      "Quantidade de arquivos processados: 159\n",
      "Saving Results\n",
      "Batch: 45 \n",
      "\n",
      "Total de arquivos: (159, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_batch=36\n",
    "batche_files = batch_files[initial_batch:]\n",
    "\n",
    "path_base = \"../../sumdata/dataset_articles\"\n",
    "extract_features_batches(batch_files, path_base, name_section='conclusion', initial_batch=initial_batch, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "#valid_len = int(len(comuns)*0.2)\n",
    "#summ_items = random.sample(comuns, valid_len)\n",
    "\n",
    "#df = pd.DataFrame({'summ': summ_items})\n",
    "#df.to_csv(\"indices_summ.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_regression(dataset):\n",
    "\n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram',\n",
    "        'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    features, scores = loader.read_features(path=\"../result/{}/features_*.csv\".format(dataset))\n",
    "\n",
    "    summ_items = list(pd.read_csv(\"indices_summ.csv\")['summ'])\n",
    "\n",
    "    data = utils.join_dataset(features, scores)\n",
    "    train, test = utils.split_dataset (data, summ_items)\n",
    "\n",
    "    X_train, scaler = normalization.scale_fit_transform(X_train, section='scaler_{}_regress'.format(dataset))\n",
    "    y_train = train['rouge_1']*100\n",
    "    X_train, y_train = utils_clf.shuffle_dataset(X_train, y_train)\n",
    "    \n",
    "    X_test = scaler.transform(test[columns_name])\n",
    "    y_test = test['rouge_1']*100\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path='.', format_dataset=True, verbose=True):\n",
    "    \n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram',\n",
    "        'count_article_keywords', 'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    sections=['introduction', 'materials', 'conclusion', 'concat']\n",
    "    #sections=[ 'introduction']\n",
    "\n",
    "    if format_dataset:\n",
    "        if verbose:\n",
    "            print(\"Preparando dataset para os classificadores\")\n",
    "        dataset = prepare_data.main_create_dataset(columns_name, sections)\n",
    "        #utils.save_json(dataset, name='dataset', path=path)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Carregando dataset\")\n",
    "        dataset = utils.load_json(name='dataset', path=path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Treinamento dos modelos\")\n",
    "        \n",
    "    return dataset\n",
    "    \n",
    "    '''models = ensemble_tree_models.create_models(dataset, sections, name_models=['knn', 'gb', 'rf', 'ab'])\n",
    "    predictions, results = ev.create_reports(models, dataset, columns_name, verbose=False)\n",
    "    \n",
    "    #utils.save_json(predictions, name='prediction', path=path)\n",
    "    #utils.save_results(results, path=path)\n",
    "    \n",
    "    parameters = {'introduction': [0.2, 100, 64],\n",
    "             'materials': [0.2, 100, 64],\n",
    "             'conclusion':[0.2, 100, 64],\n",
    "             'concat': [0.2, 100, 64]}\n",
    "\n",
    "    models_nn = mlp_classifiers.main_train_nn(dataset, sections, parameters, train=True, verbose=False)\n",
    "    predictions, results =  mlp_classifiers.eval_nn(dataset, sections)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dataset para os classificadores\n",
      "Treinamento dos modelos\n"
     ]
    }
   ],
   "source": [
    "dataset = main(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['text_rank', 'lex_rank', 'count_one_gram',\n",
    "        'count_article_keywords', 'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "sections=['introduction', 'materials', 'conclusion', 'concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ensemble_tree_models.create_models(dataset, sections, name_models=['knn', 'gb', 'rf', 'ab'])\n",
    "predictions, results = ev.create_reports(models, dataset, columns_name, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'introduction': [0.2, 100, 64],\n",
    "             'materials': [0.2, 100, 64],\n",
    "             'conclusion':[0.2, 100, 64],\n",
    "             'concat': [0.2, 100, 64]}\n",
    "\n",
    "models_nn = mlp_classifiers.main_train_nn(dataset, sections, parameters, train=True, verbose=False)\n",
    "predictions, results =  mlp_classifiers.eval_nn(dataset, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.load_json(name='dataset', path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--path', \"-p\",required=True)\n",
    "    parser.add_argument('--format_dataset', \"-f\",required=True)\n",
    "     \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram',\n",
    "        'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    sections=['introduction', 'materials', 'conclusion', 'concat']\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    ">>> import numpy as np\n",
    ">>> X = np.array([[1, 2], [2, 2], [2, 3],\n",
    "...               [8, 7], [8, 8], [25, 80]])\n",
    ">>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    ">>> clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(dataset)\n",
    "lustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_regression(dataset_name, X, y):\n",
    "    \n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    knn = KNeighborsRegressor(n_neighbors=5).fit(X, y)\n",
    "    #svm = SVC().fit(X, y)\n",
    "    gbr = classifiers.fit_model(name_model='gb_r', dataset=dataset_name, X_train=X, y_train=y,\n",
    "                                n_estimators=50, min_samples_leaf=100, min_samples_split=200, max_depth=20)\n",
    "    rfr = classifiers.fit_model(name_model='rf_r', dataset=dataset_name, X_train=X, y_train=y,\n",
    "                                n_estimators=50, min_samples_leaf=100, min_samples_split=200, max_depth=20)\n",
    "    \n",
    "    return lr, knn, gbr, rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_intro, knnr_intro, gbr_intro, rfr_intro = pipeline_regression('intro', X_train_intro, y_train_intro)\n",
    "lr_mat, knnr_mat, gbr_mat, rfr_mat = pipeline_regression('mat', X_train_mat, y_train_mat)\n",
    "lr_conc, knnr_conc, gbr_conc, rfr_conc = pipeline_regression('conc', X_train_conc, y_train_conc)\n",
    "lr_concat, knnr_concat, gbr_concat, rfr_concat = pipeline_regression('concat', X_concat_train, y_concat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fa890f71070>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.train_neural_model(\n",
    "    dataset='intro', view='features', X_train=X_train_intro, y_train= y_train_intro, input_shape=11,\n",
    "    validation_split=0.2, learning_rate=0.01,\n",
    "         epochs=300, batch_size=64, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fa87bf2a5e0>"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.train_neural_model(\n",
    "    dataset='mat', view='features', X_train=X_train_mat, y_train= y_train_mat, input_shape=11,\n",
    "    validation_split=0.2, learning_rate=0.01,\n",
    "         epochs=300, batch_size=64, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fa87b1e05e0>"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.train_neural_model(\n",
    "    dataset='conc', view='features', X_train=X_train_conc, y_train= y_train_conc, input_shape=11,\n",
    "    validation_split=0.2, learning_rate=0.01,\n",
    "         epochs=300, batch_size=64, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fa87b9b9fa0>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.train_neural_model(\n",
    "    dataset='concat', view='features', X_train=X_concat_train, y_train= y_concat_train, input_shape=11,\n",
    "    validation_split=0.2, learning_rate=0.01,\n",
    "         epochs=300, batch_size=64, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'lr':[lr_intro, lr_mat, lr_conc, lr_concat],\n",
    "    'knn': [knnr_intro, knnr_mat, knnr_conc, knnr_concat],\n",
    "    'gb': [gbr_intro, gbr_mat, gbr_conc, gbr_concat],\n",
    "    'rf': [rfr_intro, rfr_mat, rfr_conc, rfr_concat]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'intro': [X_summ_intro, y_summ_intro], \n",
    "     'mat': [X_summ_mat, y_summ_mat], \n",
    "     'conc': [X_summ_conc, y_summ_conc]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['intro'].append(pickle.load(open('scale_{}.pkl'.format('intro'), 'rb')))\n",
    "dataset['mat'].append(pickle.load(open('scale_{}.pkl'.format('mat'), 'rb')))\n",
    "dataset['conc'].append(pickle.load(open('scale_{}.pkl'.format('conc'), 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summaries(r, dataset, columns_name, summ_items, path_base):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i in r.keys():\n",
    "\n",
    "        models = r[i]\n",
    "\n",
    "        summaries, intro, mat, conc = summarization.pipeline_summarization(\n",
    "            i, models, dataset, columns_name, summ_items, path_base, sections=['intro', 'mat', 'conc', 'comb'])\n",
    "\n",
    "        results[i] = [summaries, intro, mat, conc]\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = create_summaries(models, dataset, columns_name, summ_items, path_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro_r1</th>\n",
       "      <th>intro_r2</th>\n",
       "      <th>intro_rl</th>\n",
       "      <th>mat_r1</th>\n",
       "      <th>mat_r2</th>\n",
       "      <th>mat_rl</th>\n",
       "      <th>conc_r1</th>\n",
       "      <th>conc_r2</th>\n",
       "      <th>conc_rl</th>\n",
       "      <th>concat_r1</th>\n",
       "      <th>concat_r2</th>\n",
       "      <th>concat_rl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.194058</td>\n",
       "      <td>0.070973</td>\n",
       "      <td>0.135773</td>\n",
       "      <td>0.109460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085101</td>\n",
       "      <td>0.145153</td>\n",
       "      <td>0.034170</td>\n",
       "      <td>0.098801</td>\n",
       "      <td>0.248285</td>\n",
       "      <td>0.080143</td>\n",
       "      <td>0.147645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.086495</td>\n",
       "      <td>0.064422</td>\n",
       "      <td>0.066263</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070157</td>\n",
       "      <td>0.076419</td>\n",
       "      <td>0.040349</td>\n",
       "      <td>0.051966</td>\n",
       "      <td>0.089790</td>\n",
       "      <td>0.073903</td>\n",
       "      <td>0.072677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.134669</td>\n",
       "      <td>0.030189</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041505</td>\n",
       "      <td>0.090295</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.064132</td>\n",
       "      <td>0.192424</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.106515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.189039</td>\n",
       "      <td>0.057708</td>\n",
       "      <td>0.124647</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072690</td>\n",
       "      <td>0.135375</td>\n",
       "      <td>0.023393</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.241822</td>\n",
       "      <td>0.064020</td>\n",
       "      <td>0.136037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.245761</td>\n",
       "      <td>0.094376</td>\n",
       "      <td>0.170294</td>\n",
       "      <td>0.148838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109299</td>\n",
       "      <td>0.197613</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>0.126374</td>\n",
       "      <td>0.295584</td>\n",
       "      <td>0.095501</td>\n",
       "      <td>0.169324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.583851</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.448598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403162</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.697872</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.655319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intro_r1    intro_r2    intro_rl      mat_r1  mat_r2      mat_rl  \\\n",
       "count  442.000000  442.000000  442.000000  442.000000   442.0  442.000000   \n",
       "mean     0.194058    0.070973    0.135773    0.109460     0.0    0.085101   \n",
       "std      0.086495    0.064422    0.066263    0.087211     0.0    0.070157   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.0    0.000000   \n",
       "25%      0.134669    0.030189    0.095238    0.045767     0.0    0.041505   \n",
       "50%      0.189039    0.057708    0.124647    0.097391     0.0    0.072690   \n",
       "75%      0.245761    0.094376    0.170294    0.148838     0.0    0.109299   \n",
       "max      0.583851    0.490566    0.559006    0.448598     0.0    0.403162   \n",
       "\n",
       "          conc_r1     conc_r2     conc_rl   concat_r1   concat_r2   concat_rl  \n",
       "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000  \n",
       "mean     0.145153    0.034170    0.098801    0.248285    0.080143    0.147645  \n",
       "std      0.076419    0.040349    0.051966    0.089790    0.073903    0.072677  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.090295    0.008475    0.064132    0.192424    0.040468    0.106515  \n",
       "50%      0.135375    0.023393    0.088799    0.241822    0.064020    0.136037  \n",
       "75%      0.197613    0.047621    0.126374    0.295584    0.095501    0.169324  \n",
       "max      0.444444    0.404494    0.444444    0.697872    0.609442    0.655319  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['lr'][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro_r1</th>\n",
       "      <th>intro_r2</th>\n",
       "      <th>intro_rl</th>\n",
       "      <th>mat_r1</th>\n",
       "      <th>mat_r2</th>\n",
       "      <th>mat_rl</th>\n",
       "      <th>conc_r1</th>\n",
       "      <th>conc_r2</th>\n",
       "      <th>conc_rl</th>\n",
       "      <th>concat_r1</th>\n",
       "      <th>concat_r2</th>\n",
       "      <th>concat_rl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.191479</td>\n",
       "      <td>0.060221</td>\n",
       "      <td>0.127736</td>\n",
       "      <td>0.070825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057321</td>\n",
       "      <td>0.145824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103902</td>\n",
       "      <td>0.251737</td>\n",
       "      <td>0.075088</td>\n",
       "      <td>0.149548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.077759</td>\n",
       "      <td>0.058224</td>\n",
       "      <td>0.057450</td>\n",
       "      <td>0.070325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.082883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056607</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>0.073820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.136519</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.017897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017739</td>\n",
       "      <td>0.086589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064206</td>\n",
       "      <td>0.197789</td>\n",
       "      <td>0.036890</td>\n",
       "      <td>0.113636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.183383</td>\n",
       "      <td>0.046225</td>\n",
       "      <td>0.120109</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046620</td>\n",
       "      <td>0.143646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103045</td>\n",
       "      <td>0.244966</td>\n",
       "      <td>0.059652</td>\n",
       "      <td>0.137913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.078377</td>\n",
       "      <td>0.153636</td>\n",
       "      <td>0.095676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076801</td>\n",
       "      <td>0.198792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>0.300654</td>\n",
       "      <td>0.094716</td>\n",
       "      <td>0.173067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.447489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.450704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.697872</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.655319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intro_r1    intro_r2    intro_rl      mat_r1  mat_r2      mat_rl  \\\n",
       "count  442.000000  442.000000  442.000000  442.000000   442.0  442.000000   \n",
       "mean     0.191479    0.060221    0.127736    0.070825     0.0    0.057321   \n",
       "std      0.077759    0.058224    0.057450    0.070325     0.0    0.057498   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.0    0.000000   \n",
       "25%      0.136519    0.023952    0.091013    0.017897     0.0    0.017739   \n",
       "50%      0.183383    0.046225    0.120109    0.057143     0.0    0.046620   \n",
       "75%      0.235294    0.078377    0.153636    0.095676     0.0    0.076801   \n",
       "max      0.494737    0.489362    0.494737    0.447489     0.0    0.383562   \n",
       "\n",
       "          conc_r1  conc_r2     conc_rl   concat_r1   concat_r2   concat_rl  \n",
       "count  442.000000    442.0  442.000000  442.000000  442.000000  442.000000  \n",
       "mean     0.145824      0.0    0.103902    0.251737    0.075088    0.149548  \n",
       "std      0.082883      0.0    0.056607    0.087705    0.074684    0.073820  \n",
       "min      0.000000      0.0    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.086589      0.0    0.064206    0.197789    0.036890    0.113636  \n",
       "50%      0.143646      0.0    0.103045    0.244966    0.059652    0.137913  \n",
       "75%      0.198792      0.0    0.134720    0.300654    0.094716    0.173067  \n",
       "max      0.450704      0.0    0.444444    0.697872    0.609442    0.655319  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['knn'][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro_r1</th>\n",
       "      <th>intro_r2</th>\n",
       "      <th>intro_rl</th>\n",
       "      <th>mat_r1</th>\n",
       "      <th>mat_r2</th>\n",
       "      <th>mat_rl</th>\n",
       "      <th>conc_r1</th>\n",
       "      <th>conc_r2</th>\n",
       "      <th>conc_rl</th>\n",
       "      <th>concat_r1</th>\n",
       "      <th>concat_r2</th>\n",
       "      <th>concat_rl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.188994</td>\n",
       "      <td>0.060542</td>\n",
       "      <td>0.127675</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063755</td>\n",
       "      <td>0.159980</td>\n",
       "      <td>0.043281</td>\n",
       "      <td>0.111020</td>\n",
       "      <td>0.251597</td>\n",
       "      <td>0.076973</td>\n",
       "      <td>0.151307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.056720</td>\n",
       "      <td>0.059202</td>\n",
       "      <td>0.080131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064210</td>\n",
       "      <td>0.092191</td>\n",
       "      <td>0.053815</td>\n",
       "      <td>0.065875</td>\n",
       "      <td>0.091665</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>0.076669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.137657</td>\n",
       "      <td>0.024730</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.023988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.069782</td>\n",
       "      <td>0.193722</td>\n",
       "      <td>0.036675</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.179443</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.118785</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050569</td>\n",
       "      <td>0.154643</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>0.242427</td>\n",
       "      <td>0.059353</td>\n",
       "      <td>0.137791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.233296</td>\n",
       "      <td>0.079298</td>\n",
       "      <td>0.151803</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084619</td>\n",
       "      <td>0.221995</td>\n",
       "      <td>0.063456</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>0.302510</td>\n",
       "      <td>0.094003</td>\n",
       "      <td>0.170758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382514</td>\n",
       "      <td>0.657005</td>\n",
       "      <td>0.614634</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.832061</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.793893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intro_r1    intro_r2    intro_rl      mat_r1  mat_r2      mat_rl  \\\n",
       "count  442.000000  442.000000  442.000000  442.000000   442.0  442.000000   \n",
       "mean     0.188994    0.060542    0.127675    0.081096     0.0    0.063755   \n",
       "std      0.076994    0.056720    0.059202    0.080131     0.0    0.064210   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.0    0.000000   \n",
       "25%      0.137657    0.024730    0.086957    0.023988     0.0    0.021478   \n",
       "50%      0.179443    0.048193    0.118785    0.061856     0.0    0.050569   \n",
       "75%      0.233296    0.079298    0.151803    0.111801     0.0    0.084619   \n",
       "max      0.494737    0.489362    0.494737    0.453333     0.0    0.382514   \n",
       "\n",
       "          conc_r1     conc_r2     conc_rl   concat_r1   concat_r2   concat_rl  \n",
       "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000  \n",
       "mean     0.159980    0.043281    0.111020    0.251597    0.076973    0.151307  \n",
       "std      0.092191    0.053815    0.065875    0.091665    0.075873    0.076669  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.093023    0.010230    0.069782    0.193722    0.036675    0.111111  \n",
       "50%      0.154643    0.027397    0.104870    0.242427    0.059353    0.137791  \n",
       "75%      0.221995    0.063456    0.145068    0.302510    0.094003    0.170758  \n",
       "max      0.657005    0.614634    0.647343    0.832061    0.753846    0.793893  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro_r1</th>\n",
       "      <th>intro_r2</th>\n",
       "      <th>intro_rl</th>\n",
       "      <th>mat_r1</th>\n",
       "      <th>mat_r2</th>\n",
       "      <th>mat_rl</th>\n",
       "      <th>conc_r1</th>\n",
       "      <th>conc_r2</th>\n",
       "      <th>conc_rl</th>\n",
       "      <th>concat_r1</th>\n",
       "      <th>concat_r2</th>\n",
       "      <th>concat_rl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.186354</td>\n",
       "      <td>0.061687</td>\n",
       "      <td>0.125765</td>\n",
       "      <td>0.077678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.150430</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.104882</td>\n",
       "      <td>0.246243</td>\n",
       "      <td>0.076346</td>\n",
       "      <td>0.148001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.078642</td>\n",
       "      <td>0.057585</td>\n",
       "      <td>0.058774</td>\n",
       "      <td>0.076780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.093198</td>\n",
       "      <td>0.053635</td>\n",
       "      <td>0.065212</td>\n",
       "      <td>0.096258</td>\n",
       "      <td>0.079698</td>\n",
       "      <td>0.080045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.129368</td>\n",
       "      <td>0.024578</td>\n",
       "      <td>0.085323</td>\n",
       "      <td>0.017560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.085027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063771</td>\n",
       "      <td>0.182811</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>0.107323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.177380</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.117047</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050033</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.098160</td>\n",
       "      <td>0.237456</td>\n",
       "      <td>0.059702</td>\n",
       "      <td>0.136293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.235790</td>\n",
       "      <td>0.081456</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.112322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085757</td>\n",
       "      <td>0.207385</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.134510</td>\n",
       "      <td>0.291624</td>\n",
       "      <td>0.094062</td>\n",
       "      <td>0.167247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382514</td>\n",
       "      <td>0.657005</td>\n",
       "      <td>0.614634</td>\n",
       "      <td>0.647343</td>\n",
       "      <td>0.832061</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.793893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intro_r1    intro_r2    intro_rl      mat_r1  mat_r2      mat_rl  \\\n",
       "count  442.000000  442.000000  442.000000  442.000000   442.0  442.000000   \n",
       "mean     0.186354    0.061687    0.125765    0.077678     0.0    0.060802   \n",
       "std      0.078642    0.057585    0.058774    0.076780     0.0    0.060802   \n",
       "min      0.000000    0.000000    0.000000    0.000000     0.0    0.000000   \n",
       "25%      0.129368    0.024578    0.085323    0.017560     0.0    0.016129   \n",
       "50%      0.177380    0.048780    0.117047    0.058252     0.0    0.050033   \n",
       "75%      0.235790    0.081456    0.153846    0.112322     0.0    0.085757   \n",
       "max      0.494737    0.489362    0.494737    0.388889     0.0    0.382514   \n",
       "\n",
       "          conc_r1     conc_r2     conc_rl   concat_r1   concat_r2   concat_rl  \n",
       "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000  \n",
       "mean     0.150430    0.039989    0.104882    0.246243    0.076346    0.148001  \n",
       "std      0.093198    0.053635    0.065212    0.096258    0.079698    0.080045  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.085027    0.000000    0.063771    0.182811    0.035438    0.107323  \n",
       "50%      0.142857    0.025040    0.098160    0.237456    0.059702    0.136293  \n",
       "75%      0.207385    0.057554    0.134510    0.291624    0.094062    0.167247  \n",
       "max      0.657005    0.614634    0.647343    0.832061    0.753846    0.793893  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['rf'][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_intro = mlp.load_model(dataset='intro', view='features')\n",
    "model_mlp_mat = mlp.load_model(dataset='mat', view='features')\n",
    "model_mlp_conc = mlp.load_model(dataset='conc', view='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp_intro = mlp.load_model(dataset='intro', view='features')\n",
    "model_mlp_mat = mlp.load_model(dataset='mat', view='features')\n",
    "model_mlp_conc = mlp.load_model(dataset='conc', view='features')\n",
    "\n",
    "models = {'intro': model_mlp_intro, 'mat': model_mlp_mat, 'conc': model_mlp_conc}\n",
    "\n",
    "summaries_mlp, intro_mlp, mat_mlp, conc_mlp = summarization.pipeline_summarization(\n",
    "    'mlp', p, models, columns_name, summ_items, path_base, sections=['intro', 'mat', 'conc', 'concat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "articles                                              PMC2759846.json\n",
       "references          Mutations in Leucine rich repeat kinase (LRRK2...\n",
       "summaries_intro     Protein kinases play key regulatory roles for ...\n",
       "summaries_mat       LRRK213262527 was cloned into pFastBac1 (Invit...\n",
       "summaries_conc      Several mutations in the complex multi domain ...\n",
       "summaries_concat    Protein kinases play key regulatory roles for ...\n",
       "intro_r1                                                     0.180328\n",
       "intro_r2                                                     0.033333\n",
       "intro_rl                                                     0.147541\n",
       "mat_r1                                                       0.092715\n",
       "mat_r2                                                              0\n",
       "mat_rl                                                       0.092715\n",
       "conc_r1                                                      0.285714\n",
       "conc_r2                                                      0.018182\n",
       "conc_rl                                                      0.196429\n",
       "concat_r1                                                    0.227848\n",
       "concat_r2                                                    0.025532\n",
       "concat_rl                                                    0.135021\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mutations in Leucine rich repeat kinase (LRRK2) are a common cause of inherited Parkinson’s disease (PD). The protein is large and complex, but pathogenic mutations cluster in a region containing GTPase and kinase domains. LRRK2 can autophosphorylate in vitro within a dimer pair, although the significance of this reaction is unclear. Here, we mapped the sites of autophosphorylation within LRRK2 and found several potential phosphorylation sites within the GTPase domain. Using mass spectrometry, we found that Thr1343 is phosphorylated and, using kinase dead versions of LRRK2, show that this is an autophosphorylation site. However, we also find evidence for additional sites in the GTPase domain and in other regions of the protein suggesting that there may be multiple autophosphorylation sites within LRRK2. These data suggest that the kinase and GTPase activities of LRRK2 may exhibit complex autoregulatory interdependence.'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].iloc[0]['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Protein kinases play key regulatory roles for many processes in cells, influencing signal transduction cascades by the phosphorylation of target proteins. Proposed substrates include moesin , 4EBP , MKK3/6 , β tubulin and α synuclein but none have been shown to be true in vivo, and pathogenic mutations do not consistently have increased activity towards any specific substrate. Surprisingly, we found that LRRK2 autophosphorylates within the ROC/GTPase domain of the same protein, raising the possibility that complex autoregulation of LRRK2 may occur .'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].iloc[0]['summaries_intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LRRK213262527 was cloned into pFastBac1 (Invitrogen) with N terminal Flag tag and C terminal His tags and used to generate baculovirus with the Bac to Bac Expression system (Invitrogen). For mammalian expression, full length human LRRK2, the N terminal region of LRRK2 (amino acids 11248), LRR (9821280), ROC (13351548), COR (15501880), kinase (18802138) and WD40 (21502500) were cloned into the pCHMWS expression plasmid , incorporating an N terminal 3×flag tag. Recombinant domain proteins were incubated with recombinant GST tagged LRRK29702527 or recombinant full length 3×flag tagged LRRK2 in 40 µl kinase buffer containing 6 µCi of 33P ATP (3000 Ci/mmol; Perkin Elmer) for 1 hour at 30°C.'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].iloc[0]['summaries_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Several mutations in the complex multi domain kinase LRRK2 are linked to PD but the mechanism(s) by which they affect protein function are poorly understood. Another minor site is at T2031, which we have previously shown can impact kinase activity as expected for an activation loop residue  . It is of interest that these sites are within the other enzymatic domain of LRRK2, the ROC/GTPase region as this suggests a complex autoregulation of LRRK2.'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['gb'][0].iloc[0]['summaries_conc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "articles                                             PMC2082664.json\n",
       "references         We describe two spatially distinct foci of hum...\n",
       "summaries_intro    Symptoms include general malaise, anemia, head...\n",
       "summaries_mat      The Tororo, Iganga, Jinja and Busia Districts ...\n",
       "summaries_conc     While in the present study, TGF β was not rela...\n",
       "combinacao         Symptoms include general malaise, anemia, head...\n",
       "intro_r1                                                    0.174603\n",
       "intro_r2                                                         0.0\n",
       "intro_rl                                                    0.095238\n",
       "mat_r1                                                      0.071429\n",
       "mat_r2                                                             0\n",
       "mat_rl                                                      0.071429\n",
       "conc_r1                                                     0.153846\n",
       "conc_r2                                                     0.031088\n",
       "conc_rl                                                     0.123077\n",
       "comb_r1                                                     0.146032\n",
       "comb_r2                                                     0.019169\n",
       "comb_rl                                                     0.088889\n",
       "Name: 300, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_gb.iloc[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We describe two spatially distinct foci of human African trypansomiasis in Eastern Uganda. The Tororo and Soroti foci of Trypansoma brucei rhodesiense infection were genetically distinct as characterised by microsatellite and minisatellite polymorphic markers, and were characterised by differences in disease progression and host immune response. In particular, infections with the Tororo genotype exhibited an increased frequency of progression to and severity of the meningoencephalitic stage and higher plasma IFN γ concentration compared to those with the Soroti genotype. We propose that the magnitude of the systemic IFN γ response determines the time at which infected individuals develop CNS infection, and this is consistent with the recently described role of IFN γ in facilitating blood brain barrier transmigration of trypanosomes in experimental model infection. The identification of trypanosome isolates with differing disease progression phenotypes provides the first field based genetic evidence for virulence variants in T.b.rhodesiense.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_gb.iloc[300]['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symptoms include general malaise, anemia, headache, pyrexia, weight loss and weakness. Typically T.b.rhodesiense infections are acute, while T.b.gambiense presents as a chronic disease . Both clinical and experimental animal studies have observed systemically high levels of IFN γ during trypanosome infection [ ], and following trypanosome invasion of the CNS there is direct relationship between the severity of neuropathology and expression of IFN γ in the brain  .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_gb.iloc[300]['summaries_intro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tororo, Iganga, Jinja and Busia Districts define a common ecotope for the transmission by Glossina fuscipes fuscipes of T.b.rhodesiense which will be referred to henceforth as the Tororo focus, while the Soroti District contains a separate G.f.fuscipes ecotope where HAT emerged as a new epidemic in 1998/9 . The Tororo, Iganga, Jinja and Busia Districts define a common ecotope for the transmission by Glossina fuscipes fuscipes of T.b.rhodesiense which will be referred to henceforth as the Tororo focus, while the Soroti District contains a separate G.f.fuscipes ecotope where HAT emerged as a new epidemic in 1998/9 . The Tororo, Iganga, Jinja and Busia Districts define a common ecotope for the transmission by Glossina fuscipes fuscipes of T.b.rhodesiense which will be referred to henceforth as the Tororo focus, while the Soroti District contains a separate G.f.fuscipes ecotope where HAT emerged as a new epidemic in 1998/9 .'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_gb.iloc[300]['summaries_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While in the present study, TGF β was not related to disease severity, when compared to the cytokine responses reported in , both Tororo and Soroti patient plasma TGF β levels were lower than in Malawi HAT patients, while at the same time Tororo patient plasma IFN γ levels were significantly higher than those of Malawi patients, which were not significantly different to the Soroti patient plasma IFN γ concentration. While in the present study, TGF β was not related to disease severity, when compared to the cytokine responses reported in , both Tororo and Soroti patient plasma TGF β levels were lower than in Malawi HAT patients, while at the same time Tororo patient plasma IFN γ levels were significantly higher than those of Malawi patients, which were not significantly different to the Soroti patient plasma IFN γ concentration. While in the present study, TGF β was not related to disease severity, when compared to the cytokine responses reported in , both Tororo and Soroti patient plasma TGF β levels were lower than in Malawi HAT patients, while at the same time Tororo patient plasma IFN γ levels were significantly higher than those of Malawi patients, which were not significantly different to the Soroti patient plasma IFN γ concentration.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_gb.iloc[300]['summaries_conc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(models, datasets, predictions, y_true):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for i in dataset.keys():\n",
    "        \n",
    "        knn, ab, gbc, rfc = models[i]\n",
    "        X, _ = dataset[i]\n",
    "        y = y_true[i]['bin']\n",
    "        \n",
    "        results['knn_{}'.format(i)] = create_df('knn_{}'.format(i), X, y, predictions['knn_{}'.format(i)])\n",
    "        results['ab_{}'.format(i)] = create_df('ab_{}'.format(i), X, y, predictions['ab_{}'.format(i)])\n",
    "        results['gb_{}'.format(i)] = create_df('gb_{}'.format(i), X, y, predictions['gb_{}'.format(i)])\n",
    "        results['rf_{}'.format(i)] = create_df('rf_{}'.format(i), X, y, predictions['rf_{}'.format(i)])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(name_model, x_summ, y_true, y_pred):\n",
    "    \n",
    "    df = pd.DataFrame({'sentences': x_summ['sentences'],\n",
    "                       'rouge_1': list(y_true),\n",
    "                        name_model : y_pred.reshape(1, -1)[0],\n",
    "                       'articles': x_summ['articles']})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name_model, intro_result, mat_result, conc_result, summ_items, path_base):\n",
    "\n",
    "    references = [summarization.get_ref_summary(i, path_base) for i in summ_items]\n",
    "    \n",
    "    summaries = summarization.create_summaries_df(name_model, intro_result, mat_result, conc_result, summ_items, references)\n",
    "        \n",
    "    summaries = summarization.evaluate_summaries(summaries, sections=['intro', 'mat', 'conc', 'comb'])\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = aux(models, dataset, predictions, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = \"../../sumdata/dataset_articles\"\n",
    "summaries = {}\n",
    "\n",
    "name_models = ['knn', 'ab', 'rf', 'gb']\n",
    "\n",
    "for i in name_models:\n",
    "\n",
    "    summ = main(name_model=i, intro_result=results['{}_intro'.format(i)], mat_result=results['{}_mat'.format(i)], conc_result=results['{}_conc'.format(i)], summ_items=summ_items, path_base=path_base)\n",
    "    summaries[i] = summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# One Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from src import utils_classification as utils\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dataset['concat'][1]\n",
    "y_test = dataset['concat'][3]\n",
    "\n",
    "X_train = dataset['concat'][0]\n",
    "y_train = dataset['concat'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.11      0.19    127617\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.11    127617\n",
      "   macro avg       0.50      0.05      0.10    127617\n",
      "weighted avg       1.00      0.11      0.19    127617\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.07      0.13    127617\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.07    127617\n",
      "   macro avg       0.50      0.03      0.06    127617\n",
      "weighted avg       1.00      0.07      0.13    127617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainX = X_train[y_train==1]\n",
    "\n",
    "nu = (sum(y_test))/trainX.shape[0]\n",
    "\n",
    "svm = OneClassSVM(gamma='scale', nu=0.08)\n",
    "rf = IsolationForest(contamination=0.08)\n",
    "\n",
    "svm.fit(trainX)\n",
    "rf.fit (trainX)\n",
    "\n",
    "pred_svm = svm.predict(X_test)\n",
    "pred_rf = rf.predict(X_test)\n",
    "\n",
    "y_test[y_test == 1] = 1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "score = classification_report(y_test, pred_svm, labels=[-1, 1])\n",
    "print( score)\n",
    "\n",
    "score = classification_report(y_test, pred_rf,  labels=[-1, 1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(y_test, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm[pred_svm == -1] = 1\n",
    "pred_svm[pred_svm == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf[pred_rf == -1] = 1\n",
    "pred_rf[pred_rf == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[y_test == -1] = 1\n",
    "y_test[y_test == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_summ_concat\n",
    "y_test = y_summ_concat['bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_concat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -3932.93, time = 5.37s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -7809.93, time = 8.98s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -11667.83, time = 9.20s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -15515.23, time = 8.90s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -19355.68, time = 9.22s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -23190.91, time = 10.12s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -27021.96, time = 9.59s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -30849.52, time = 9.99s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -34674.14, time = 9.56s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -38496.26, time = 9.68s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46061/3551022711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     steps=[('rbm', rbm), ('gb', gb)])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrbm_features_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_concat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_concat_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbm_features_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             sample_weight_val, begin_at_stage, monitor)\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 random_state, X_csc, X_csr)\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    215\u001b[0m                      check_input=False)\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \"\"\"\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1253\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/cinthia/Dados/Mestrado/mv-text-summarizer/env/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "X_test = X_summ_concat.copy()\n",
    "y_test = y_summ_concat.copy()['bin']\n",
    "\n",
    "rbm = BernoulliRBM(n_components=100, learning_rate=0.01, random_state=42, verbose=True)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
    "\n",
    "rbm_features_classifier = Pipeline(\n",
    "    steps=[('rbm', rbm), ('gb', gb)])\n",
    "\n",
    "rbm_features_classifier.fit(X_concat_train, y_concat_train)\n",
    "\n",
    "y_pred = rbm_features_classifier.predict(X_test)\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_features_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using RBM features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    123652\n",
      "           1       0.00      0.00      0.00      3965\n",
      "\n",
      "    accuracy                           0.97    127617\n",
      "   macro avg       0.48      0.50      0.49    127617\n",
      "weighted avg       0.94      0.97      0.95    127617\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd7fce1a8cb8d97b2536bbe54fd7faa274378c3acb961864d5bd989f2d52777"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
