{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "#import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = \"/scratch/cinthiasouza/mv-text-summarizer\"\n",
    "path_to_read=\"/scratch/cinthiasouza/mv-text-summarizer/result/{}/{}_*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from src import preprocess\n",
    "#from src import extract_features\n",
    "#from src import tokenizer\n",
    "#from src import create_features_df\n",
    "from src import transform_data\n",
    "#from src import loader\n",
    "#from src import utils\n",
    "#from src import ensemble_tree_models\n",
    "#from src import tunning_hyperparametrs as th\n",
    "#from src import mlp_regressor\n",
    "#from src import mlp_classifier\n",
    "from src import summarization\n",
    "from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from src import pipeline_extract_features as pef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keras_model(path_to_save, name_model, section):\n",
    "\n",
    "    json_file = open('{}/{}_{}.json'.format(path_to_save, name_model, section), 'r')\n",
    "    model = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(model)\n",
    "    model.load_weights('{}/{}_{}.h5'.format(path_to_save, name_model, section))\n",
    "    print(\"Loaded model from disk\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(\n",
    "                            learning_rate=0.001), metrics=[keras.metrics.Precision()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifiers(sections, path_to_read, name_models):\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    for section in sections:\n",
    "        \n",
    "        aux = {}\n",
    "        \n",
    "        for name_model in name_models:\n",
    "            if name_model != 'mlp':\n",
    "                aux[name_model] = joblib.load('{}/{}_{}.pkl'.format(path_to_read, name_model,  section))\n",
    "            elif (name_model == 'mlp') or (name_model == 'embed_mlp') or (name_model == 'mv_mlp_bert') :\n",
    "                aux[name_model] = load_keras_model(section, path_to_read)\n",
    "            \n",
    "        models[section] = aux\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ascii(text):\n",
    "    \n",
    "    try:\n",
    "        return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_summarization(\n",
    "    features, scores, references, predictions, section, name_models,\n",
    "    summ_items, k=3, sort_scores=True, proba=False, ascending=False):\n",
    "    \n",
    "    X_test = features\n",
    "    y_test = scores\n",
    "    \n",
    "    vfunc = np.vectorize(remove_ascii)\n",
    "    X_test['sentences'] = vfunc(X_test['sentences'])\n",
    "\n",
    "    df_proba = summarization.create_df(name_models, X_test, y_test['rouge_1'], predictions, section, proba=proba)\n",
    "    \n",
    "    if proba:\n",
    "        df = summarization.binarize_proba(df_proba.copy(), name_models, k, sort_scores, ascending)\n",
    "    else:\n",
    "        df = df_proba.copy()\n",
    "        \n",
    "    summaries = summarization.create_summaries(df, references, summ_items, name_models)\n",
    "    result = summarization.evaluate_summariesv2(summaries, name_models, metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n",
    "    \n",
    "    return df_proba, df, summaries, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_v2(name_models, x_summ):\n",
    "    \n",
    "    df = pd.DataFrame({'sentences': x_summ['sentences'],\n",
    "                       'rouge_1': list(x_summ['rouge_1']),\n",
    "                       'articles': x_summ['articles']})\n",
    "    \n",
    "    for name_model in name_models:\n",
    "            df[name_model] = list(x_summ['rouge_1'])\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization_target(\n",
    "    df, references, name_models, summ_items, sort_scores=False, proba=False, ascending=False):\n",
    "    \n",
    "    df = create_df_v2(name_models, df.copy())\n",
    "\n",
    "    df = summarization.binarize_proba(df.copy(), name_models, 3, sort_scores, ascending)\n",
    "\n",
    "    summaries = summarization.create_summaries(df, references, summ_items, name_models)\n",
    "    result = summarization.evaluate_summariesv2(summaries,  name_models, metrics=['rouge-1', 'rouge-2', 'rouge-l'])\n",
    "    \n",
    "    return df, summaries, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predict_models(dataset, sections, name_models, columns, path_to_save, embed_path, num_test, index_Xtest):\n",
    "     \n",
    "    predictions_proba = {}\n",
    "    models = {}\n",
    "\n",
    "    for section in sections:\n",
    "\n",
    "        aux = {}\n",
    "        aux_models = {}\n",
    "        \n",
    "        X_test_embed = pd.read_csv(\"dataset/{}_{}_test.csv\".format(embed_path, section))\n",
    "        X_test_embed = X_test_embed[columns]\n",
    "        \n",
    "        X_test_features = dataset[section][index_Xtest]\n",
    "        \n",
    "        for name_model in name_models:\n",
    "\n",
    "            #Load\n",
    "            if (name_model != 'mlp') and (name_model != 'mlp_embed') and (name_model != 'mv_mlp_bert'):\n",
    "                model = joblib.load('{}/test_{}/{}_{}.pkl'.format(path_to_save, num_test, name_model, section))\n",
    "            else :\n",
    "                model = load_keras_model(path_to_save, name_model, section)\n",
    "\n",
    "            #predict\n",
    "            if (name_model == 'mv_mlp_bert') or (name_model == 'mv_mlp'):\n",
    "                y_pred = model.predict([X_test_embed, X_test_features])\n",
    "            elif name_model == \"mlp\":\n",
    "                y_pred = model.predict(X_test_features)\n",
    "            elif name_model == 'mlp_embed':\n",
    "                y_pred = model.predict(X_test_embed)\n",
    "            else:\n",
    "                y_pred = model.predict_proba(X_test_features)\n",
    "\n",
    "\n",
    "            aux[name_model] = y_pred\n",
    "            aux_models[name_model] = model\n",
    "\n",
    "        predictions_proba[section]= aux\n",
    "        models[section] = aux_models\n",
    "        \n",
    "    return predictions_proba, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_to_summarize(dataset, section):\n",
    "    \n",
    "    df = dataset[section][5].reset_index(drop=True)\n",
    "    features = df[['sentences', 'articles']]\n",
    "    scores = pd.DataFrame()\n",
    "    scores['rouge_1'] = df['rouge_1']\n",
    "    \n",
    "    return features, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('autoencoder_64dim/dataset_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "    \n",
    "columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram', 'count_three_gram',\n",
    "        'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=['introduction', 'materials', 'conclusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_models = ['knn', 'rf', 'ab', 'gb', 'cb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(range(0, 383))\n",
    "columns = list(map(str, columns))\n",
    "\n",
    "embed_path = 'embed_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_items = pd.read_csv(\"dataset/indices_summ.csv\")['summ']\n",
    "references_df = pd.read_csv(\"dataset/references_df.csv\")\n",
    "features_intro = pd.read_csv('dataset/dataset_introduction.csv')\n",
    "features_mat = pd.read_csv('dataset/dataset_materials.csv')\n",
    "features_conc = pd.read_csv('dataset/dataset_conclusion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {'mv_models_64dim':'mv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_summaries_eval(\n",
    "    summaries_intro, summaries_mat, summaries_conc, references_df,\n",
    "    name_models = ['knn', 'rf', 'ab', 'gb', 'cb'], metrics=['rouge-1', 'rouge-2', 'rouge-l']):\n",
    "    \n",
    "    summaries = summarization.combine_three_summ(\n",
    "        summaries_intro, summaries_mat, summaries_conc, references_df, name_models)\n",
    "    result = summarization.evaluate_summariesv2(summaries, name_models, metrics)\n",
    "    \n",
    "    return summaries, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (\n",
    "    dataset, tests, num_test, references_df, sections, name_models, columns, embed_path):\n",
    "    \n",
    "    for name_test, approach in tests.items():\n",
    "\n",
    "        if approach == 'sv':\n",
    "            index_Xtest=1\n",
    "            index_ytest=3\n",
    "        elif approach == 'mv':\n",
    "            index_Xtest=-1\n",
    "            index_ytest=3\n",
    "\n",
    "        print(num_test)\n",
    "\n",
    "        path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(name_test)\n",
    "\n",
    "        predictions_proba, models = load_predict_models(\n",
    "            dataset, sections, name_models, columns, path_to_save, embed_path, num_test, index_Xtest=index_Xtest)\n",
    "\n",
    "        predictions, predictions_proba, results = ev.create_reports(\n",
    "            models, dataset, name_models, index_Xtest=index_Xtest, index_ytest=index_ytest, verbose=False)\n",
    "\n",
    "        if not os.path.exists(\"{}/test_{}/summaries\".format(path_to_save, num_test)):\n",
    "                os.makedirs(\"{}/test_{}/predictions\".format(path_to_save, num_test))\n",
    "                os.makedirs(\"{}/test_{}/summaries\".format(path_to_save, num_test))\n",
    "\n",
    "        aux = {}\n",
    "        for section in sections:\n",
    "\n",
    "            features, scores = format_data_to_summarize(dataset, section)\n",
    "\n",
    "            proba_ex1, df_ex1, summaries_ex1, result_ex1 = pipeline_summarization(\n",
    "                features, scores, references_df, predictions_proba, section, name_models,\n",
    "                summ_items, sort_scores=True, proba=True, ascending=False)\n",
    "\n",
    "            aux[section] = summaries_ex1\n",
    "\n",
    "            result_ex1.to_csv(\"{}/test_{}/summaries/{}.csv\".format(path_to_save, num_test, section), index=False)\n",
    "            proba_ex1.to_csv(\"{}/test_{}/predictions/{}.csv\".format(path_to_save,num_test, section), index=False)\n",
    "            df_ex1.to_csv(\"{}/test_{}/predictions/df_{}.csv\".format(path_to_save, num_test, section), index=False)\n",
    "\n",
    "        summaries, result_comb = combine_summaries_eval(\n",
    "            aux['introduction'], aux['materials'], aux['conclusion'], references_df)\n",
    "\n",
    "        result_comb.to_csv(\"{}/test_{}/summaries/{}.csv\".format(path_to_save, num_test, 'comb'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "Parallel(n_jobs=5)(delayed(main)(dataset, tests, num_test, references_df, sections, name_models, columns, embed_path) for num_test in range(1, 31))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv_models_64dim\n"
     ]
    }
   ],
   "source": [
    "for num_test in range(1, 31):\n",
    "    for name_test, approach in tests.items():\n",
    "\n",
    "        if approach == 'sv':\n",
    "            index_Xtest=1\n",
    "            index_ytest=3\n",
    "        elif approach == 'mv':\n",
    "            index_Xtest=-1\n",
    "            index_ytest=3\n",
    "\n",
    "        print(num_test)\n",
    "\n",
    "        path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(name_test)\n",
    "\n",
    "        predictions_proba, models = load_predict_models(\n",
    "            dataset, sections, name_models, columns, path_to_save, embed_path, index_Xtest=index_Xtest)\n",
    "\n",
    "        predictions, predictions_proba, results = ev.create_reports(\n",
    "            models, dataset, name_models, index_Xtest=index_Xtest, index_ytest=index_ytest, verbose=False)\n",
    "\n",
    "        if not os.path.exists(\"{}/test_{}/summaries\".format(path_to_save, num_test)):\n",
    "                os.makedirs(\"{}/test_{}/predictions\".format(path_to_save, num_test))\n",
    "                os.makedirs(\"{}/test_{}/summaries\".format(path_to_save, num_test))\n",
    "\n",
    "        aux = {}\n",
    "        for section in sections:\n",
    "\n",
    "            features, scores = format_data_to_summarize(dataset, section)\n",
    "\n",
    "            proba_ex1, df_ex1, summaries_ex1, result_ex1 = pipeline_summarization(\n",
    "                features, scores, references_df, predictions_proba, section, name_models,\n",
    "                summ_items, sort_scores=True, proba=True, ascending=False)\n",
    "\n",
    "            aux[section] = summaries_ex1\n",
    "\n",
    "            result_ex1.to_csv(\"{}/summaries/{}.csv\".format(path_to_save, section), index=False)\n",
    "            proba_ex1.to_csv(\"{}/predictions/{}.csv\".format(path_to_save, section), index=False)\n",
    "            df_ex1.to_csv(\"{}/predictions/df_{}.csv\".format(path_to_save, section), index=False)\n",
    "\n",
    "        summaries, result_comb = combine_summaries_eval(\n",
    "            aux['introduction'], aux['materials'], aux['conclusion'], references_df)\n",
    "\n",
    "        result_comb.to_csv(\"{}/summaries/{}.csv\".format(path_to_save, 'comb'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd7fce1a8cb8d97b2536bbe54fd7faa274378c3acb961864d5bd989f2d52777"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
