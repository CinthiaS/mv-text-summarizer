{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "#import smogn\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pysbd.utils import PySBDFactory\n",
    "import math\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from timeit import default_timer as timer \n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "#from tensorflow.keras.models import model_from_json\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "#import en_core_web_sm\n",
    "#nlp_md = en_core_web_sm.load()\n",
    "\n",
    "#import en_core_web_md\n",
    "#nlp_md = en_core_web_md.load()\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path_base = \"/scratch/cinthiasouza/mv-text-summarizer\"\n",
    "path_to_read=\"/scratch/cinthiasouza/mv-text-summarizer/result/{}/{}_*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/scratch/cinthiasouza/anaconda3/envs/mvsumm/lib python3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "#from src import ensemble_tree_models\n",
    "from src import tunning_hyperparametrs as th\n",
    "from src import mlp_classifier\n",
    "#from src import summarization\n",
    "#from src import normalization\n",
    "from src import ensemble_tree_models as classifiers\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#from src import pipeline_extract_features as pef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import random\n",
    "\n",
    "features_intro, scores_intro = loader.read_features(path=\"../result/introduction/features_*.csv\")\n",
    "features_mat, scores_mat = loader.read_features(path=\"../result/materials/features_*.csv\")\n",
    "features_conc, scores_conc = loader.read_features(path=\"../result/conclusion/features_*.csv\")\n",
    "\n",
    "intro = pd.unique(features_intro['articles'])\n",
    "mat = pd.unique(features_mat['articles'])\n",
    "conc = pd.unique(features_conc['articles'])\n",
    "\n",
    "comuns = list((set(intro) & set(mat)) & set(conc))\n",
    "\n",
    "valid_len = int(len(comuns)*0.2)\n",
    "summ_items = random.sample(comuns, valid_len)\n",
    "\n",
    "df = pd.DataFrame({'summ': summ_items})\n",
    "df.to_csv(\"indices_summ.csv\", index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path_to_read, name_csv=\"features\", format_dataset=True, verbose=True):\n",
    "    \n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    sections=['introduction', 'materials', 'conclusion']\n",
    "    #sections=[ 'introduction']\n",
    "\n",
    "    if format_dataset:\n",
    "        if verbose:\n",
    "            print(\"Preparando dataset para os classificadores\")\n",
    "        dataset = prepare_data.main_create_dataset(columns_name, sections, path_to_read, name_csv)\n",
    "        #utils.,save_json(dataset, name='dataset', path=path)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Carregando dataset\")\n",
    "        dataset = utils.load_json(name='dataset', path=path)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Treinamento dos modelos\")\n",
    "        \n",
    "    with open('dataset5_{}.pkl'.format(name_csv), 'wb') as fp:\n",
    "        pickle.dump(dataset, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return dataset\n",
    "    \n",
    "    '''models = ensemble_tree_models.create_models(dataset, sections, name_models=['knn', 'gb', 'rf', 'ab'])\n",
    "    predictions, results = ev.create_reports(models, dataset, columns_name, verbose=False)\n",
    "    \n",
    "    #utils.save_json(predictions, name='prediction', path=path)\n",
    "    #utils.save_results(results, path=path)\n",
    "    \n",
    "    parameters = {'introduction': [0.2, 100, 64],\n",
    "             'materials': [0.2, 100, 64],\n",
    "             'conclusion':[0.2, 100, 64],\n",
    "             'concat': [0.2, 100, 64]}\n",
    "\n",
    "    models_nn = mlp_classifiers.main_train_nn(dataset, sections, parameters, train=True, verbose=False)\n",
    "    predictions, results =  mlp_classifiers.eval_nn(dataset, sections)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--path', \"-p\",required=True)\n",
    "    parser.add_argument('--format_dataset', \"-f\",required=True)\n",
    "     \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    columns_name = ['text_rank', 'lex_rank', 'count_one_gram',\n",
    "        'count_article_keywords',\n",
    "       'tf-isf', 'position_score', 'paragraph_score',\n",
    "       'length_score', 'pos_score', 'ner_score', 'dist_centroid']\n",
    "\n",
    "    sections=['introduction', 'materials', 'conclusion', 'concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dataset para os classificadores\n",
      "/scratch/cinthiasouza/mv-text-summarizer/result/introduction/features_*.csv\n",
      "/scratch/cinthiasouza/mv-text-summarizer/result/materials/features_*.csv\n",
      "/scratch/cinthiasouza/mv-text-summarizer/result/conclusion/features_*.csv\n",
      "Treinamento dos modelos\n"
     ]
    }
   ],
   "source": [
    "dataset, train, test  = main(verbose=True, path_to_read=path_to_read, name_csv='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models <-{0:0.9, 1:0.1} dataset3 [intro, mat, conc]\n",
    "\n",
    "models_v2 <-{0:0.95, 1:0.05} dataset3 [intro, mat, conc]\n",
    "\n",
    "models_v3 <-{0:0.95, 1:0.05} dataset4 [intro, mat, conc]\n",
    "\n",
    "models_v4 <-{0:0.95, 1:0.05} dataset4 [concat]\n",
    "\n",
    "models_v5 <-{0:1, 1:10} dataset4 [intro, mat, conc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset5_{}.pkl'.format('features'), 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "columns_name = ['text_rank', 'lex_rank', 'count_one_gram', 'count_two_gram',\n",
    "       'count_three_gram', 'count_article_keywords', 'tf-isf', 'pos_score', 'ner_score']\n",
    "\n",
    "sections=['introduction', 'materials', 'conclusion']\n",
    "\n",
    "folder_to_save = 'models_v5'\n",
    "\n",
    "path_to_save = \"/scratch/cinthiasouza/mv-text-summarizer/notebook/{}\".format(folder_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search -  CV 5 Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_intro = {0:7, 1:1}\n",
    "class_weight_mat = {0:6, 1:1}\n",
    "class_weight_conc = {0:12, 1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_tunning = {\n",
    "    \n",
    "'introduction': {'rf': {'n_estimators': [100, 150, 300],\n",
    "              'min_samples_leaf':[100, 500, 200],\n",
    "              'min_samples_split':[200, 1000, 30000],\n",
    "              'max_depth':[10, 20, 30, 50],\n",
    "              'class_weight': class_weight_intro\n",
    "              },\n",
    "              'gb': {'n_estimators': [50, 100, 300],\n",
    "              'min_samples_leaf':[10, 50, 100, 200, 1000],\n",
    "              'min_samples_split':[100, 500, 750, 1000],\n",
    "              'max_depth':[10, 20, 30, 40, 50]        \n",
    "              },\n",
    "              'cb': {'iterations': [100, 150, 300],\n",
    "            'learning_rate': [0.01, 0.001, 0.0001],\n",
    "            'depth': [10, 20, 30],\n",
    "            'class_weights':class_weight_intro,\n",
    "            'min_data_in_leaf':[500, 1000, 2000]},\n",
    "              'knn':  {'n_neighbors': [5, 10, 20, 50]},\n",
    "              'ab': {'n_estimators': [50,  100, 200]},\n",
    "              'svm': {'kernel': ('poly', 'rbf'), 'degree':[2, 3, 4], 'class_weight': class_weight_intro}\n",
    "                 },\n",
    "\n",
    "\"materials\": {'rf': {'n_estimators': [100, 150, 300],\n",
    "              'min_samples_leaf':[100, 500, 200],\n",
    "              'min_samples_split':[200, 1000, 30000],\n",
    "              'max_depth':[10, 20, 30, 50],\n",
    "              'class_weight': class_weight_mat\n",
    "              },\n",
    "              'gb': {'n_estimators': [50, 100, 150, 200, 300],\n",
    "              'min_samples_leaf':[5, 10, 100],\n",
    "              'min_samples_split':[10, 20],\n",
    "              'max_depth':[20, 10, 50, 100]\n",
    "              },\n",
    "              'cb': {'iterations': [100, 150, 300],\n",
    "            'learning_rate': [0.01, 0.001, 0.0001],\n",
    "            'depth': [10, 20, 30],\n",
    "            'class_weights':class_weight_mat,\n",
    "            'min_data_in_leaf':[500, 1000, 2000]},\n",
    "              'knn':  {'n_neighbors': [5, 10, 20, 50]},\n",
    "              'ab': {'n_estimators': [50,  100, 200]},\n",
    "              'svm': {'kernel': ('poly', 'rbf'), 'degree':[2, 3, 4], 'class_weight': class_weight_mat}\n",
    "                 },\n",
    "\n",
    "'conclusion': {'rf': {'n_estimators': [100, 150, 300],\n",
    "              'min_samples_leaf':[100, 500, 200],\n",
    "              'min_samples_split':[200, 1000, 30000],\n",
    "              'max_depth':[10, 20, 30, 50],\n",
    "              'class_weight': class_weight_conc\n",
    "              },\n",
    "               'gb': {'n_estimators': [50, 100, 150, 200, 300],\n",
    "              'min_samples_leaf':[5, 10, 100],\n",
    "              'min_samples_split':[10, 20],\n",
    "              'max_depth':[20, 50, 100]},\n",
    "               'cb': {'iterations': [100, 150, 300],\n",
    "            'learning_rate': [0.01, 0.001, 0.0001],\n",
    "            'depth': [10, 20, 30],\n",
    "            'class_weights':class_weight_conc,\n",
    "            'min_data_in_leaf':[500, 1000, 2000]},\n",
    "              'knn':  {'n_neighbors': [5, 10, 20, 50]},\n",
    "              'ab': {'n_estimators': [50,  100, 200]},\n",
    "              'svm': {'kernel': ('poly', 'rbf'), 'degree':[2, 3, 4], 'class_weight': class_weight_conc}\n",
    "                 }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tunning = {'gb': GradientBoostingClassifier(), \n",
    "                          'rf': RandomForestClassifier(n_jobs=-1, class_weight={0:1, 1:3}),\n",
    "                          'ab': AdaBoostClassifier(DecisionTreeClassifier(class_weight={0:1, 1: 3})),\n",
    "                          'knn': KNeighborsClassifier(n_jobs=-1),\n",
    "                          'cb': CatBoostClassifier(class_weights={0:1, 1: 3}),\n",
    "                          'svm': SVC(class_weight={0:1, 1: 3})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.pipeline_tunning(\n",
    "         dataset, {'rf': RandomForestClassifier(n_jobs=-1, class_weight=class_weight)},\n",
    "        sections, parameters_tunning, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.pipeline_tunning(\n",
    "         dataset, {'gb': GradientBoostingClassifier()},\n",
    "        sections, parameters_tunning, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "th.pipeline_tunning(\n",
    "         dataset, {'ab': AdaBoostClassifier()},\n",
    "        sections, parameters_tunning, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.pipeline_tunning(\n",
    "         dataset, {'knn': KNeighborsClassifier()},\n",
    "        sections, parameters_tunning, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rs_model(name_model, sections, path_to_read):\n",
    "    \n",
    "    rs_models = {}\n",
    "    \n",
    "    for section in sections:\n",
    "        \n",
    "        rs_models[section] = joblib.load('{}/search_{}_{}.pkl'.format(path_to_read, name_model, section))\n",
    "        \n",
    "    return rs_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_scores(name_models, sections, path_to_read):\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for name_model in name_models:\n",
    "        \n",
    "        rs_models = load_rs_model(name_model, sections, path_to_read)\n",
    "    \n",
    "        aux  = { 'score_{}'.format(section): rs_models[section].best_score_ for section in sections}\n",
    "        results[name_model] = aux\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=['introduction', 'materials', 'conclusion']\n",
    "name_models = ['knn', 'ab', 'gb']\n",
    "\n",
    "results = get_scores(name_models, sections, path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(results).T\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_rs(name_models, sections, path_to_read, verbose=False):\n",
    "\n",
    "    searches = {}\n",
    "\n",
    "    for name_model in name_models:\n",
    "\n",
    "        aux = {}\n",
    "\n",
    "        for section in sections:\n",
    "\n",
    "            aux[section] = joblib.load('{}/search_{}_{}.pkl'.format(path_to_read, name_model, section))\n",
    "\n",
    "            if verbose: \n",
    "                print(aux[section].best_params_)\n",
    "\n",
    "        searches[name_model] = aux\n",
    "\n",
    "    return searches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_parameters(searches, name_models, sections):\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for name_model in name_models:\n",
    "\n",
    "        aux = {}\n",
    "\n",
    "        for section in sections:\n",
    "\n",
    "            aux[section] = searches[name_model][section].best_params_\n",
    "\n",
    "        parameters[name_model] = aux\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=['introduction', 'materials', 'conclusion']\n",
    "name_models = ['rf', 'cb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = get_models_rs(name_models, sections, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = get_dict_parameters(searches, name_models, sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_intro = {0:3, 1:1}\n",
    "class_weight_mat = {0:3, 1:1}\n",
    "class_weight_conc = {0:3, 1:1}\n",
    "\n",
    "parameters = {'rf': {'introduction': {'n_estimators': 300,\n",
    "   'min_samples_split': 200,\n",
    "   'min_samples_leaf': 10,\n",
    "   'max_depth': 50,\n",
    "   'class_weight': class_weight_intro},\n",
    "  'materials': {'n_estimators': 300,\n",
    "   'min_samples_split': 30000,\n",
    "   'min_samples_leaf': 10,\n",
    "   'max_depth': 50,\n",
    "   'class_weight': class_weight_mat},\n",
    "  'conclusion': {'n_estimators': 150,\n",
    "   'min_samples_split': 1000,\n",
    "   'min_samples_leaf': 10,\n",
    "   'max_depth': 50,\n",
    "   'class_weight': class_weight_conc}},\n",
    " 'cb': {'introduction': {'min_data_in_leaf': 10,\n",
    "   'learning_rate': 0.01,\n",
    "   'iterations': 100,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_intro},\n",
    "  'materials': {'min_data_in_leaf': 10,\n",
    "   'learning_rate': 0.001,\n",
    "   'iterations': 100,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_mat},\n",
    "  'conclusion': {'min_data_in_leaf': 10,\n",
    "   'learning_rate': 0.0001,\n",
    "   'iterations': 150,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_conc}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_intro = {0:7, 1:1}\n",
    "class_weight_mat = {0:6, 1:1}\n",
    "class_weight_conc = {0:12, 1:1}\n",
    "\n",
    "parameters = {'rf': {'introduction': {'n_estimators': 500,\n",
    "   'min_samples_split': 200,\n",
    "   'min_samples_leaf': 500,\n",
    "   'max_depth': 30,\n",
    "   'class_weight': class_weight_intro},\n",
    "  'materials': {'n_estimators': 500,\n",
    "   'min_samples_split': 1000,\n",
    "   'min_samples_leaf': 500,\n",
    "   'max_depth': 10,\n",
    "   'class_weight': class_weight_mat},\n",
    "  'conclusion': {'n_estimators': 500,\n",
    "   'min_samples_split': 1000,\n",
    "   'min_samples_leaf': 500,\n",
    "   'max_depth': 30,\n",
    "   'class_weight': class_weight_conc}},\n",
    " 'cb': {'introduction': {'min_data_in_leaf': 500,\n",
    "   'learning_rate': 0.01,\n",
    "   'iterations': 100,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_intro},\n",
    "  'materials': {'min_data_in_leaf': 500,\n",
    "   'learning_rate': 0.001,\n",
    "   'iterations': 100,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_mat},\n",
    "  'conclusion': {'min_data_in_leaf': 500,\n",
    "   'learning_rate': 0.01,\n",
    "   'iterations': 200,\n",
    "   'depth': 10,\n",
    "   'class_weights': class_weight_conc}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'introduction': {'rf': RandomForestClassifier(class_weight={0: 7, 1: 1}, max_depth=30,\n",
       "                         min_samples_leaf=500, min_samples_split=200,\n",
       "                         n_estimators=500, n_jobs=-1)},\n",
       " 'materials': {'rf': RandomForestClassifier(class_weight={0: 6, 1: 1}, max_depth=10,\n",
       "                         min_samples_leaf=500, min_samples_split=1000,\n",
       "                         n_estimators=500, n_jobs=-1)},\n",
       " 'conclusion': {'rf': RandomForestClassifier(class_weight={0: 12, 1: 1}, max_depth=30,\n",
       "                         min_samples_leaf=500, min_samples_split=1000,\n",
       "                         n_estimators=500, n_jobs=-1)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers.pipeline_classifiers(dataset, parameters, sections, ['rf'], folder_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'introduction': {'cb': <catboost.core.CatBoostClassifier at 0x7f45168b6490>},\n",
       " 'materials': {'cb': <catboost.core.CatBoostClassifier at 0x7f45168b6520>},\n",
       " 'conclusion': {'cb': <catboost.core.CatBoostClassifier at 0x7f45168b6280>}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers.pipeline_classifiers(dataset, parameters, sections, ['cb'], folder_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keras_model(section, path_to_save):\n",
    "\n",
    "    json_file = open('{}/mlp_{}.json'.format(path_to_save, section), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights('{}/mlp_{}.h5'.format(path_to_save, section))\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer=Adam(\n",
    "                learning_rate=0.001), metrics=[Precision()])\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classifiers(sections, path_to_read, name_models):\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    for section in sections:\n",
    "        \n",
    "        aux = {}\n",
    "        \n",
    "        for name_model in name_models:\n",
    "            if name_model != 'mlp':\n",
    "                aux[name_model] = joblib.load('{}/{}_{}.pkl'.format(path_to_read, name_model,  section))\n",
    "            elif name_model == 'mlp':\n",
    "                aux[name_model] = load_keras_model(section, path_to_read)\n",
    "            \n",
    "        models[section] = aux\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections=['introduction', 'materials', 'conclusion']\n",
    "name_models = ['rf', 'cb', 'mlp']\n",
    "\n",
    "models = load_classifiers(sections, path_to_save, name_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, predictions_proba, results = ev.create_reports(models, dataset, columns_name, name_models, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['introduction']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['introduction']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['introduction']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['introduction']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['materials']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['materials']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['conclusion']['rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['materials']['cb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.display_results4(results, section='introduction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.display_results4(results, section='materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.display_results4(results, section='conclusion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matthews Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.matthews(sections, dataset, predictions, name_models)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.roc_curve(sections, dataset, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_estimators = 300\n",
    "estimators = []\n",
    "\n",
    "for i in range(300):\n",
    "    \n",
    "    rf =  DecisionTreeClassifier(\n",
    "        max_depth=10, splitter='best', min_samples_split=200,\n",
    "        min_samples_leaf= 100, class_weight={0:0.98,1:0.02})\n",
    "    \n",
    "    estimators.append(('rf_' + str(i), rf))\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators, voting='hard')\n",
    "model = model.fit(dataset['introduction'][0], dataset['introduction'][2])\n",
    "predictions = model.predict(dataset['introduction'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for multiclass classification\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for multiclass classification\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    " \n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    " \n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 3)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Softmax(dim=1)\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # output layer\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    " \n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(500):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    " \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        # convert to class labels\n",
    "        yhat = argmax(yhat, axis=1)\n",
    "        # reshape for stacking\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    " \n",
    "# prepare the data\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(4)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import mlp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "#from keras.callbacks import LearningRateScheduler\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_mlp = { \n",
    "    'introduction':  [ 0.2, 100, 64],\n",
    "    'materials':     [ 0.2, 100, 64],\n",
    "    'conclusion':  [ 0.2, 100, 64]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = mlp_classifier.main_train_nn(dataset, sections, parameters_mlp, train=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = 'introduction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=13, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=0.001), metrics=['accuracy', keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[section][0]\n",
    "y = dataset[section][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size, epochs, batch_size =parameters_mlp.get(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, one_hot_label, stratify=one_hot_label, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        np.array(X_train), np.array(y_train), validation_data=(X_valid,y_valid), epochs=epochs,\n",
    "         batch_size=batch_size, class_weight={0:0.95, 1:0.05}, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd7fce1a8cb8d97b2536bbe54fd7faa274378c3acb961864d5bd989f2d52777"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
