{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e070ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/scratch/cinthiasouza/mv-text-summarizer')\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec32170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, concatenate, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "874dfb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src import preprocess\n",
    "from src import extract_features\n",
    "from src import tokenizer\n",
    "from src import create_features_df\n",
    "from src import transform_data\n",
    "from src import loader\n",
    "from src import utils\n",
    "from src import utils_classification as utils_clf\n",
    "from src import evaluate_classifiers as ev\n",
    "from src import prepare_data\n",
    "from src import display_results as dr\n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bf29b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autoencoder(X_embedd, X_features, y_train, bottleneck_dim, section, path_to_write):\n",
    "    \n",
    "    sequence_input = Input(shape=(X_embedd.shape[1],), dtype='int32')\n",
    "\n",
    "    e_1 = Dense(X_embedd.shape[1]*2)(sequence_input)\n",
    "    e_1 = BatchNormalization()(e_1)\n",
    "    e_1 = LeakyReLU()(e_1)\n",
    "\n",
    "    e_2 = Dense(X_embedd.shape[1])(e_1)\n",
    "    e_2 = BatchNormalization()(e_2)\n",
    "    e_2 = LeakyReLU()(e_2)\n",
    "\n",
    "\n",
    "    #encoder vision 2\n",
    "    sequence_input2 = Input(shape=(X_features.shape[1],), dtype='int32')\n",
    "\n",
    "    e_3 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "    e_3 = BatchNormalization()(e_3)\n",
    "    e_3 = LeakyReLU()(e_3)\n",
    "\n",
    "    e_4 = Dense(X_features.shape[1]*2)(sequence_input2)\n",
    "    e_4 = BatchNormalization()(e_4)\n",
    "    e_4 = LeakyReLU()(e_4)\n",
    "\n",
    "    #Concatenate visions\n",
    "    v_1 = e_2\n",
    "\n",
    "    v_2_concat = concatenate([v_1, e_3])\n",
    "    v_2 = Dense(256, activation='relu')(v_2_concat)\n",
    "\n",
    "    v_3_concat = concatenate([v_1, v_2, e_4])\n",
    "    v_3 = Dense(256, activation='relu')(v_3_concat)\n",
    "\n",
    "    out_concat = concatenate([v_1, v_2, v_3])\n",
    "\n",
    "    #Shared Inputs\n",
    "\n",
    "    shared_input = Dense(bottleneck_dim)(out_concat)\n",
    "    bottleneck = Dense(bottleneck_dim)(shared_input)\n",
    "\n",
    "    # decoder  vision 1\n",
    "    d_1 = Dense(X_embedd.shape[1])(bottleneck)\n",
    "    d_1 = BatchNormalization()(d_1)\n",
    "    d_1 = LeakyReLU()(d_1)\n",
    "    dropout1 = Dropout(.2)(d_1)\n",
    "\n",
    "    d_2 = Dense(X_embedd.shape[1])(dropout1)\n",
    "    d_2 = BatchNormalization()(d_2)\n",
    "    d_2 = LeakyReLU()(d_2)\n",
    "    dropout2 = Dropout(.2)(d_2)\n",
    "\n",
    "    d_v1 = Dense(X_embedd.shape[1])(dropout2)\n",
    "    d_v1 = BatchNormalization()(d_v1)\n",
    "    d_v1 = LeakyReLU()(d_v1)\n",
    "\n",
    "    #decoder vision 2\n",
    "    d_5 = Dense(X_features.shape[1])(bottleneck)\n",
    "    d_5 = BatchNormalization()(d_5)\n",
    "    d_5 = LeakyReLU()(d_5)\n",
    "    dropout3 = Dropout(.2)(d_5)\n",
    "\n",
    "    d_4 = Dense(X_embedd.shape[1])(dropout3)\n",
    "    d_4 = BatchNormalization()(d_4)\n",
    "    d_4 = LeakyReLU()(d_4)\n",
    "    dropout4 = Dropout(.2)(d_4)\n",
    "\n",
    "    d_v2 = Dense(X_features.shape[1])(dropout4)\n",
    "    d_v2 = BatchNormalization()(d_v2)\n",
    "    d_v2 = LeakyReLU()(d_v2)\n",
    "\n",
    "    output_v1 = Dense(X_embedd.shape[1], activation='linear')(d_v1)\n",
    "    output_v2 = Dense(X_features.shape[1], activation='linear')(d_v2)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, sequence_input2], outputs=[output_v1, output_v2])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "                    learning_rate=0.0001) ,loss=keras.metrics.mean_squared_error)\n",
    "    \n",
    "    one_hot_label = to_categorical(y_train)\n",
    "    X_train_embedd, X_valid_embedd, y_train_embedd, y_valid_embedd = train_test_split(\n",
    "            X_embedd, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "\n",
    "    one_hot_label = to_categorical(y_train)\n",
    "    X_train_features, X_valid_features, y_train_features, y_valid_features = train_test_split(\n",
    "            X_features, one_hot_label, stratify=one_hot_label, shuffle=True, test_size=0.2)\n",
    "    \n",
    "    history = model.fit(\n",
    "            x=[X_train_embedd, X_train_features], y=[X_train_embedd, X_train_features],\n",
    "                epochs=5, validation_data=([X_valid_embedd, X_valid_features], [X_valid_embedd, X_valid_features]),\n",
    "            shuffle=True, batch_size=64)\n",
    "        \n",
    "    encoder = Model(inputs=[sequence_input, sequence_input2], outputs=bottleneck)\n",
    "    encoder.save('{}/encoder_{}.h5'.format(path_to_write, section))\n",
    "    \n",
    "    save_model(model, bottleneck_dim, section, path_to_write)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe7b9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, bottleneck_dim, section, path_to_write):\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    with open('{}/autoencoder_{}.json'.format(path_to_write, section), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights('{}/autoencoder_{}.h5'.format(path_to_write, section))\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e19f5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_autoencoder(\n",
    "    sections, path_to_read='dataset', path_to_write='../autoencoder_test', bottleneck_dim=64):\n",
    "    \n",
    "    with open('{}/dataset_{}.pkl'.format(path_to_read, 'features'), 'rb') as fp:\n",
    "        dataset = pickle.load(fp)\n",
    "        \n",
    "    columns = [str(i) for i in range(300)]\n",
    "    \n",
    "    for section in sections:\n",
    "        \n",
    "        X_features = dataset[section]['X_train_features']\n",
    "        y_train = dataset[section][\"y_train\"]\n",
    "\n",
    "        X_embedd = dataset[section]['X_train_embedd'][columns]\n",
    "        \n",
    "        model = fit_autoencoder(X_embedd, X_features, y_train, bottleneck_dim, section, path_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2d3bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'result_plosonev2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c84bcd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'plosone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "521454ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1197/1197 [==============================] - 404s 238ms/step - loss: 0.9272 - dense_62_loss: 0.1131 - dense_63_loss: 0.8141 - val_loss: 0.3755 - val_dense_62_loss: 0.0094 - val_dense_63_loss: 0.3662\n",
      "Epoch 2/5\n",
      "1197/1197 [==============================] - 84s 70ms/step - loss: 0.4418 - dense_62_loss: 0.0127 - dense_63_loss: 0.4291 - val_loss: 0.2912 - val_dense_62_loss: 0.0087 - val_dense_63_loss: 0.2826\n",
      "Epoch 3/5\n",
      "1197/1197 [==============================] - 63s 53ms/step - loss: 0.3687 - dense_62_loss: 0.0106 - dense_63_loss: 0.3581 - val_loss: 0.2565 - val_dense_62_loss: 0.0085 - val_dense_63_loss: 0.2480\n",
      "Epoch 4/5\n",
      "1197/1197 [==============================] - 55s 46ms/step - loss: 0.3364 - dense_62_loss: 0.0097 - dense_63_loss: 0.3268 - val_loss: 0.2369 - val_dense_62_loss: 0.0084 - val_dense_63_loss: 0.2285\n",
      "Epoch 5/5\n",
      "1197/1197 [==============================] - 55s 46ms/step - loss: 0.3176 - dense_62_loss: 0.0092 - dense_63_loss: 0.3084 - val_loss: 0.2264 - val_dense_62_loss: 0.0083 - val_dense_63_loss: 0.2181\n",
      "Saved model to disk\n",
      "Epoch 1/5\n",
      "879/879 [==============================] - 54s 49ms/step - loss: 1.0696 - dense_78_loss: 0.1506 - dense_79_loss: 0.9189 - val_loss: 0.4478 - val_dense_78_loss: 0.0127 - val_dense_79_loss: 0.4351\n",
      "Epoch 2/5\n",
      "879/879 [==============================] - 40s 46ms/step - loss: 0.5403 - dense_78_loss: 0.0192 - dense_79_loss: 0.5210 - val_loss: 0.3546 - val_dense_78_loss: 0.0117 - val_dense_79_loss: 0.3430\n",
      "Epoch 3/5\n",
      "879/879 [==============================] - 41s 47ms/step - loss: 0.4573 - dense_78_loss: 0.0151 - dense_79_loss: 0.4421 - val_loss: 0.3033 - val_dense_78_loss: 0.0115 - val_dense_79_loss: 0.2918\n",
      "Epoch 4/5\n",
      "879/879 [==============================] - 45s 51ms/step - loss: 0.3864 - dense_78_loss: 0.0137 - dense_79_loss: 0.3728 - val_loss: 0.2678 - val_dense_78_loss: 0.0112 - val_dense_79_loss: 0.2566\n",
      "Epoch 5/5\n",
      "879/879 [==============================] - 41s 47ms/step - loss: 0.3617 - dense_78_loss: 0.0129 - dense_79_loss: 0.3488 - val_loss: 0.2344 - val_dense_78_loss: 0.0111 - val_dense_79_loss: 0.2233\n",
      "Saved model to disk\n",
      "Epoch 1/5\n",
      "1049/1049 [==============================] - 122s 107ms/step - loss: 1.0340 - dense_94_loss: 0.1300 - dense_95_loss: 0.9040 - val_loss: 0.3748 - val_dense_94_loss: 0.0125 - val_dense_95_loss: 0.3624\n",
      "Epoch 2/5\n",
      "1049/1049 [==============================] - 52s 50ms/step - loss: 0.4560 - dense_94_loss: 0.0169 - dense_95_loss: 0.4391 - val_loss: 0.2819 - val_dense_94_loss: 0.0117 - val_dense_95_loss: 0.2702\n",
      "Epoch 3/5\n",
      "1049/1049 [==============================] - 52s 50ms/step - loss: 0.3606 - dense_94_loss: 0.0139 - dense_95_loss: 0.3467 - val_loss: 0.2269 - val_dense_94_loss: 0.0114 - val_dense_95_loss: 0.2155\n",
      "Epoch 4/5\n",
      "1049/1049 [==============================] - 49s 47ms/step - loss: 0.3186 - dense_94_loss: 0.0128 - dense_95_loss: 0.3058 - val_loss: 0.2065 - val_dense_94_loss: 0.0113 - val_dense_95_loss: 0.1952\n",
      "Epoch 5/5\n",
      "1049/1049 [==============================] - 48s 46ms/step - loss: 0.2929 - dense_94_loss: 0.0122 - dense_95_loss: 0.2807 - val_loss: 0.1845 - val_dense_94_loss: 0.0110 - val_dense_95_loss: 0.1734\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "main_autoencoder(\n",
    "    ['introduction', 'materials', 'conclusion'],\n",
    "    path_to_read=f'/scratch/cinthiasouza/mv-text-summarizer/{data_path}',\n",
    "    path_to_write=f'../autoencoder_{base}',  bottleneck_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc6937bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representation(\n",
    "    sections, path_to_read, path_to_write, encoder_path, verbose=True):\n",
    "    \n",
    "    with open('{}/dataset_{}.pkl'.format(path_to_read,'features'), 'rb') as fp:\n",
    "        dataset = pickle.load(fp)\n",
    "        \n",
    "    columns = [str(i) for i in range(300)]\n",
    "    \n",
    "    for section in sections:\n",
    "    \n",
    "        X_features_test = dataset[section][\"X_test_features\"]\n",
    "        X_features_train = dataset[section][\"X_train_features\"]\n",
    "    \n",
    "        X_embedd_test = dataset[section]['X_test_embedd'][columns]\n",
    "        X_embedd_train = dataset[section]['X_train_embedd'][columns]\n",
    "\n",
    "        encoder = load_model('{}/encoder_{}.h5'.format(encoder_path, section))\n",
    "\n",
    "        X_test_encode = encoder.predict([X_embedd_test, X_features_test])\n",
    "        X_train_encode = encoder.predict([X_embedd_train, X_features_train])\n",
    "\n",
    "        dataset[section]['X_train_f1'] = X_train_encode\n",
    "        dataset[section]['X_test_f1'] = X_test_encode\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Write dataset\")\n",
    "    \n",
    "    with open('{}/dataset_{}.pkl'.format(path_to_write, 'features'), 'wb') as fp:\n",
    "        pickle.dump(dataset, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68e2ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Write dataset\n"
     ]
    }
   ],
   "source": [
    "create_representation(\n",
    "    sections=['introduction', 'materials', 'conclusion'],\n",
    "    path_to_read=f'/scratch/cinthiasouza/mv-text-summarizer/{data_path}',\n",
    "    path_to_write=f'/scratch/cinthiasouza/mv-text-summarizer/{data_path}',\n",
    "    encoder_path=f'../autoencoder_{base}', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67bc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e15e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
